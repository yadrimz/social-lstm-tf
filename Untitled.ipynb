{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todor/miniconda3/envs/work/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/todor/miniconda3/envs/work/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MSEC = cv2.CAP_PROP_POS_MSEC\n",
    "POS_FRAMES = cv2.CAP_PROP_POS_FRAMES\n",
    "\n",
    "POS_MSEC = cv2.CAP_PROP_POS_MSEC\n",
    "POS_FRAMES = cv2.CAP_PROP_POS_FRAMES\n",
    "\n",
    "NO_SAMPLES = 0\n",
    "PRIOR_SAMPLES = 1\n",
    "POSTERIOR_SAMPLES = 2\n",
    "SAMPLE_CHOICES = 3\n",
    "\n",
    "ESC = 27\n",
    "UP = 0\n",
    "RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['/home/todor/Documents/data/eth/univ',\n",
    "               '/home/todor/Documents/data/ucy/zara/zara01',\n",
    "               '/home/todor/Documents/data/ucy/zara/zara02']\n",
    "sequence_length = 8\n",
    "batch_size = 50\n",
    "num_epochs = 100\n",
    "decay_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_dirs):\n",
    "        all_ped_data = []\n",
    "        all_ped_dict = {}\n",
    "        dataset_indices = []\n",
    "        current_ped = 0\n",
    "        for directory in data_dirs:\n",
    "            file_path = os.path.join(directory, 'pixel_pos.csv')\n",
    "            data = np.genfromtxt(file_path, delimiter=',')\n",
    "\n",
    "            numPeds = np.size(np.unique(data[1, :]))\n",
    "            print(data[0])\n",
    "\n",
    "            for ped in range(1, numPeds+1):\n",
    "                traj = data[:, data[1, :] == ped]\n",
    "                traj = traj[[0, 3, 2], :].T\n",
    "\n",
    "                all_ped_data.append(np.array(traj))\n",
    "                all_ped_dict[current_ped + ped] = traj\n",
    "\n",
    "            dataset_indices.append(current_ped+numPeds)\n",
    "            current_ped += numPeds\n",
    "\n",
    "        return np.array(all_ped_data), all_ped_dict, dataset_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_preprocess(directory=[\"/home/todor/Documents/data/ewap_dataset/seq_eth\"]):\n",
    "    '''\n",
    "    Preprocess the frames from the datasets.\n",
    "    Convert values to pixel locations from millimeters\n",
    "    obtain and store all frames data the actually used frames (as some are skipped), \n",
    "    the ids of all pedestrians that are present at each of those frames and the sufficient statistics.\n",
    "    '''\n",
    "    def collect_stats(agents):\n",
    "        ''' Collect the means and standard deviation of all (x, y) positions of the agents '''\n",
    "        x_pos = []\n",
    "        y_pos = []\n",
    "        for agent_id in range(1, len(agents)):\n",
    "            trajectory = [[] for _ in range(3)]\n",
    "            traj = agents[agent_id]\n",
    "            for step in traj:\n",
    "                x_pos.append(step[1])\n",
    "                y_pos.append(step[2])\n",
    "        x_pos = np.asarray(x_pos)\n",
    "        y_pos = np.asarray(y_pos)\n",
    "        # takes the average over all points through all agents\n",
    "        return [[np.min(x_pos), np.max(x_pos)], [np.min(y_pos), np.max(y_pos)]]\n",
    "\n",
    "    print(directory)\n",
    "    Hfile = os.path.join(directory, \"H.txt\")\n",
    "    obsfile = os.path.join(directory, \"obsmat.txt\")\n",
    "    # Parse homography matrix.\n",
    "    H = np.loadtxt(Hfile)\n",
    "    Hinv = np.linalg.inv(H)\n",
    "    # Parse pedestrian annotations.\n",
    "    frames, pedsInFrame, agents = parse_annotations(Hinv, obsfile)\n",
    "    # collect mean and std\n",
    "    statistics = collect_stats(agents)\n",
    "    norm_agents = []\n",
    "    # collect the id, normalised x and normalised y of each agent's position\n",
    "    pedsWithPos = []\n",
    "    for agent in agents:\n",
    "        norm_traj = []\n",
    "        for step in agent:\n",
    "            _x = (step[1] - statistics[0][0]) / (statistics[0][1] - statistics[0][0])\n",
    "            _y = (step[2] - statistics[1][0]) / (statistics[1][1] - statistics[1][0])\n",
    "            norm_traj.append([int(frames[int(step[0])]), _x, _y])\n",
    "\n",
    "        norm_agents.append(np.array(norm_traj))\n",
    "\n",
    "    return np.array(norm_agents), statistics, pedsInFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(Hinv, obsmat_txt):\n",
    "    def to_image_frame(loc):\n",
    "        \"\"\"\n",
    "        Given H^-1 and (x, y, z) in world coordinates, returns (u, v, 1) in image\n",
    "        frame coordinates.\n",
    "        \"\"\"\n",
    "        loc = np.dot(Hinv, loc) # to camera frame\n",
    "        return loc/loc[2] # to pixels (from millimeters)\n",
    "\n",
    "    mat = np.loadtxt(obsmat_txt)\n",
    "    num_peds = int(np.max(mat[:,1])) + 1\n",
    "    peds = [np.array([]).reshape(0,4) for _ in range(num_peds)] # maps ped ID -> (t,x,y,z) path\n",
    "    \n",
    "    num_frames = (mat[-1,0] + 1).astype(\"int\")\n",
    "    num_unique_frames = np.unique(mat[:,0]).size\n",
    "    recorded_frames = [-1] * num_unique_frames # maps timestep -> (first) frame\n",
    "    peds_in_frame = [[] for _ in range(num_unique_frames)] # maps timestep -> ped IDs\n",
    "    \n",
    "    frame = 0\n",
    "    time = -1\n",
    "    blqk = False\n",
    "    for row in mat:\n",
    "        if row[0] != frame:\n",
    "            frame = int(row[0])\n",
    "            time += 1\n",
    "            recorded_frames[time] = frame\n",
    "\n",
    "        ped = int(row[1])\n",
    "\n",
    "        peds_in_frame[time].append(ped)\n",
    "        loc = np.array([row[2], row[4], 1])\n",
    "        loc = to_image_frame(loc)\n",
    "        loc = [time, loc[0], loc[1], loc[2]]\n",
    "        peds[ped] = np.vstack((peds[ped], loc))\n",
    "\n",
    "    return recorded_frames, peds_in_frame, peds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  780.   790.   800. ... 12380. 12380. 12380.]\n",
      "[   0.    0.    0. ... 9000. 9000. 9010.]\n",
      "[1.000e+01 1.000e+01 2.000e+01 ... 1.052e+04 1.052e+04 1.052e+04]\n"
     ]
    }
   ],
   "source": [
    "# agentsData, statistics = frame_preprocess()\n",
    "agentsData, dicto, dataset_indices = preprocess(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712,)\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "print(agentsData.shape)\n",
    "print(len(dicto.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " def load_preprocessed(pedData):\n",
    "    '''\n",
    "    Function to load the pre-processed data into the DataLoader object\n",
    "    params:\n",
    "    data_file : The path to the pickled data file\n",
    "    '''\n",
    "\n",
    "    # Construct the data with sequences(or trajectories) longer than sequence_length\n",
    "    load_data = []\n",
    "    counter = 0\n",
    "#     print(pedData[3])\n",
    "    # For each pedestrian in the data\n",
    "    for idx, traj in enumerate(pedData):\n",
    "        # Extract their trajectory\n",
    "        # If the length of the trajectory is greater than sequence_length\n",
    "        # (+1 as we need both source and target data)\n",
    "        if traj.shape[0] > (sequence_length+2):\n",
    "            load_data.append(traj)\n",
    "            # Number of batches this datapoint is worth\n",
    "            counter += int(traj.shape[0] / ((sequence_length+2)))\n",
    "\n",
    "    # Calculate the number of batches (each of batch_size) in the data\n",
    "    num_batches = int(counter / batch_size)\n",
    "\n",
    "    return load_data, num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data, num_batches = load_preprocessed(agentsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tick_batch_pointer(pointer, data_len):\n",
    "    '''\n",
    "    Advance the data pointer\n",
    "    '''\n",
    "    pointer += 1\n",
    "    if (pointer >= data_len):\n",
    "        return  0\n",
    "\n",
    "    return pointer\n",
    "\n",
    "def reset_batch_pointer():\n",
    "    '''\n",
    "    Reset the data pointer\n",
    "    '''\n",
    "    return 0\n",
    "\n",
    "def next_batch(_data, pointer, infer=False):\n",
    "    '''\n",
    "    Function to get the next batch of points\n",
    "    '''\n",
    "    # List of source and target data for the current batch\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    # For each sequence in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Extract the trajectory of the pedestrian pointed out by self.pointer\n",
    "        traj = _data[pointer]\n",
    "        # Number of sequences corresponding to his trajectory\n",
    "        n_batch = int(traj.shape[0] / (sequence_length+1))\n",
    "        # Randomly sample an index from which his trajectory is to be considered\n",
    "        if not infer:\n",
    "            idx = random.randint(0, traj.shape[0] - sequence_length - 1)\n",
    "        else:\n",
    "            idx = 0\n",
    "\n",
    "        # Append the trajectory from idx until sequence_length into source and target data\n",
    "        x_batch.append(np.copy(traj[idx:idx+sequence_length, :]))\n",
    "        y_batch.append(np.copy(traj[idx+1:idx+sequence_length+1, :]))\n",
    "\n",
    "        if random.random() < (1.0/float(n_batch)):\n",
    "            # Adjust sampling probability\n",
    "            # if this is a long datapoint, sample this data more with\n",
    "            # higher probability\n",
    "            pointer = tick_batch_pointer(pointer, len(_data))\n",
    "\n",
    "    return x_batch, y_batch, pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 5\n",
    "grad_clip = 10\n",
    "num_units = 128\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM():\n",
    "    def __init__(self,\n",
    "                 batch_size=50,\n",
    "                 sequence_length=8,\n",
    "                 num_units=128,\n",
    "                 embedding_size=128,\n",
    "                 learning_rate=0.003,\n",
    "                 grad_clip=10,\n",
    "                 mode='train',\n",
    "                 reuse=False):\n",
    "\n",
    "        with tf.variable_scope('basic_lstm', reuse=reuse):\n",
    "            self.batch_size = batch_size\n",
    "            self.sequence_length = sequence_length\n",
    "            self.num_units = num_units\n",
    "            self.embedding_size = embedding_size\n",
    "            self.learning_rate = learning_rate\n",
    "            self.grad_clip = grad_clip\n",
    "            self.mode = mode\n",
    "            self.g = tf.Graph()\n",
    "\n",
    "            with self.g.as_default():\n",
    "                self._build_graph()\n",
    "                \n",
    "        self._init_session()\n",
    "                \n",
    "    def _init_session(self):\n",
    "        \"\"\"Launch TensorFlow session and initialize variables\"\"\"\n",
    "        self.sess = tf.Session(graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        with self.g.as_default():\n",
    "            with tf.name_scope(\"LSTM_cell\"):\n",
    "                lstm_cell = rnn_cell.BasicLSTMCell(num_units, state_is_tuple=True)\n",
    "\n",
    "            self.input_data = tf.placeholder(tf.float32, [None, self.sequence_length, 2])\n",
    "            self.target_data = tf.placeholder(tf.float32, [None, self.sequence_length, 2])\n",
    "\n",
    "            self.lr = tf.Variable(self.learning_rate, trainable=False, name=\"learning_rate\")\n",
    "            self.initial_state = lstm_cell.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "\n",
    "            with tf.variable_scope(\"coordinate_embedding\"):\n",
    "                embedding_w = tf.get_variable(\"embedding_w\", [2, self.embedding_size])\n",
    "                embedding_b = tf.get_variable(\"embedding_b\", [self.embedding_size])\n",
    "\n",
    "            # Define variables for the output linear layer\n",
    "            with tf.variable_scope(\"output_layer\"):\n",
    "                output_w = tf.get_variable(\"output_w\", [num_units, output_size], initializer=tf.truncated_normal_initializer(stddev=0.1), trainable=True)\n",
    "                output_b = tf.get_variable(\"output_b\", [output_size], initializer=tf.constant_initializer(0.1), trainable=True)\n",
    "\n",
    "            inputs = tf.split(self.input_data, self.sequence_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "            embedded_inputs = []\n",
    "            for x in inputs:\n",
    "                # Each x is a 2D tensor of size numPoints x 2\n",
    "                # Embedding layer\n",
    "                embedded_x = tf.nn.relu(tf.add(tf.matmul(x, embedding_w), embedding_b))\n",
    "                embedded_inputs.append(embedded_x)\n",
    "\n",
    "#             embedded_inputs = tf.convert_to_tensor(embedded_inputs)            \n",
    "            with tf.variable_scope(\"LSTM\") as scope:\n",
    "                state = self.initial_state\n",
    "                outputs = []\n",
    "                prev = None\n",
    "                for i, inp in enumerate(embedded_inputs):\n",
    "                    if i > 0:\n",
    "                        scope.reuse_variables()\n",
    "                    output, last_state = lstm_cell(inp, state)\n",
    "                    outputs.append(output)\n",
    "\n",
    "            # Apply the linear layer. Output would be a tensor of shape 1 x output_size\n",
    "            output = tf.reshape(tf.concat(outputs, 1), [-1, self.num_units])\n",
    "            output = tf.nn.xw_plus_b(output, output_w, output_b)\n",
    "            self.final_state = last_state\n",
    "\n",
    "            # reshape target data so that it aligns with predictions\n",
    "            flat_target_data = tf.reshape(self.target_data, [-1, 2])\n",
    "            # Extract the x-coordinates and y-coordinates from the target data\n",
    "            [x_data, y_data] = tf.split(flat_target_data, 2, 1)\n",
    "                \n",
    "            def tf_2d_normal(x, y, mux, muy, sx, sy, rho):\n",
    "                # eq 3 in the paper\n",
    "                # and eq 24 & 25 in Graves (2013)\n",
    "                # Calculate (x - mux) and (y-muy)\n",
    "                normx = tf.subtract(x, mux)\n",
    "                normy = tf.subtract(y, muy)\n",
    "                # Calculate sx*sy\n",
    "                sxsy = tf.multiply(sx, sy)\n",
    "                # Calculate the exponential factor\n",
    "                z = tf.square(tf.divide(normx, sx)) + tf.square(tf.divide(normy, sy)) - 2*tf.divide(tf.multiply(rho, tf.multiply(normx, normy)), sxsy)\n",
    "                negRho = 1 - tf.square(rho)\n",
    "                # Numerator\n",
    "                result = tf.exp(tf.divide(-z, 2*negRho))\n",
    "                # Normalization constant\n",
    "                denom = 2 * np.pi * tf.multiply(sxsy, tf.sqrt(negRho))\n",
    "                # Final PDF calculation\n",
    "                result = tf.divide(result, denom)\n",
    "                self.result = result\n",
    "                return result\n",
    "\n",
    "            # Important difference between loss func of Social LSTM and Graves (2013)\n",
    "            # is that it is evaluated over all time steps in the latter whereas it is\n",
    "            # done from t_obs+1 to t_pred in the former\n",
    "            def get_lossfunc(z_mux, z_muy, z_sx, z_sy, z_corr, x_data, y_data):\n",
    "                # Calculate the PDF of the data w.r.t to the distribution\n",
    "                result0 = tf_2d_normal(x_data, y_data, z_mux, z_muy, z_sx, z_sy, z_corr)\n",
    "                # For numerical stability purposes\n",
    "                epsilon = 1e-20\n",
    "                result1 = -tf.log(tf.maximum(result0, epsilon))  # Numerical stability\n",
    "\n",
    "                return tf.reduce_sum(result1)\n",
    "\n",
    "            def get_coef(output):\n",
    "                # eq 20 -> 22 of Graves (2013)\n",
    "                z = output\n",
    "                z_mux, z_muy, z_sx, z_sy, z_corr = tf.split(z, 5, 1)\n",
    "\n",
    "                # The output must be exponentiated for the std devs\n",
    "                z_sx = tf.exp(z_sx)\n",
    "                z_sy = tf.exp(z_sy)\n",
    "                # Tanh applied to keep it in the range [-1, 1]\n",
    "                z_corr = tf.tanh(z_corr)\n",
    "\n",
    "                return [z_mux, z_muy, z_sx, z_sy, z_corr]\n",
    "\n",
    "            # Extract coef from output of the linear output layer\n",
    "            [o_mux, o_muy, o_sx, o_sy, o_corr] = get_coef(output)\n",
    "            self.mux = o_mux\n",
    "            self.muy = o_muy\n",
    "            self.sx = o_sx\n",
    "            self.sy = o_sy\n",
    "            self.corr = o_corr\n",
    "\n",
    "            with tf.name_scope(\"calculate_loss\"):\n",
    "                lossfunc = get_lossfunc(o_mux, o_muy, o_sx, o_sy, o_corr, x_data, y_data)\n",
    "        \n",
    "            with tf.name_scope(\"compute_cost\"):\n",
    "                self.cost = tf.div(lossfunc, (self.batch_size * self.sequence_length))\n",
    "                trainable_params = tf.trainable_variables()\n",
    "                # apply L2\n",
    "                l2 = 0.05 * sum(tf.nn.l2_loss(t_param) for t_param in trainable_params)\n",
    "                self.cost = self.cost + l2\n",
    "\n",
    "            self.gradients = tf.gradients(self.cost, trainable_params)\n",
    "            grads, _ = tf.clip_by_global_norm(self.gradients, self.grad_clip)\n",
    "\n",
    "            # NOTE: Using RMSprop as suggested by Social LSTM instead of Adam as Graves(2013)\n",
    "            optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "\n",
    "            # Train operator\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "    # modified from https://github.com/hardmaru/WorldModelsExperiments\n",
    "    def get_model_params(self):\n",
    "        # get trainable params.\n",
    "        model_names = []\n",
    "        model_params = []\n",
    "        model_shapes = []\n",
    "        with self.g.as_default():\n",
    "            t_vars = tf.trainable_variables()\n",
    "            for var in t_vars:\n",
    "                param_name = var.name\n",
    "                p = self.sess.run(var)\n",
    "                model_names.append(param_name)\n",
    "                params = np.round(p*10000).astype(np.int).tolist() # ..?!\n",
    "                model_params.append(params)\n",
    "                model_shapes.append(p.shape)\n",
    "        return model_params, model_shapes, model_names\n",
    "\n",
    "    def set_model_params(self, params):\n",
    "        with self.g.as_default():\n",
    "            t_vars = tf.trainable_variables()\n",
    "            idx = 0\n",
    "            for var in t_vars:\n",
    "                pshape = self.sess.run(var).shape\n",
    "                p = np.array(params[idx])\n",
    "                assert pshape == p.shape, \"inconsistent shape\"\n",
    "                assign_op = var.assign(p.astype(np.float)/10000.)\n",
    "                self.sess.run(assign_op)\n",
    "                idx += 1\n",
    "        \n",
    "    def save_json(self, jsonfile='rnn.json'):\n",
    "        model_params, model_shapes, model_names = self.get_model_params()\n",
    "        qparams = []\n",
    "        for p in model_params:\n",
    "            qparams.append(p)\n",
    "        with open(jsonfile, 'wt') as outfile:\n",
    "            json.dump(qparams, outfile, sort_keys=True, indent=0, separators=(',', ': '))\n",
    "\n",
    "    def load_json(self, jsonfile='rnn.json'):\n",
    "        with open(jsonfile, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        self.set_model_params(params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    ops.reset_default_graph()\n",
    "    print(\"graph successfully reset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph successfully reset\n",
      "0/3300 (epoch 0), train_loss = 10.959, time/batch = 0.251\n",
      "1/3300 (epoch 0), train_loss = 10.938, time/batch = 0.007\n",
      "2/3300 (epoch 0), train_loss = 10.901, time/batch = 0.007\n",
      "3/3300 (epoch 0), train_loss = 10.901, time/batch = 0.007\n",
      "4/3300 (epoch 0), train_loss = 10.876, time/batch = 0.007\n",
      "5/3300 (epoch 0), train_loss = 10.893, time/batch = 0.005\n",
      "6/3300 (epoch 0), train_loss = 10.856, time/batch = 0.009\n",
      "7/3300 (epoch 0), train_loss = 10.809, time/batch = 0.011\n",
      "8/3300 (epoch 0), train_loss = 10.844, time/batch = 0.008\n",
      "9/3300 (epoch 0), train_loss = 10.785, time/batch = 0.006\n",
      "10/3300 (epoch 0), train_loss = 10.778, time/batch = 0.007\n",
      "11/3300 (epoch 0), train_loss = 10.807, time/batch = 0.008\n",
      "12/3300 (epoch 0), train_loss = 10.822, time/batch = 0.010\n",
      "13/3300 (epoch 0), train_loss = 10.750, time/batch = 0.009\n",
      "14/3300 (epoch 0), train_loss = 10.735, time/batch = 0.008\n",
      "15/3300 (epoch 0), train_loss = 10.701, time/batch = 0.009\n",
      "16/3300 (epoch 0), train_loss = 10.683, time/batch = 0.008\n",
      "17/3300 (epoch 0), train_loss = 10.642, time/batch = 0.009\n",
      "18/3300 (epoch 0), train_loss = 10.531, time/batch = 0.010\n",
      "19/3300 (epoch 0), train_loss = 10.564, time/batch = 0.007\n",
      "20/3300 (epoch 0), train_loss = 10.464, time/batch = 0.007\n",
      "21/3300 (epoch 0), train_loss = 10.442, time/batch = 0.010\n",
      "22/3300 (epoch 0), train_loss = 10.441, time/batch = 0.009\n",
      "23/3300 (epoch 0), train_loss = 10.354, time/batch = 0.008\n",
      "24/3300 (epoch 0), train_loss = 10.307, time/batch = 0.015\n",
      "25/3300 (epoch 0), train_loss = 10.288, time/batch = 0.011\n",
      "26/3300 (epoch 0), train_loss = 10.242, time/batch = 0.013\n",
      "27/3300 (epoch 0), train_loss = 10.193, time/batch = 0.008\n",
      "28/3300 (epoch 0), train_loss = 10.205, time/batch = 0.017\n",
      "29/3300 (epoch 0), train_loss = 10.132, time/batch = 0.014\n",
      "30/3300 (epoch 0), train_loss = 10.074, time/batch = 0.008\n",
      "31/3300 (epoch 0), train_loss = 10.026, time/batch = 0.015\n",
      "32/3300 (epoch 0), train_loss = 9.968, time/batch = 0.012\n",
      "33/3300 (epoch 1), train_loss = 10.382, time/batch = 0.006\n",
      "34/3300 (epoch 1), train_loss = 10.222, time/batch = 0.015\n",
      "35/3300 (epoch 1), train_loss = 10.084, time/batch = 0.008\n",
      "36/3300 (epoch 1), train_loss = 9.976, time/batch = 0.006\n",
      "37/3300 (epoch 1), train_loss = 9.855, time/batch = 0.010\n",
      "38/3300 (epoch 1), train_loss = 9.783, time/batch = 0.007\n",
      "39/3300 (epoch 1), train_loss = 9.612, time/batch = 0.008\n",
      "40/3300 (epoch 1), train_loss = 9.510, time/batch = 0.007\n",
      "41/3300 (epoch 1), train_loss = 9.439, time/batch = 0.005\n",
      "42/3300 (epoch 1), train_loss = 9.355, time/batch = 0.006\n",
      "43/3300 (epoch 1), train_loss = 9.206, time/batch = 0.008\n",
      "44/3300 (epoch 1), train_loss = 9.084, time/batch = 0.007\n",
      "45/3300 (epoch 1), train_loss = 9.064, time/batch = 0.009\n",
      "46/3300 (epoch 1), train_loss = 8.820, time/batch = 0.007\n",
      "47/3300 (epoch 1), train_loss = 8.686, time/batch = 0.006\n",
      "48/3300 (epoch 1), train_loss = 8.507, time/batch = 0.004\n",
      "49/3300 (epoch 1), train_loss = 8.371, time/batch = 0.009\n",
      "50/3300 (epoch 1), train_loss = 8.280, time/batch = 0.007\n",
      "51/3300 (epoch 1), train_loss = 8.141, time/batch = 0.005\n",
      "52/3300 (epoch 1), train_loss = 8.004, time/batch = 0.005\n",
      "53/3300 (epoch 1), train_loss = 8.463, time/batch = 0.008\n",
      "54/3300 (epoch 1), train_loss = 8.023, time/batch = 0.008\n",
      "55/3300 (epoch 1), train_loss = 7.486, time/batch = 0.005\n",
      "56/3300 (epoch 1), train_loss = 7.171, time/batch = 0.006\n",
      "57/3300 (epoch 1), train_loss = 6.997, time/batch = 0.006\n",
      "58/3300 (epoch 1), train_loss = 6.712, time/batch = 0.008\n",
      "59/3300 (epoch 1), train_loss = 6.535, time/batch = 0.009\n",
      "60/3300 (epoch 1), train_loss = 6.650, time/batch = 0.006\n",
      "61/3300 (epoch 1), train_loss = 6.222, time/batch = 0.006\n",
      "62/3300 (epoch 1), train_loss = 6.392, time/batch = 0.008\n",
      "63/3300 (epoch 1), train_loss = 8.548, time/batch = 0.008\n",
      "64/3300 (epoch 1), train_loss = 7.147, time/batch = 0.006\n",
      "65/3300 (epoch 1), train_loss = 6.501, time/batch = 0.007\n",
      "66/3300 (epoch 2), train_loss = 8.805, time/batch = 0.010\n",
      "67/3300 (epoch 2), train_loss = 8.117, time/batch = 0.009\n",
      "68/3300 (epoch 2), train_loss = 7.594, time/batch = 0.009\n",
      "69/3300 (epoch 2), train_loss = 7.577, time/batch = 0.008\n",
      "70/3300 (epoch 2), train_loss = 6.746, time/batch = 0.011\n",
      "71/3300 (epoch 2), train_loss = 6.249, time/batch = 0.018\n",
      "72/3300 (epoch 2), train_loss = 6.472, time/batch = 0.012\n",
      "73/3300 (epoch 2), train_loss = 6.912, time/batch = 0.008\n",
      "74/3300 (epoch 2), train_loss = 5.885, time/batch = 0.012\n",
      "75/3300 (epoch 2), train_loss = 8.037, time/batch = 0.013\n",
      "76/3300 (epoch 2), train_loss = 7.385, time/batch = 0.015\n",
      "77/3300 (epoch 2), train_loss = 6.097, time/batch = 0.006\n",
      "78/3300 (epoch 2), train_loss = 5.943, time/batch = 0.004\n",
      "79/3300 (epoch 2), train_loss = 6.361, time/batch = 0.007\n",
      "80/3300 (epoch 2), train_loss = 5.505, time/batch = 0.005\n",
      "81/3300 (epoch 2), train_loss = 5.500, time/batch = 0.010\n",
      "82/3300 (epoch 2), train_loss = 6.306, time/batch = 0.007\n",
      "83/3300 (epoch 2), train_loss = 5.520, time/batch = 0.009\n",
      "84/3300 (epoch 2), train_loss = 5.293, time/batch = 0.007\n",
      "85/3300 (epoch 2), train_loss = 5.869, time/batch = 0.015\n",
      "86/3300 (epoch 2), train_loss = 5.259, time/batch = 0.006\n",
      "87/3300 (epoch 2), train_loss = 5.304, time/batch = 0.011\n",
      "88/3300 (epoch 2), train_loss = 4.907, time/batch = 0.013\n",
      "89/3300 (epoch 2), train_loss = 4.468, time/batch = 0.008\n",
      "90/3300 (epoch 2), train_loss = 3.657, time/batch = 0.013\n",
      "91/3300 (epoch 2), train_loss = 5.434, time/batch = 0.009\n",
      "92/3300 (epoch 2), train_loss = 6.342, time/batch = 0.006\n",
      "93/3300 (epoch 2), train_loss = 4.100, time/batch = 0.016\n",
      "94/3300 (epoch 2), train_loss = 4.289, time/batch = 0.006\n",
      "95/3300 (epoch 2), train_loss = 4.930, time/batch = 0.011\n",
      "96/3300 (epoch 2), train_loss = 3.894, time/batch = 0.005\n",
      "97/3300 (epoch 2), train_loss = 3.708, time/batch = 0.012\n",
      "98/3300 (epoch 2), train_loss = 3.579, time/batch = 0.016\n",
      "99/3300 (epoch 3), train_loss = 5.433, time/batch = 0.009\n",
      "100/3300 (epoch 3), train_loss = 4.565, time/batch = 0.012\n",
      "101/3300 (epoch 3), train_loss = 3.721, time/batch = 0.016\n",
      "102/3300 (epoch 3), train_loss = 3.177, time/batch = 0.016\n",
      "103/3300 (epoch 3), train_loss = 3.169, time/batch = 0.017\n",
      "104/3300 (epoch 3), train_loss = 3.628, time/batch = 0.017\n",
      "105/3300 (epoch 3), train_loss = 2.013, time/batch = 0.016\n",
      "106/3300 (epoch 3), train_loss = 1.912, time/batch = 0.017\n",
      "107/3300 (epoch 3), train_loss = 2.812, time/batch = 0.020\n",
      "108/3300 (epoch 3), train_loss = 4.217, time/batch = 0.011\n",
      "109/3300 (epoch 3), train_loss = 2.973, time/batch = 0.012\n",
      "110/3300 (epoch 3), train_loss = 2.193, time/batch = 0.008\n",
      "111/3300 (epoch 3), train_loss = 1.772, time/batch = 0.015\n",
      "112/3300 (epoch 3), train_loss = 1.657, time/batch = 0.016\n",
      "113/3300 (epoch 3), train_loss = 1.581, time/batch = 0.016\n",
      "114/3300 (epoch 3), train_loss = 1.666, time/batch = 0.017\n",
      "115/3300 (epoch 3), train_loss = 0.970, time/batch = 0.014\n",
      "116/3300 (epoch 3), train_loss = 1.286, time/batch = 0.012\n",
      "117/3300 (epoch 3), train_loss = 0.864, time/batch = 0.009\n",
      "118/3300 (epoch 3), train_loss = 0.923, time/batch = 0.009\n",
      "119/3300 (epoch 3), train_loss = 0.998, time/batch = 0.008\n",
      "120/3300 (epoch 3), train_loss = 2.011, time/batch = 0.008\n",
      "121/3300 (epoch 3), train_loss = 0.774, time/batch = 0.008\n",
      "122/3300 (epoch 3), train_loss = 0.209, time/batch = 0.006\n",
      "123/3300 (epoch 3), train_loss = -0.197, time/batch = 0.005\n",
      "124/3300 (epoch 3), train_loss = 0.581, time/batch = 0.009\n",
      "125/3300 (epoch 3), train_loss = -0.564, time/batch = 0.006\n",
      "126/3300 (epoch 3), train_loss = 0.409, time/batch = 0.008\n",
      "127/3300 (epoch 3), train_loss = 0.487, time/batch = 0.006\n",
      "128/3300 (epoch 3), train_loss = 0.460, time/batch = 0.005\n",
      "129/3300 (epoch 3), train_loss = 1.198, time/batch = 0.006\n",
      "130/3300 (epoch 3), train_loss = -0.345, time/batch = 0.018\n",
      "131/3300 (epoch 3), train_loss = 0.466, time/batch = 0.009\n",
      "132/3300 (epoch 4), train_loss = 2.096, time/batch = 0.005\n",
      "133/3300 (epoch 4), train_loss = 1.342, time/batch = 0.005\n",
      "134/3300 (epoch 4), train_loss = 0.780, time/batch = 0.007\n",
      "135/3300 (epoch 4), train_loss = 0.060, time/batch = 0.005\n",
      "136/3300 (epoch 4), train_loss = -0.377, time/batch = 0.007\n",
      "137/3300 (epoch 4), train_loss = 1.081, time/batch = 0.013\n",
      "138/3300 (epoch 4), train_loss = 0.661, time/batch = 0.012\n",
      "139/3300 (epoch 4), train_loss = 0.056, time/batch = 0.005\n",
      "140/3300 (epoch 4), train_loss = 0.276, time/batch = 0.009\n",
      "141/3300 (epoch 4), train_loss = -0.077, time/batch = 0.019\n",
      "142/3300 (epoch 4), train_loss = 0.089, time/batch = 0.005\n",
      "143/3300 (epoch 4), train_loss = -0.635, time/batch = 0.012\n",
      "144/3300 (epoch 4), train_loss = -0.363, time/batch = 0.005\n",
      "145/3300 (epoch 4), train_loss = 0.443, time/batch = 0.004\n",
      "146/3300 (epoch 4), train_loss = -0.584, time/batch = 0.014\n",
      "147/3300 (epoch 4), train_loss = -0.218, time/batch = 0.014\n",
      "148/3300 (epoch 4), train_loss = -0.573, time/batch = 0.005\n",
      "149/3300 (epoch 4), train_loss = 0.110, time/batch = 0.012\n",
      "150/3300 (epoch 4), train_loss = -0.516, time/batch = 0.004\n",
      "151/3300 (epoch 4), train_loss = -0.565, time/batch = 0.013\n",
      "152/3300 (epoch 4), train_loss = -0.295, time/batch = 0.004\n",
      "153/3300 (epoch 4), train_loss = -0.403, time/batch = 0.013\n",
      "154/3300 (epoch 4), train_loss = -1.192, time/batch = 0.005\n",
      "155/3300 (epoch 4), train_loss = -1.689, time/batch = 0.014\n",
      "156/3300 (epoch 4), train_loss = -1.057, time/batch = 0.007\n",
      "157/3300 (epoch 4), train_loss = 19.975, time/batch = 0.005\n",
      "158/3300 (epoch 4), train_loss = -0.860, time/batch = 0.021\n",
      "159/3300 (epoch 4), train_loss = 3.087, time/batch = 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/3300 (epoch 4), train_loss = -1.016, time/batch = 0.007\n",
      "161/3300 (epoch 4), train_loss = -1.054, time/batch = 0.015\n",
      "162/3300 (epoch 4), train_loss = -0.380, time/batch = 0.004\n",
      "163/3300 (epoch 4), train_loss = 0.792, time/batch = 0.006\n",
      "164/3300 (epoch 4), train_loss = -0.455, time/batch = 0.005\n",
      "165/3300 (epoch 5), train_loss = 1.396, time/batch = 0.005\n",
      "166/3300 (epoch 5), train_loss = 0.569, time/batch = 0.004\n",
      "167/3300 (epoch 5), train_loss = 0.002, time/batch = 0.005\n",
      "168/3300 (epoch 5), train_loss = -0.124, time/batch = 0.007\n",
      "169/3300 (epoch 5), train_loss = -0.324, time/batch = 0.004\n",
      "170/3300 (epoch 5), train_loss = -0.234, time/batch = 0.005\n",
      "171/3300 (epoch 5), train_loss = -0.810, time/batch = 0.005\n",
      "172/3300 (epoch 5), train_loss = 0.207, time/batch = 0.005\n",
      "173/3300 (epoch 5), train_loss = -0.739, time/batch = 0.005\n",
      "174/3300 (epoch 5), train_loss = -0.277, time/batch = 0.013\n",
      "175/3300 (epoch 5), train_loss = -0.138, time/batch = 0.005\n",
      "176/3300 (epoch 5), train_loss = -1.007, time/batch = 0.009\n",
      "177/3300 (epoch 5), train_loss = -1.068, time/batch = 0.004\n",
      "178/3300 (epoch 5), train_loss = 1.594, time/batch = 0.004\n",
      "179/3300 (epoch 5), train_loss = -1.029, time/batch = 0.008\n",
      "180/3300 (epoch 5), train_loss = 0.359, time/batch = 0.006\n",
      "181/3300 (epoch 5), train_loss = -1.383, time/batch = 0.020\n",
      "182/3300 (epoch 5), train_loss = -0.420, time/batch = 0.004\n",
      "183/3300 (epoch 5), train_loss = -0.490, time/batch = 0.010\n",
      "184/3300 (epoch 5), train_loss = -1.112, time/batch = 0.006\n",
      "185/3300 (epoch 5), train_loss = -0.529, time/batch = 0.010\n",
      "186/3300 (epoch 5), train_loss = -0.545, time/batch = 0.004\n",
      "187/3300 (epoch 5), train_loss = -1.115, time/batch = 0.013\n",
      "188/3300 (epoch 5), train_loss = -0.951, time/batch = 0.004\n",
      "189/3300 (epoch 5), train_loss = -1.598, time/batch = 0.013\n",
      "190/3300 (epoch 5), train_loss = -1.302, time/batch = 0.004\n",
      "191/3300 (epoch 5), train_loss = -1.191, time/batch = 0.012\n",
      "192/3300 (epoch 5), train_loss = -1.800, time/batch = 0.004\n",
      "193/3300 (epoch 5), train_loss = -1.398, time/batch = 0.012\n",
      "194/3300 (epoch 5), train_loss = -0.957, time/batch = 0.010\n",
      "195/3300 (epoch 5), train_loss = 0.406, time/batch = 0.007\n",
      "196/3300 (epoch 5), train_loss = -0.925, time/batch = 0.012\n",
      "197/3300 (epoch 5), train_loss = -1.032, time/batch = 0.006\n",
      "198/3300 (epoch 6), train_loss = 1.233, time/batch = 0.005\n",
      "199/3300 (epoch 6), train_loss = 0.292, time/batch = 0.004\n",
      "200/3300 (epoch 6), train_loss = -0.410, time/batch = 0.005\n",
      "201/3300 (epoch 6), train_loss = -0.645, time/batch = 0.005\n",
      "202/3300 (epoch 6), train_loss = -0.071, time/batch = 0.005\n",
      "203/3300 (epoch 6), train_loss = -0.371, time/batch = 0.005\n",
      "204/3300 (epoch 6), train_loss = 1.443, time/batch = 0.004\n",
      "205/3300 (epoch 6), train_loss = -1.018, time/batch = 0.005\n",
      "206/3300 (epoch 6), train_loss = 0.786, time/batch = 0.004\n",
      "207/3300 (epoch 6), train_loss = -0.436, time/batch = 0.011\n",
      "208/3300 (epoch 6), train_loss = -0.640, time/batch = 0.007\n",
      "209/3300 (epoch 6), train_loss = -0.983, time/batch = 0.004\n",
      "210/3300 (epoch 6), train_loss = -0.393, time/batch = 0.008\n",
      "211/3300 (epoch 6), train_loss = -1.313, time/batch = 0.005\n",
      "212/3300 (epoch 6), train_loss = -0.645, time/batch = 0.012\n",
      "213/3300 (epoch 6), train_loss = -0.727, time/batch = 0.005\n",
      "214/3300 (epoch 6), train_loss = -1.223, time/batch = 0.012\n",
      "215/3300 (epoch 6), train_loss = -1.260, time/batch = 0.004\n",
      "216/3300 (epoch 6), train_loss = -0.551, time/batch = 0.006\n",
      "217/3300 (epoch 6), train_loss = -1.323, time/batch = 0.004\n",
      "218/3300 (epoch 6), train_loss = -0.457, time/batch = 0.004\n",
      "219/3300 (epoch 6), train_loss = -1.389, time/batch = 0.011\n",
      "220/3300 (epoch 6), train_loss = -1.524, time/batch = 0.008\n",
      "221/3300 (epoch 6), train_loss = -1.049, time/batch = 0.004\n",
      "222/3300 (epoch 6), train_loss = -0.940, time/batch = 0.004\n",
      "223/3300 (epoch 6), train_loss = -1.000, time/batch = 0.004\n",
      "224/3300 (epoch 6), train_loss = 0.237, time/batch = 0.018\n",
      "225/3300 (epoch 6), train_loss = -0.371, time/batch = 0.005\n",
      "226/3300 (epoch 6), train_loss = -1.069, time/batch = 0.004\n",
      "227/3300 (epoch 6), train_loss = -0.597, time/batch = 0.004\n",
      "228/3300 (epoch 6), train_loss = -1.718, time/batch = 0.012\n",
      "229/3300 (epoch 6), train_loss = -1.594, time/batch = 0.004\n",
      "230/3300 (epoch 6), train_loss = -0.848, time/batch = 0.004\n",
      "231/3300 (epoch 7), train_loss = 1.074, time/batch = 0.004\n",
      "232/3300 (epoch 7), train_loss = 0.299, time/batch = 0.005\n",
      "233/3300 (epoch 7), train_loss = -0.688, time/batch = 0.006\n",
      "234/3300 (epoch 7), train_loss = -0.824, time/batch = 0.004\n",
      "235/3300 (epoch 7), train_loss = 1.775, time/batch = 0.011\n",
      "236/3300 (epoch 7), train_loss = -0.785, time/batch = 0.006\n",
      "237/3300 (epoch 7), train_loss = 4.099, time/batch = 0.005\n",
      "238/3300 (epoch 7), train_loss = -1.284, time/batch = 0.004\n",
      "239/3300 (epoch 7), train_loss = -0.100, time/batch = 0.004\n",
      "240/3300 (epoch 7), train_loss = -0.955, time/batch = 0.005\n",
      "241/3300 (epoch 7), train_loss = -0.894, time/batch = 0.005\n",
      "242/3300 (epoch 7), train_loss = -0.868, time/batch = 0.010\n",
      "243/3300 (epoch 7), train_loss = -1.298, time/batch = 0.006\n",
      "244/3300 (epoch 7), train_loss = -1.296, time/batch = 0.011\n",
      "245/3300 (epoch 7), train_loss = -1.247, time/batch = 0.005\n",
      "246/3300 (epoch 7), train_loss = -1.482, time/batch = 0.005\n",
      "247/3300 (epoch 7), train_loss = -1.011, time/batch = 0.005\n",
      "248/3300 (epoch 7), train_loss = -1.579, time/batch = 0.005\n",
      "249/3300 (epoch 7), train_loss = -1.533, time/batch = 0.010\n",
      "250/3300 (epoch 7), train_loss = 0.256, time/batch = 0.009\n",
      "251/3300 (epoch 7), train_loss = -1.309, time/batch = 0.005\n",
      "252/3300 (epoch 7), train_loss = -1.487, time/batch = 0.006\n",
      "253/3300 (epoch 7), train_loss = -0.232, time/batch = 0.004\n",
      "254/3300 (epoch 7), train_loss = -1.802, time/batch = 0.008\n",
      "255/3300 (epoch 7), train_loss = -2.259, time/batch = 0.006\n",
      "256/3300 (epoch 7), train_loss = 1.115, time/batch = 0.005\n",
      "257/3300 (epoch 7), train_loss = -0.960, time/batch = 0.004\n",
      "258/3300 (epoch 7), train_loss = 0.916, time/batch = 0.005\n",
      "259/3300 (epoch 7), train_loss = 2.054, time/batch = 0.004\n",
      "260/3300 (epoch 7), train_loss = -1.673, time/batch = 0.004\n",
      "261/3300 (epoch 7), train_loss = -0.843, time/batch = 0.010\n",
      "262/3300 (epoch 7), train_loss = -1.345, time/batch = 0.009\n",
      "263/3300 (epoch 7), train_loss = -1.436, time/batch = 0.006\n",
      "264/3300 (epoch 8), train_loss = 0.983, time/batch = 0.012\n",
      "265/3300 (epoch 8), train_loss = 0.013, time/batch = 0.007\n",
      "266/3300 (epoch 8), train_loss = -0.698, time/batch = 0.005\n",
      "267/3300 (epoch 8), train_loss = -1.038, time/batch = 0.007\n",
      "268/3300 (epoch 8), train_loss = -0.092, time/batch = 0.018\n",
      "269/3300 (epoch 8), train_loss = 1.633, time/batch = 0.007\n",
      "270/3300 (epoch 8), train_loss = -1.574, time/batch = 0.004\n",
      "271/3300 (epoch 8), train_loss = 0.860, time/batch = 0.010\n",
      "272/3300 (epoch 8), train_loss = -1.197, time/batch = 0.009\n",
      "273/3300 (epoch 8), train_loss = -0.061, time/batch = 0.015\n",
      "274/3300 (epoch 8), train_loss = -1.401, time/batch = 0.009\n",
      "275/3300 (epoch 8), train_loss = -0.173, time/batch = 0.007\n",
      "276/3300 (epoch 8), train_loss = -1.415, time/batch = 0.009\n",
      "277/3300 (epoch 8), train_loss = -0.799, time/batch = 0.008\n",
      "278/3300 (epoch 8), train_loss = -1.634, time/batch = 0.010\n",
      "279/3300 (epoch 8), train_loss = -1.594, time/batch = 0.011\n",
      "280/3300 (epoch 8), train_loss = -0.721, time/batch = 0.005\n",
      "281/3300 (epoch 8), train_loss = -0.646, time/batch = 0.010\n",
      "282/3300 (epoch 8), train_loss = -1.288, time/batch = 0.007\n",
      "283/3300 (epoch 8), train_loss = -0.806, time/batch = 0.009\n",
      "284/3300 (epoch 8), train_loss = -1.051, time/batch = 0.004\n",
      "285/3300 (epoch 8), train_loss = -1.345, time/batch = 0.007\n",
      "286/3300 (epoch 8), train_loss = -1.641, time/batch = 0.009\n",
      "287/3300 (epoch 8), train_loss = -1.539, time/batch = 0.014\n",
      "288/3300 (epoch 8), train_loss = -1.990, time/batch = 0.007\n",
      "289/3300 (epoch 8), train_loss = -1.913, time/batch = 0.010\n",
      "290/3300 (epoch 8), train_loss = -2.076, time/batch = 0.008\n",
      "291/3300 (epoch 8), train_loss = -0.587, time/batch = 0.011\n",
      "292/3300 (epoch 8), train_loss = -1.244, time/batch = 0.013\n",
      "293/3300 (epoch 8), train_loss = -1.061, time/batch = 0.007\n",
      "294/3300 (epoch 8), train_loss = -2.482, time/batch = 0.007\n",
      "295/3300 (epoch 8), train_loss = 2.146, time/batch = 0.022\n",
      "296/3300 (epoch 8), train_loss = -0.968, time/batch = 0.007\n",
      "297/3300 (epoch 9), train_loss = 0.888, time/batch = 0.007\n",
      "298/3300 (epoch 9), train_loss = -0.006, time/batch = 0.005\n",
      "299/3300 (epoch 9), train_loss = -0.857, time/batch = 0.020\n",
      "300/3300 (epoch 9), train_loss = -1.072, time/batch = 0.008\n",
      "301/3300 (epoch 9), train_loss = -0.541, time/batch = 0.009\n",
      "302/3300 (epoch 9), train_loss = 0.875, time/batch = 0.007\n",
      "303/3300 (epoch 9), train_loss = -1.521, time/batch = 0.004\n",
      "304/3300 (epoch 9), train_loss = -0.046, time/batch = 0.010\n",
      "305/3300 (epoch 9), train_loss = -1.406, time/batch = 0.008\n",
      "306/3300 (epoch 9), train_loss = -0.670, time/batch = 0.015\n",
      "307/3300 (epoch 9), train_loss = -1.400, time/batch = 0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/3300 (epoch 9), train_loss = 0.963, time/batch = 0.005\n",
      "309/3300 (epoch 9), train_loss = -0.980, time/batch = 0.004\n",
      "310/3300 (epoch 9), train_loss = -0.996, time/batch = 0.014\n",
      "311/3300 (epoch 9), train_loss = -1.396, time/batch = 0.005\n",
      "312/3300 (epoch 9), train_loss = -1.481, time/batch = 0.013\n",
      "313/3300 (epoch 9), train_loss = -1.414, time/batch = 0.010\n",
      "314/3300 (epoch 9), train_loss = -0.806, time/batch = 0.007\n",
      "315/3300 (epoch 9), train_loss = -1.713, time/batch = 0.004\n",
      "316/3300 (epoch 9), train_loss = -1.707, time/batch = 0.013\n",
      "317/3300 (epoch 9), train_loss = 0.124, time/batch = 0.004\n",
      "318/3300 (epoch 9), train_loss = -1.881, time/batch = 0.004\n",
      "319/3300 (epoch 9), train_loss = 0.100, time/batch = 0.013\n",
      "320/3300 (epoch 9), train_loss = -1.952, time/batch = 0.004\n",
      "321/3300 (epoch 9), train_loss = 0.590, time/batch = 0.012\n",
      "322/3300 (epoch 9), train_loss = -1.560, time/batch = 0.004\n",
      "323/3300 (epoch 9), train_loss = -0.650, time/batch = 0.007\n",
      "324/3300 (epoch 9), train_loss = -1.406, time/batch = 0.004\n",
      "325/3300 (epoch 9), train_loss = -0.905, time/batch = 0.020\n",
      "326/3300 (epoch 9), train_loss = -1.763, time/batch = 0.004\n",
      "327/3300 (epoch 9), train_loss = -1.607, time/batch = 0.005\n",
      "328/3300 (epoch 9), train_loss = -1.471, time/batch = 0.005\n",
      "329/3300 (epoch 9), train_loss = -1.670, time/batch = 0.013\n",
      "330/3300 (epoch 10), train_loss = 0.676, time/batch = 0.011\n",
      "331/3300 (epoch 10), train_loss = -0.346, time/batch = 0.005\n",
      "332/3300 (epoch 10), train_loss = -0.626, time/batch = 0.015\n",
      "333/3300 (epoch 10), train_loss = -1.272, time/batch = 0.004\n",
      "334/3300 (epoch 10), train_loss = 2.027, time/batch = 0.013\n",
      "335/3300 (epoch 10), train_loss = 1.788, time/batch = 0.004\n",
      "336/3300 (epoch 10), train_loss = -1.693, time/batch = 0.006\n",
      "337/3300 (epoch 10), train_loss = 2.531, time/batch = 0.006\n",
      "338/3300 (epoch 10), train_loss = -1.544, time/batch = 0.004\n",
      "339/3300 (epoch 10), train_loss = -0.233, time/batch = 0.006\n",
      "340/3300 (epoch 10), train_loss = -1.539, time/batch = 0.009\n",
      "341/3300 (epoch 10), train_loss = -0.248, time/batch = 0.009\n",
      "342/3300 (epoch 10), train_loss = -1.649, time/batch = 0.006\n",
      "343/3300 (epoch 10), train_loss = -0.827, time/batch = 0.009\n",
      "344/3300 (epoch 10), train_loss = -1.582, time/batch = 0.007\n",
      "345/3300 (epoch 10), train_loss = -0.524, time/batch = 0.007\n",
      "346/3300 (epoch 10), train_loss = -1.918, time/batch = 0.004\n",
      "347/3300 (epoch 10), train_loss = -0.874, time/batch = 0.004\n",
      "348/3300 (epoch 10), train_loss = -2.096, time/batch = 0.004\n",
      "349/3300 (epoch 10), train_loss = -0.208, time/batch = 0.013\n",
      "350/3300 (epoch 10), train_loss = -1.319, time/batch = 0.004\n",
      "351/3300 (epoch 10), train_loss = -1.374, time/batch = 0.013\n",
      "352/3300 (epoch 10), train_loss = -1.648, time/batch = 0.005\n",
      "353/3300 (epoch 10), train_loss = -1.544, time/batch = 0.009\n",
      "354/3300 (epoch 10), train_loss = -1.046, time/batch = 0.008\n",
      "355/3300 (epoch 10), train_loss = -2.350, time/batch = 0.010\n",
      "356/3300 (epoch 10), train_loss = 0.401, time/batch = 0.005\n",
      "357/3300 (epoch 10), train_loss = -2.621, time/batch = 0.011\n",
      "358/3300 (epoch 10), train_loss = -0.319, time/batch = 0.007\n",
      "359/3300 (epoch 10), train_loss = -2.047, time/batch = 0.007\n",
      "360/3300 (epoch 10), train_loss = 0.556, time/batch = 0.004\n",
      "361/3300 (epoch 10), train_loss = -2.285, time/batch = 0.006\n",
      "362/3300 (epoch 10), train_loss = 1.836, time/batch = 0.010\n",
      "363/3300 (epoch 11), train_loss = 0.590, time/batch = 0.004\n",
      "364/3300 (epoch 11), train_loss = -0.469, time/batch = 0.004\n",
      "365/3300 (epoch 11), train_loss = -1.082, time/batch = 0.009\n",
      "366/3300 (epoch 11), train_loss = -1.422, time/batch = 0.004\n",
      "367/3300 (epoch 11), train_loss = 3.061, time/batch = 0.004\n",
      "368/3300 (epoch 11), train_loss = -1.166, time/batch = 0.014\n",
      "369/3300 (epoch 11), train_loss = 2.083, time/batch = 0.005\n",
      "370/3300 (epoch 11), train_loss = -1.906, time/batch = 0.013\n",
      "371/3300 (epoch 11), train_loss = 1.839, time/batch = 0.004\n",
      "372/3300 (epoch 11), train_loss = -1.606, time/batch = 0.013\n",
      "373/3300 (epoch 11), train_loss = -1.070, time/batch = 0.012\n",
      "374/3300 (epoch 11), train_loss = -1.388, time/batch = 0.009\n",
      "375/3300 (epoch 11), train_loss = -1.326, time/batch = 0.010\n",
      "376/3300 (epoch 11), train_loss = -1.733, time/batch = 0.005\n",
      "377/3300 (epoch 11), train_loss = -0.854, time/batch = 0.016\n",
      "378/3300 (epoch 11), train_loss = -1.797, time/batch = 0.014\n",
      "379/3300 (epoch 11), train_loss = -1.135, time/batch = 0.005\n",
      "380/3300 (epoch 11), train_loss = -1.814, time/batch = 0.010\n",
      "381/3300 (epoch 11), train_loss = -0.169, time/batch = 0.004\n",
      "382/3300 (epoch 11), train_loss = -1.925, time/batch = 0.007\n",
      "383/3300 (epoch 11), train_loss = -1.166, time/batch = 0.005\n",
      "384/3300 (epoch 11), train_loss = -1.401, time/batch = 0.007\n",
      "385/3300 (epoch 11), train_loss = -1.562, time/batch = 0.010\n",
      "386/3300 (epoch 11), train_loss = -1.648, time/batch = 0.007\n",
      "387/3300 (epoch 11), train_loss = -1.652, time/batch = 0.008\n",
      "388/3300 (epoch 11), train_loss = -1.348, time/batch = 0.007\n",
      "389/3300 (epoch 11), train_loss = -2.079, time/batch = 0.006\n",
      "390/3300 (epoch 11), train_loss = -0.183, time/batch = 0.004\n",
      "391/3300 (epoch 11), train_loss = -1.393, time/batch = 0.004\n",
      "392/3300 (epoch 11), train_loss = -2.161, time/batch = 0.004\n",
      "393/3300 (epoch 11), train_loss = -2.314, time/batch = 0.012\n",
      "394/3300 (epoch 11), train_loss = -1.542, time/batch = 0.004\n",
      "395/3300 (epoch 11), train_loss = -1.987, time/batch = 0.012\n",
      "396/3300 (epoch 12), train_loss = 0.557, time/batch = 0.008\n",
      "397/3300 (epoch 12), train_loss = -0.591, time/batch = 0.004\n",
      "398/3300 (epoch 12), train_loss = -1.461, time/batch = 0.007\n",
      "399/3300 (epoch 12), train_loss = -1.898, time/batch = 0.004\n",
      "400/3300 (epoch 12), train_loss = 2.098, time/batch = 0.004\n",
      "401/3300 (epoch 12), train_loss = 2.981, time/batch = 0.006\n",
      "402/3300 (epoch 12), train_loss = 3.174, time/batch = 0.009\n",
      "403/3300 (epoch 12), train_loss = 0.111, time/batch = 0.007\n",
      "404/3300 (epoch 12), train_loss = 0.130, time/batch = 0.010\n",
      "405/3300 (epoch 12), train_loss = 0.516, time/batch = 0.004\n",
      "406/3300 (epoch 12), train_loss = -1.301, time/batch = 0.006\n",
      "407/3300 (epoch 12), train_loss = -1.737, time/batch = 0.004\n",
      "408/3300 (epoch 12), train_loss = -1.465, time/batch = 0.004\n",
      "409/3300 (epoch 12), train_loss = -1.135, time/batch = 0.004\n",
      "410/3300 (epoch 12), train_loss = -1.830, time/batch = 0.010\n",
      "411/3300 (epoch 12), train_loss = -1.197, time/batch = 0.013\n",
      "412/3300 (epoch 12), train_loss = -1.731, time/batch = 0.007\n",
      "413/3300 (epoch 12), train_loss = -1.604, time/batch = 0.004\n",
      "414/3300 (epoch 12), train_loss = -1.551, time/batch = 0.004\n",
      "415/3300 (epoch 12), train_loss = -0.836, time/batch = 0.005\n",
      "416/3300 (epoch 12), train_loss = -1.308, time/batch = 0.004\n",
      "417/3300 (epoch 12), train_loss = -1.422, time/batch = 0.009\n",
      "418/3300 (epoch 12), train_loss = -1.727, time/batch = 0.007\n",
      "419/3300 (epoch 12), train_loss = -2.105, time/batch = 0.004\n",
      "420/3300 (epoch 12), train_loss = -2.207, time/batch = 0.006\n",
      "421/3300 (epoch 12), train_loss = -1.401, time/batch = 0.004\n",
      "422/3300 (epoch 12), train_loss = -2.537, time/batch = 0.004\n",
      "423/3300 (epoch 12), train_loss = -0.491, time/batch = 0.004\n",
      "424/3300 (epoch 12), train_loss = -2.507, time/batch = 0.004\n",
      "425/3300 (epoch 12), train_loss = -0.515, time/batch = 0.004\n",
      "426/3300 (epoch 12), train_loss = -2.208, time/batch = 0.006\n",
      "427/3300 (epoch 12), train_loss = -1.245, time/batch = 0.009\n",
      "428/3300 (epoch 12), train_loss = -2.176, time/batch = 0.007\n",
      "429/3300 (epoch 13), train_loss = 0.031, time/batch = 0.004\n",
      "430/3300 (epoch 13), train_loss = -0.341, time/batch = 0.008\n",
      "431/3300 (epoch 13), train_loss = -1.044, time/batch = 0.011\n",
      "432/3300 (epoch 13), train_loss = -1.305, time/batch = 0.008\n",
      "433/3300 (epoch 13), train_loss = -1.134, time/batch = 0.005\n",
      "434/3300 (epoch 13), train_loss = -0.095, time/batch = 0.005\n",
      "435/3300 (epoch 13), train_loss = -0.028, time/batch = 0.005\n",
      "436/3300 (epoch 13), train_loss = -1.524, time/batch = 0.011\n",
      "437/3300 (epoch 13), train_loss = -1.384, time/batch = 0.005\n",
      "438/3300 (epoch 13), train_loss = -2.079, time/batch = 0.005\n",
      "439/3300 (epoch 13), train_loss = -0.218, time/batch = 0.008\n",
      "440/3300 (epoch 13), train_loss = -2.078, time/batch = 0.008\n",
      "441/3300 (epoch 13), train_loss = -0.810, time/batch = 0.008\n",
      "442/3300 (epoch 13), train_loss = -2.523, time/batch = 0.007\n",
      "443/3300 (epoch 13), train_loss = -0.706, time/batch = 0.005\n",
      "444/3300 (epoch 13), train_loss = -2.213, time/batch = 0.004\n",
      "445/3300 (epoch 13), train_loss = -0.718, time/batch = 0.009\n",
      "446/3300 (epoch 13), train_loss = -2.366, time/batch = 0.008\n",
      "447/3300 (epoch 13), train_loss = -0.689, time/batch = 0.005\n",
      "448/3300 (epoch 13), train_loss = -2.475, time/batch = 0.012\n",
      "449/3300 (epoch 13), train_loss = 1.687, time/batch = 0.004\n",
      "450/3300 (epoch 13), train_loss = -2.559, time/batch = 0.011\n",
      "451/3300 (epoch 13), train_loss = -0.629, time/batch = 0.008\n",
      "452/3300 (epoch 13), train_loss = -2.455, time/batch = 0.009\n",
      "453/3300 (epoch 13), train_loss = 0.487, time/batch = 0.004\n",
      "454/3300 (epoch 13), train_loss = -2.317, time/batch = 0.004\n",
      "455/3300 (epoch 13), train_loss = -1.254, time/batch = 0.004\n",
      "456/3300 (epoch 13), train_loss = -2.315, time/batch = 0.004\n",
      "457/3300 (epoch 13), train_loss = -1.775, time/batch = 0.007\n",
      "458/3300 (epoch 13), train_loss = -2.068, time/batch = 0.010\n",
      "459/3300 (epoch 13), train_loss = -1.680, time/batch = 0.005\n",
      "460/3300 (epoch 13), train_loss = -2.272, time/batch = 0.006\n",
      "461/3300 (epoch 13), train_loss = -0.666, time/batch = 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/3300 (epoch 14), train_loss = 0.224, time/batch = 0.019\n",
      "463/3300 (epoch 14), train_loss = -0.883, time/batch = 0.005\n",
      "464/3300 (epoch 14), train_loss = -1.365, time/batch = 0.004\n",
      "465/3300 (epoch 14), train_loss = -0.945, time/batch = 0.004\n",
      "466/3300 (epoch 14), train_loss = -1.749, time/batch = 0.008\n",
      "467/3300 (epoch 14), train_loss = -0.475, time/batch = 0.009\n",
      "468/3300 (epoch 14), train_loss = -0.915, time/batch = 0.011\n",
      "469/3300 (epoch 14), train_loss = -1.270, time/batch = 0.008\n",
      "470/3300 (epoch 14), train_loss = -1.962, time/batch = 0.009\n",
      "471/3300 (epoch 14), train_loss = -1.381, time/batch = 0.005\n",
      "472/3300 (epoch 14), train_loss = -1.315, time/batch = 0.014\n",
      "473/3300 (epoch 14), train_loss = -1.698, time/batch = 0.005\n",
      "474/3300 (epoch 14), train_loss = -1.381, time/batch = 0.013\n",
      "475/3300 (epoch 14), train_loss = -1.477, time/batch = 0.004\n",
      "476/3300 (epoch 14), train_loss = -2.218, time/batch = 0.012\n",
      "477/3300 (epoch 14), train_loss = -1.852, time/batch = 0.015\n",
      "478/3300 (epoch 14), train_loss = -1.921, time/batch = 0.006\n",
      "479/3300 (epoch 14), train_loss = -1.574, time/batch = 0.013\n",
      "480/3300 (epoch 14), train_loss = -2.187, time/batch = 0.011\n",
      "481/3300 (epoch 14), train_loss = 0.801, time/batch = 0.006\n",
      "482/3300 (epoch 14), train_loss = -2.164, time/batch = 0.009\n",
      "483/3300 (epoch 14), train_loss = -0.355, time/batch = 0.005\n",
      "484/3300 (epoch 14), train_loss = -2.196, time/batch = 0.005\n",
      "485/3300 (epoch 14), train_loss = -1.314, time/batch = 0.013\n",
      "486/3300 (epoch 14), train_loss = -2.967, time/batch = 0.005\n",
      "487/3300 (epoch 14), train_loss = 2.593, time/batch = 0.012\n",
      "488/3300 (epoch 14), train_loss = -3.495, time/batch = 0.009\n",
      "489/3300 (epoch 14), train_loss = 6.985, time/batch = 0.005\n",
      "490/3300 (epoch 14), train_loss = -2.440, time/batch = 0.007\n",
      "491/3300 (epoch 14), train_loss = 5.046, time/batch = 0.008\n",
      "492/3300 (epoch 14), train_loss = -2.657, time/batch = 0.004\n",
      "493/3300 (epoch 14), train_loss = 1.858, time/batch = 0.004\n",
      "494/3300 (epoch 14), train_loss = -2.694, time/batch = 0.013\n",
      "495/3300 (epoch 15), train_loss = 0.066, time/batch = 0.005\n",
      "496/3300 (epoch 15), train_loss = -0.842, time/batch = 0.015\n",
      "497/3300 (epoch 15), train_loss = -1.746, time/batch = 0.005\n",
      "498/3300 (epoch 15), train_loss = -1.327, time/batch = 0.007\n",
      "499/3300 (epoch 15), train_loss = 0.328, time/batch = 0.007\n",
      "500/3300 (epoch 15), train_loss = -2.775, time/batch = 0.009\n",
      "501/3300 (epoch 15), train_loss = 10.297, time/batch = 0.011\n",
      "502/3300 (epoch 15), train_loss = -2.860, time/batch = 0.010\n",
      "503/3300 (epoch 15), train_loss = 5.287, time/batch = 0.011\n",
      "504/3300 (epoch 15), train_loss = -2.881, time/batch = 0.006\n",
      "505/3300 (epoch 15), train_loss = 6.385, time/batch = 0.006\n",
      "506/3300 (epoch 15), train_loss = -2.917, time/batch = 0.006\n",
      "507/3300 (epoch 15), train_loss = 6.442, time/batch = 0.005\n",
      "508/3300 (epoch 15), train_loss = -2.945, time/batch = 0.008\n",
      "509/3300 (epoch 15), train_loss = 5.133, time/batch = 0.005\n",
      "510/3300 (epoch 15), train_loss = -3.031, time/batch = 0.004\n",
      "511/3300 (epoch 15), train_loss = 1.326, time/batch = 0.004\n",
      "512/3300 (epoch 15), train_loss = -2.579, time/batch = 0.013\n",
      "513/3300 (epoch 15), train_loss = 0.334, time/batch = 0.004\n",
      "514/3300 (epoch 15), train_loss = -2.447, time/batch = 0.009\n",
      "515/3300 (epoch 15), train_loss = -0.502, time/batch = 0.008\n",
      "516/3300 (epoch 15), train_loss = -2.414, time/batch = 0.009\n",
      "517/3300 (epoch 15), train_loss = -0.959, time/batch = 0.004\n",
      "518/3300 (epoch 15), train_loss = -2.813, time/batch = 0.014\n",
      "519/3300 (epoch 15), train_loss = -0.068, time/batch = 0.004\n",
      "520/3300 (epoch 15), train_loss = -2.714, time/batch = 0.013\n",
      "521/3300 (epoch 15), train_loss = -0.009, time/batch = 0.005\n",
      "522/3300 (epoch 15), train_loss = -2.946, time/batch = 0.012\n",
      "523/3300 (epoch 15), train_loss = -0.303, time/batch = 0.004\n",
      "524/3300 (epoch 15), train_loss = -3.139, time/batch = 0.010\n",
      "525/3300 (epoch 15), train_loss = 1.979, time/batch = 0.014\n",
      "526/3300 (epoch 15), train_loss = -2.880, time/batch = 0.009\n",
      "527/3300 (epoch 15), train_loss = 0.423, time/batch = 0.009\n",
      "528/3300 (epoch 16), train_loss = -0.073, time/batch = 0.004\n",
      "529/3300 (epoch 16), train_loss = -1.090, time/batch = 0.004\n",
      "530/3300 (epoch 16), train_loss = -1.937, time/batch = 0.013\n",
      "531/3300 (epoch 16), train_loss = -1.386, time/batch = 0.005\n",
      "532/3300 (epoch 16), train_loss = 0.738, time/batch = 0.015\n",
      "533/3300 (epoch 16), train_loss = -0.551, time/batch = 0.010\n",
      "534/3300 (epoch 16), train_loss = 5.417, time/batch = 0.005\n",
      "535/3300 (epoch 16), train_loss = -2.504, time/batch = 0.014\n",
      "536/3300 (epoch 16), train_loss = 0.578, time/batch = 0.004\n",
      "537/3300 (epoch 16), train_loss = -2.596, time/batch = 0.008\n",
      "538/3300 (epoch 16), train_loss = 0.099, time/batch = 0.004\n",
      "539/3300 (epoch 16), train_loss = -2.874, time/batch = 0.004\n",
      "540/3300 (epoch 16), train_loss = 1.270, time/batch = 0.004\n",
      "541/3300 (epoch 16), train_loss = -3.020, time/batch = 0.005\n",
      "542/3300 (epoch 16), train_loss = -0.690, time/batch = 0.005\n",
      "543/3300 (epoch 16), train_loss = -2.765, time/batch = 0.013\n",
      "544/3300 (epoch 16), train_loss = 0.332, time/batch = 0.006\n",
      "545/3300 (epoch 16), train_loss = -2.548, time/batch = 0.004\n",
      "546/3300 (epoch 16), train_loss = 0.640, time/batch = 0.005\n",
      "547/3300 (epoch 16), train_loss = -2.299, time/batch = 0.004\n",
      "548/3300 (epoch 16), train_loss = -1.357, time/batch = 0.014\n",
      "549/3300 (epoch 16), train_loss = -2.364, time/batch = 0.004\n",
      "550/3300 (epoch 16), train_loss = 0.070, time/batch = 0.014\n",
      "551/3300 (epoch 16), train_loss = -2.606, time/batch = 0.004\n",
      "552/3300 (epoch 16), train_loss = -0.332, time/batch = 0.012\n",
      "553/3300 (epoch 16), train_loss = -3.097, time/batch = 0.016\n",
      "554/3300 (epoch 16), train_loss = 1.555, time/batch = 0.005\n",
      "555/3300 (epoch 16), train_loss = -3.324, time/batch = 0.012\n",
      "556/3300 (epoch 16), train_loss = -1.570, time/batch = 0.013\n",
      "557/3300 (epoch 16), train_loss = -0.790, time/batch = 0.008\n",
      "558/3300 (epoch 16), train_loss = -0.544, time/batch = 0.010\n",
      "559/3300 (epoch 16), train_loss = -2.437, time/batch = 0.005\n",
      "560/3300 (epoch 16), train_loss = -1.468, time/batch = 0.014\n",
      "561/3300 (epoch 17), train_loss = -0.106, time/batch = 0.005\n",
      "562/3300 (epoch 17), train_loss = -1.069, time/batch = 0.013\n",
      "563/3300 (epoch 17), train_loss = -1.922, time/batch = 0.011\n",
      "564/3300 (epoch 17), train_loss = -1.641, time/batch = 0.008\n",
      "565/3300 (epoch 17), train_loss = -0.227, time/batch = 0.014\n",
      "566/3300 (epoch 17), train_loss = -2.112, time/batch = 0.005\n",
      "567/3300 (epoch 17), train_loss = 0.447, time/batch = 0.012\n",
      "568/3300 (epoch 17), train_loss = -2.414, time/batch = 0.016\n",
      "569/3300 (epoch 17), train_loss = 0.150, time/batch = 0.012\n",
      "570/3300 (epoch 17), train_loss = -2.601, time/batch = 0.007\n",
      "571/3300 (epoch 17), train_loss = 0.576, time/batch = 0.007\n",
      "572/3300 (epoch 17), train_loss = -2.818, time/batch = 0.027\n",
      "573/3300 (epoch 17), train_loss = 2.818, time/batch = 0.005\n",
      "574/3300 (epoch 17), train_loss = -2.872, time/batch = 0.007\n",
      "575/3300 (epoch 17), train_loss = 1.087, time/batch = 0.026\n",
      "576/3300 (epoch 17), train_loss = -2.305, time/batch = 0.007\n",
      "577/3300 (epoch 17), train_loss = 0.121, time/batch = 0.009\n",
      "578/3300 (epoch 17), train_loss = -2.963, time/batch = 0.006\n",
      "579/3300 (epoch 17), train_loss = -0.656, time/batch = 0.005\n",
      "580/3300 (epoch 17), train_loss = -2.668, time/batch = 0.009\n",
      "581/3300 (epoch 17), train_loss = -1.521, time/batch = 0.012\n",
      "582/3300 (epoch 17), train_loss = -2.703, time/batch = 0.009\n",
      "583/3300 (epoch 17), train_loss = -0.237, time/batch = 0.007\n",
      "584/3300 (epoch 17), train_loss = -2.951, time/batch = 0.010\n",
      "585/3300 (epoch 17), train_loss = -1.595, time/batch = 0.007\n",
      "586/3300 (epoch 17), train_loss = -2.974, time/batch = 0.006\n",
      "587/3300 (epoch 17), train_loss = -1.678, time/batch = 0.021\n",
      "588/3300 (epoch 17), train_loss = -3.057, time/batch = 0.007\n",
      "589/3300 (epoch 17), train_loss = -1.959, time/batch = 0.004\n",
      "590/3300 (epoch 17), train_loss = -3.064, time/batch = 0.011\n",
      "591/3300 (epoch 17), train_loss = -1.681, time/batch = 0.012\n",
      "592/3300 (epoch 17), train_loss = -2.891, time/batch = 0.005\n",
      "593/3300 (epoch 17), train_loss = 0.321, time/batch = 0.004\n",
      "594/3300 (epoch 18), train_loss = -0.095, time/batch = 0.008\n",
      "595/3300 (epoch 18), train_loss = -1.100, time/batch = 0.007\n",
      "596/3300 (epoch 18), train_loss = -1.858, time/batch = 0.023\n",
      "597/3300 (epoch 18), train_loss = -1.340, time/batch = 0.011\n",
      "598/3300 (epoch 18), train_loss = -1.533, time/batch = 0.006\n",
      "599/3300 (epoch 18), train_loss = -2.173, time/batch = 0.004\n",
      "600/3300 (epoch 18), train_loss = 0.535, time/batch = 0.021\n",
      "601/3300 (epoch 18), train_loss = -1.575, time/batch = 0.014\n",
      "602/3300 (epoch 18), train_loss = -1.540, time/batch = 0.005\n",
      "603/3300 (epoch 18), train_loss = -2.395, time/batch = 0.012\n",
      "604/3300 (epoch 18), train_loss = -1.795, time/batch = 0.008\n",
      "605/3300 (epoch 18), train_loss = -2.632, time/batch = 0.005\n",
      "606/3300 (epoch 18), train_loss = -1.938, time/batch = 0.005\n",
      "607/3300 (epoch 18), train_loss = -2.308, time/batch = 0.019\n",
      "608/3300 (epoch 18), train_loss = -1.481, time/batch = 0.005\n",
      "609/3300 (epoch 18), train_loss = -0.215, time/batch = 0.004\n",
      "610/3300 (epoch 18), train_loss = -1.874, time/batch = 0.004\n",
      "611/3300 (epoch 18), train_loss = -1.993, time/batch = 0.014\n",
      "612/3300 (epoch 18), train_loss = -2.479, time/batch = 0.005\n",
      "613/3300 (epoch 18), train_loss = -1.947, time/batch = 0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/3300 (epoch 18), train_loss = -2.301, time/batch = 0.008\n",
      "615/3300 (epoch 18), train_loss = -2.223, time/batch = 0.007\n",
      "616/3300 (epoch 18), train_loss = -2.761, time/batch = 0.010\n",
      "617/3300 (epoch 18), train_loss = -2.460, time/batch = 0.004\n",
      "618/3300 (epoch 18), train_loss = -1.831, time/batch = 0.007\n",
      "619/3300 (epoch 18), train_loss = -2.055, time/batch = 0.004\n",
      "620/3300 (epoch 18), train_loss = -2.490, time/batch = 0.022\n",
      "621/3300 (epoch 18), train_loss = -1.777, time/batch = 0.005\n",
      "622/3300 (epoch 18), train_loss = -2.769, time/batch = 0.005\n",
      "623/3300 (epoch 18), train_loss = -2.347, time/batch = 0.004\n",
      "624/3300 (epoch 18), train_loss = -2.509, time/batch = 0.019\n",
      "625/3300 (epoch 18), train_loss = -2.511, time/batch = 0.007\n",
      "626/3300 (epoch 18), train_loss = -2.287, time/batch = 0.004\n",
      "627/3300 (epoch 19), train_loss = -0.244, time/batch = 0.006\n",
      "628/3300 (epoch 19), train_loss = -0.984, time/batch = 0.004\n",
      "629/3300 (epoch 19), train_loss = -1.846, time/batch = 0.012\n",
      "630/3300 (epoch 19), train_loss = -2.399, time/batch = 0.004\n",
      "631/3300 (epoch 19), train_loss = 1.716, time/batch = 0.021\n",
      "632/3300 (epoch 19), train_loss = -2.345, time/batch = 0.015\n",
      "633/3300 (epoch 19), train_loss = 3.792, time/batch = 0.005\n",
      "634/3300 (epoch 19), train_loss = -2.575, time/batch = 0.013\n",
      "635/3300 (epoch 19), train_loss = 1.953, time/batch = 0.007\n",
      "636/3300 (epoch 19), train_loss = -2.621, time/batch = 0.005\n",
      "637/3300 (epoch 19), train_loss = -1.188, time/batch = 0.004\n",
      "638/3300 (epoch 19), train_loss = -2.738, time/batch = 0.019\n",
      "639/3300 (epoch 19), train_loss = 0.682, time/batch = 0.011\n",
      "640/3300 (epoch 19), train_loss = -2.912, time/batch = 0.007\n",
      "641/3300 (epoch 19), train_loss = 0.262, time/batch = 0.010\n",
      "642/3300 (epoch 19), train_loss = -3.112, time/batch = 0.006\n",
      "643/3300 (epoch 19), train_loss = 0.211, time/batch = 0.007\n",
      "644/3300 (epoch 19), train_loss = -2.738, time/batch = 0.005\n",
      "645/3300 (epoch 19), train_loss = -0.969, time/batch = 0.020\n",
      "646/3300 (epoch 19), train_loss = -2.831, time/batch = 0.004\n",
      "647/3300 (epoch 19), train_loss = -1.507, time/batch = 0.004\n",
      "648/3300 (epoch 19), train_loss = -2.593, time/batch = 0.014\n",
      "649/3300 (epoch 19), train_loss = -2.075, time/batch = 0.008\n",
      "650/3300 (epoch 19), train_loss = -2.898, time/batch = 0.020\n",
      "651/3300 (epoch 19), train_loss = -2.399, time/batch = 0.007\n",
      "652/3300 (epoch 19), train_loss = -2.754, time/batch = 0.011\n",
      "653/3300 (epoch 19), train_loss = -1.751, time/batch = 0.017\n",
      "654/3300 (epoch 19), train_loss = -2.824, time/batch = 0.006\n",
      "655/3300 (epoch 19), train_loss = -2.387, time/batch = 0.004\n",
      "656/3300 (epoch 19), train_loss = -3.216, time/batch = 0.021\n",
      "657/3300 (epoch 19), train_loss = -2.595, time/batch = 0.005\n",
      "658/3300 (epoch 19), train_loss = -2.192, time/batch = 0.004\n",
      "659/3300 (epoch 19), train_loss = -2.169, time/batch = 0.004\n",
      "660/3300 (epoch 20), train_loss = -0.265, time/batch = 0.007\n",
      "661/3300 (epoch 20), train_loss = -1.040, time/batch = 0.005\n",
      "662/3300 (epoch 20), train_loss = -2.022, time/batch = 0.020\n",
      "663/3300 (epoch 20), train_loss = -2.250, time/batch = 0.014\n",
      "664/3300 (epoch 20), train_loss = -3.245, time/batch = 0.005\n",
      "665/3300 (epoch 20), train_loss = 2.027, time/batch = 0.012\n",
      "666/3300 (epoch 20), train_loss = 6.883, time/batch = 0.004\n",
      "667/3300 (epoch 20), train_loss = -0.794, time/batch = 0.006\n",
      "668/3300 (epoch 20), train_loss = 2.290, time/batch = 0.010\n",
      "669/3300 (epoch 20), train_loss = -2.300, time/batch = 0.007\n",
      "670/3300 (epoch 20), train_loss = 1.030, time/batch = 0.004\n",
      "671/3300 (epoch 20), train_loss = -2.305, time/batch = 0.020\n",
      "672/3300 (epoch 20), train_loss = -0.634, time/batch = 0.004\n",
      "673/3300 (epoch 20), train_loss = -3.012, time/batch = 0.012\n",
      "674/3300 (epoch 20), train_loss = -1.561, time/batch = 0.004\n",
      "675/3300 (epoch 20), train_loss = -2.806, time/batch = 0.012\n",
      "676/3300 (epoch 20), train_loss = -0.194, time/batch = 0.004\n",
      "677/3300 (epoch 20), train_loss = -2.873, time/batch = 0.006\n",
      "678/3300 (epoch 20), train_loss = -1.171, time/batch = 0.004\n",
      "679/3300 (epoch 20), train_loss = -2.639, time/batch = 0.023\n",
      "680/3300 (epoch 20), train_loss = -2.172, time/batch = 0.011\n",
      "681/3300 (epoch 20), train_loss = -2.749, time/batch = 0.007\n",
      "682/3300 (epoch 20), train_loss = -1.993, time/batch = 0.005\n",
      "683/3300 (epoch 20), train_loss = -3.178, time/batch = 0.005\n",
      "684/3300 (epoch 20), train_loss = -2.146, time/batch = 0.013\n",
      "685/3300 (epoch 20), train_loss = -3.354, time/batch = 0.013\n",
      "686/3300 (epoch 20), train_loss = -2.144, time/batch = 0.006\n",
      "687/3300 (epoch 20), train_loss = -3.415, time/batch = 0.020\n",
      "688/3300 (epoch 20), train_loss = -1.189, time/batch = 0.010\n",
      "689/3300 (epoch 20), train_loss = -1.041, time/batch = 0.007\n",
      "690/3300 (epoch 20), train_loss = -2.352, time/batch = 0.015\n",
      "691/3300 (epoch 20), train_loss = -3.054, time/batch = 0.004\n",
      "692/3300 (epoch 20), train_loss = -2.830, time/batch = 0.006\n",
      "693/3300 (epoch 21), train_loss = -0.423, time/batch = 0.014\n",
      "694/3300 (epoch 21), train_loss = -1.087, time/batch = 0.005\n",
      "695/3300 (epoch 21), train_loss = -1.986, time/batch = 0.008\n",
      "696/3300 (epoch 21), train_loss = -0.455, time/batch = 0.009\n",
      "697/3300 (epoch 21), train_loss = -1.688, time/batch = 0.013\n",
      "698/3300 (epoch 21), train_loss = -2.205, time/batch = 0.013\n",
      "699/3300 (epoch 21), train_loss = -1.151, time/batch = 0.016\n",
      "700/3300 (epoch 21), train_loss = -2.203, time/batch = 0.004\n",
      "701/3300 (epoch 21), train_loss = -1.450, time/batch = 0.018\n",
      "702/3300 (epoch 21), train_loss = -2.597, time/batch = 0.008\n",
      "703/3300 (epoch 21), train_loss = -1.057, time/batch = 0.006\n",
      "704/3300 (epoch 21), train_loss = -2.807, time/batch = 0.016\n",
      "705/3300 (epoch 21), train_loss = -2.392, time/batch = 0.004\n",
      "706/3300 (epoch 21), train_loss = -2.864, time/batch = 0.007\n",
      "707/3300 (epoch 21), train_loss = -2.102, time/batch = 0.007\n",
      "708/3300 (epoch 21), train_loss = -3.119, time/batch = 0.011\n",
      "709/3300 (epoch 21), train_loss = -2.335, time/batch = 0.004\n",
      "710/3300 (epoch 21), train_loss = -0.706, time/batch = 0.004\n",
      "711/3300 (epoch 21), train_loss = -2.423, time/batch = 0.017\n",
      "712/3300 (epoch 21), train_loss = -1.607, time/batch = 0.008\n",
      "713/3300 (epoch 21), train_loss = -2.520, time/batch = 0.004\n",
      "714/3300 (epoch 21), train_loss = -2.467, time/batch = 0.013\n",
      "715/3300 (epoch 21), train_loss = -2.760, time/batch = 0.005\n",
      "716/3300 (epoch 21), train_loss = -1.889, time/batch = 0.024\n",
      "717/3300 (epoch 21), train_loss = -3.408, time/batch = 0.005\n",
      "718/3300 (epoch 21), train_loss = -1.174, time/batch = 0.005\n",
      "719/3300 (epoch 21), train_loss = -3.376, time/batch = 0.019\n",
      "720/3300 (epoch 21), train_loss = 1.356, time/batch = 0.014\n",
      "721/3300 (epoch 21), train_loss = -3.211, time/batch = 0.016\n",
      "722/3300 (epoch 21), train_loss = -0.989, time/batch = 0.005\n",
      "723/3300 (epoch 21), train_loss = -3.482, time/batch = 0.015\n",
      "724/3300 (epoch 21), train_loss = -0.346, time/batch = 0.013\n",
      "725/3300 (epoch 21), train_loss = -3.120, time/batch = 0.010\n",
      "726/3300 (epoch 22), train_loss = -0.401, time/batch = 0.007\n",
      "727/3300 (epoch 22), train_loss = -1.175, time/batch = 0.004\n",
      "728/3300 (epoch 22), train_loss = -2.225, time/batch = 0.013\n",
      "729/3300 (epoch 22), train_loss = -1.152, time/batch = 0.004\n",
      "730/3300 (epoch 22), train_loss = -1.700, time/batch = 0.004\n",
      "731/3300 (epoch 22), train_loss = -1.344, time/batch = 0.010\n",
      "732/3300 (epoch 22), train_loss = -1.560, time/batch = 0.018\n",
      "733/3300 (epoch 22), train_loss = -2.441, time/batch = 0.013\n",
      "734/3300 (epoch 22), train_loss = -2.188, time/batch = 0.004\n",
      "735/3300 (epoch 22), train_loss = -2.161, time/batch = 0.004\n",
      "736/3300 (epoch 22), train_loss = -1.479, time/batch = 0.016\n",
      "737/3300 (epoch 22), train_loss = -2.867, time/batch = 0.014\n",
      "738/3300 (epoch 22), train_loss = -1.241, time/batch = 0.016\n",
      "739/3300 (epoch 22), train_loss = -2.540, time/batch = 0.004\n",
      "740/3300 (epoch 22), train_loss = -1.711, time/batch = 0.016\n",
      "741/3300 (epoch 22), train_loss = -2.854, time/batch = 0.013\n",
      "742/3300 (epoch 22), train_loss = -0.739, time/batch = 0.017\n",
      "743/3300 (epoch 22), train_loss = -2.752, time/batch = 0.004\n",
      "744/3300 (epoch 22), train_loss = -2.217, time/batch = 0.011\n",
      "745/3300 (epoch 22), train_loss = -3.005, time/batch = 0.004\n",
      "746/3300 (epoch 22), train_loss = -1.345, time/batch = 0.006\n",
      "747/3300 (epoch 22), train_loss = -2.817, time/batch = 0.015\n",
      "748/3300 (epoch 22), train_loss = -1.809, time/batch = 0.026\n",
      "749/3300 (epoch 22), train_loss = -3.718, time/batch = 0.015\n",
      "750/3300 (epoch 22), train_loss = 0.227, time/batch = 0.016\n",
      "751/3300 (epoch 22), train_loss = -3.529, time/batch = 0.004\n",
      "752/3300 (epoch 22), train_loss = 1.569, time/batch = 0.007\n",
      "753/3300 (epoch 22), train_loss = -3.306, time/batch = 0.005\n",
      "754/3300 (epoch 22), train_loss = -0.179, time/batch = 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755/3300 (epoch 22), train_loss = -3.748, time/batch = 0.015\n",
      "756/3300 (epoch 22), train_loss = 1.276, time/batch = 0.004\n",
      "757/3300 (epoch 22), train_loss = -3.375, time/batch = 0.015\n",
      "758/3300 (epoch 22), train_loss = 1.506, time/batch = 0.016\n",
      "759/3300 (epoch 23), train_loss = -0.468, time/batch = 0.005\n",
      "760/3300 (epoch 23), train_loss = -1.372, time/batch = 0.017\n",
      "761/3300 (epoch 23), train_loss = -2.225, time/batch = 0.006\n",
      "762/3300 (epoch 23), train_loss = -2.004, time/batch = 0.004\n",
      "763/3300 (epoch 23), train_loss = 0.411, time/batch = 0.018\n",
      "764/3300 (epoch 23), train_loss = -2.929, time/batch = 0.017\n",
      "765/3300 (epoch 23), train_loss = 0.827, time/batch = 0.005\n",
      "766/3300 (epoch 23), train_loss = -1.650, time/batch = 0.007\n",
      "767/3300 (epoch 23), train_loss = -1.749, time/batch = 0.004\n",
      "768/3300 (epoch 23), train_loss = -3.386, time/batch = 0.004\n",
      "769/3300 (epoch 23), train_loss = -0.476, time/batch = 0.013\n",
      "770/3300 (epoch 23), train_loss = -3.240, time/batch = 0.016\n",
      "771/3300 (epoch 23), train_loss = 0.139, time/batch = 0.018\n",
      "772/3300 (epoch 23), train_loss = -3.123, time/batch = 0.027\n",
      "773/3300 (epoch 23), train_loss = 0.402, time/batch = 0.005\n",
      "774/3300 (epoch 23), train_loss = -2.981, time/batch = 0.017\n",
      "775/3300 (epoch 23), train_loss = -0.348, time/batch = 0.004\n",
      "776/3300 (epoch 23), train_loss = -3.223, time/batch = 0.004\n",
      "777/3300 (epoch 23), train_loss = -1.678, time/batch = 0.006\n",
      "778/3300 (epoch 23), train_loss = -3.101, time/batch = 0.005\n",
      "779/3300 (epoch 23), train_loss = -0.792, time/batch = 0.013\n",
      "780/3300 (epoch 23), train_loss = -3.001, time/batch = 0.007\n",
      "781/3300 (epoch 23), train_loss = -1.310, time/batch = 0.012\n",
      "782/3300 (epoch 23), train_loss = -3.221, time/batch = 0.006\n",
      "783/3300 (epoch 23), train_loss = -0.333, time/batch = 0.006\n",
      "784/3300 (epoch 23), train_loss = -3.117, time/batch = 0.004\n",
      "785/3300 (epoch 23), train_loss = -0.489, time/batch = 0.004\n",
      "786/3300 (epoch 23), train_loss = -3.046, time/batch = 0.006\n",
      "787/3300 (epoch 23), train_loss = -1.977, time/batch = 0.004\n",
      "788/3300 (epoch 23), train_loss = -3.261, time/batch = 0.004\n",
      "789/3300 (epoch 23), train_loss = -1.119, time/batch = 0.006\n",
      "790/3300 (epoch 23), train_loss = -3.082, time/batch = 0.004\n",
      "791/3300 (epoch 23), train_loss = -1.379, time/batch = 0.007\n",
      "792/3300 (epoch 24), train_loss = -0.478, time/batch = 0.004\n",
      "793/3300 (epoch 24), train_loss = -1.208, time/batch = 0.004\n",
      "794/3300 (epoch 24), train_loss = -2.269, time/batch = 0.004\n",
      "795/3300 (epoch 24), train_loss = -2.578, time/batch = 0.015\n",
      "796/3300 (epoch 24), train_loss = -0.874, time/batch = 0.009\n",
      "797/3300 (epoch 24), train_loss = -3.007, time/batch = 0.004\n",
      "798/3300 (epoch 24), train_loss = 5.282, time/batch = 0.004\n",
      "799/3300 (epoch 24), train_loss = -3.137, time/batch = 0.004\n",
      "800/3300 (epoch 24), train_loss = 3.094, time/batch = 0.004\n",
      "801/3300 (epoch 24), train_loss = -3.156, time/batch = 0.004\n",
      "802/3300 (epoch 24), train_loss = 0.514, time/batch = 0.005\n",
      "803/3300 (epoch 24), train_loss = -2.754, time/batch = 0.005\n",
      "804/3300 (epoch 24), train_loss = -0.645, time/batch = 0.005\n",
      "805/3300 (epoch 24), train_loss = -3.393, time/batch = 0.005\n",
      "806/3300 (epoch 24), train_loss = -0.460, time/batch = 0.004\n",
      "807/3300 (epoch 24), train_loss = -3.459, time/batch = 0.004\n",
      "808/3300 (epoch 24), train_loss = -0.760, time/batch = 0.005\n",
      "809/3300 (epoch 24), train_loss = -2.775, time/batch = 0.012\n",
      "810/3300 (epoch 24), train_loss = -1.494, time/batch = 0.007\n",
      "811/3300 (epoch 24), train_loss = -2.852, time/batch = 0.016\n",
      "812/3300 (epoch 24), train_loss = -2.836, time/batch = 0.008\n",
      "813/3300 (epoch 24), train_loss = -2.694, time/batch = 0.005\n",
      "814/3300 (epoch 24), train_loss = -2.531, time/batch = 0.004\n",
      "815/3300 (epoch 24), train_loss = -3.266, time/batch = 0.004\n",
      "816/3300 (epoch 24), train_loss = -2.557, time/batch = 0.004\n",
      "817/3300 (epoch 24), train_loss = -3.087, time/batch = 0.004\n",
      "818/3300 (epoch 24), train_loss = -2.080, time/batch = 0.004\n",
      "819/3300 (epoch 24), train_loss = -3.022, time/batch = 0.004\n",
      "820/3300 (epoch 24), train_loss = -2.488, time/batch = 0.005\n",
      "821/3300 (epoch 24), train_loss = -3.406, time/batch = 0.006\n",
      "822/3300 (epoch 24), train_loss = -1.385, time/batch = 0.004\n",
      "823/3300 (epoch 24), train_loss = -2.908, time/batch = 0.005\n",
      "824/3300 (epoch 24), train_loss = -2.334, time/batch = 0.005\n",
      "825/3300 (epoch 25), train_loss = -0.425, time/batch = 0.006\n",
      "826/3300 (epoch 25), train_loss = -1.206, time/batch = 0.005\n",
      "827/3300 (epoch 25), train_loss = -2.219, time/batch = 0.004\n",
      "828/3300 (epoch 25), train_loss = -2.628, time/batch = 0.004\n",
      "829/3300 (epoch 25), train_loss = 0.542, time/batch = 0.004\n",
      "830/3300 (epoch 25), train_loss = -2.719, time/batch = 0.006\n",
      "831/3300 (epoch 25), train_loss = -0.412, time/batch = 0.005\n",
      "832/3300 (epoch 25), train_loss = -2.532, time/batch = 0.016\n",
      "833/3300 (epoch 25), train_loss = 0.403, time/batch = 0.007\n",
      "834/3300 (epoch 25), train_loss = -3.096, time/batch = 0.014\n",
      "835/3300 (epoch 25), train_loss = 0.982, time/batch = 0.007\n",
      "836/3300 (epoch 25), train_loss = -2.950, time/batch = 0.004\n",
      "837/3300 (epoch 25), train_loss = -0.666, time/batch = 0.005\n",
      "838/3300 (epoch 25), train_loss = -2.900, time/batch = 0.004\n",
      "839/3300 (epoch 25), train_loss = -1.751, time/batch = 0.006\n",
      "840/3300 (epoch 25), train_loss = -3.047, time/batch = 0.004\n",
      "841/3300 (epoch 25), train_loss = -1.689, time/batch = 0.004\n",
      "842/3300 (epoch 25), train_loss = -3.164, time/batch = 0.013\n",
      "843/3300 (epoch 25), train_loss = -2.689, time/batch = 0.017\n",
      "844/3300 (epoch 25), train_loss = -2.348, time/batch = 0.019\n",
      "845/3300 (epoch 25), train_loss = -3.041, time/batch = 0.006\n",
      "846/3300 (epoch 25), train_loss = -2.458, time/batch = 0.008\n",
      "847/3300 (epoch 25), train_loss = -2.867, time/batch = 0.004\n",
      "848/3300 (epoch 25), train_loss = -2.885, time/batch = 0.004\n",
      "849/3300 (epoch 25), train_loss = -3.107, time/batch = 0.004\n",
      "850/3300 (epoch 25), train_loss = -2.709, time/batch = 0.004\n",
      "851/3300 (epoch 25), train_loss = -3.225, time/batch = 0.004\n",
      "852/3300 (epoch 25), train_loss = -1.867, time/batch = 0.004\n",
      "853/3300 (epoch 25), train_loss = -2.766, time/batch = 0.004\n",
      "854/3300 (epoch 25), train_loss = -2.676, time/batch = 0.005\n",
      "855/3300 (epoch 25), train_loss = -3.533, time/batch = 0.004\n",
      "856/3300 (epoch 25), train_loss = -2.653, time/batch = 0.005\n",
      "857/3300 (epoch 25), train_loss = -2.919, time/batch = 0.004\n",
      "858/3300 (epoch 26), train_loss = -0.554, time/batch = 0.004\n",
      "859/3300 (epoch 26), train_loss = -1.477, time/batch = 0.005\n",
      "860/3300 (epoch 26), train_loss = -2.267, time/batch = 0.004\n",
      "861/3300 (epoch 26), train_loss = -2.278, time/batch = 0.004\n",
      "862/3300 (epoch 26), train_loss = -0.571, time/batch = 0.004\n",
      "863/3300 (epoch 26), train_loss = -3.162, time/batch = 0.004\n",
      "864/3300 (epoch 26), train_loss = 0.076, time/batch = 0.004\n",
      "865/3300 (epoch 26), train_loss = -2.662, time/batch = 0.004\n",
      "866/3300 (epoch 26), train_loss = -0.725, time/batch = 0.004\n",
      "867/3300 (epoch 26), train_loss = -3.198, time/batch = 0.004\n",
      "868/3300 (epoch 26), train_loss = 0.042, time/batch = 0.014\n",
      "869/3300 (epoch 26), train_loss = -3.300, time/batch = 0.007\n",
      "870/3300 (epoch 26), train_loss = 0.242, time/batch = 0.004\n",
      "871/3300 (epoch 26), train_loss = -3.273, time/batch = 0.004\n",
      "872/3300 (epoch 26), train_loss = -0.239, time/batch = 0.004\n",
      "873/3300 (epoch 26), train_loss = -3.454, time/batch = 0.004\n",
      "874/3300 (epoch 26), train_loss = -0.114, time/batch = 0.004\n",
      "875/3300 (epoch 26), train_loss = -3.083, time/batch = 0.004\n",
      "876/3300 (epoch 26), train_loss = -2.223, time/batch = 0.014\n",
      "877/3300 (epoch 26), train_loss = -3.118, time/batch = 0.017\n",
      "878/3300 (epoch 26), train_loss = -2.309, time/batch = 0.005\n",
      "879/3300 (epoch 26), train_loss = -3.273, time/batch = 0.007\n",
      "880/3300 (epoch 26), train_loss = -2.511, time/batch = 0.005\n",
      "881/3300 (epoch 26), train_loss = -3.055, time/batch = 0.004\n",
      "882/3300 (epoch 26), train_loss = -1.941, time/batch = 0.004\n",
      "883/3300 (epoch 26), train_loss = -3.261, time/batch = 0.004\n",
      "884/3300 (epoch 26), train_loss = -2.825, time/batch = 0.004\n",
      "885/3300 (epoch 26), train_loss = -3.727, time/batch = 0.004\n",
      "886/3300 (epoch 26), train_loss = -2.153, time/batch = 0.004\n",
      "887/3300 (epoch 26), train_loss = -3.337, time/batch = 0.005\n",
      "888/3300 (epoch 26), train_loss = -1.754, time/batch = 0.004\n",
      "889/3300 (epoch 26), train_loss = -3.513, time/batch = 0.004\n",
      "890/3300 (epoch 26), train_loss = -2.442, time/batch = 0.004\n",
      "891/3300 (epoch 27), train_loss = -0.625, time/batch = 0.004\n",
      "892/3300 (epoch 27), train_loss = -1.633, time/batch = 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893/3300 (epoch 27), train_loss = -2.248, time/batch = 0.004\n",
      "894/3300 (epoch 27), train_loss = -2.033, time/batch = 0.004\n",
      "895/3300 (epoch 27), train_loss = -2.748, time/batch = 0.004\n",
      "896/3300 (epoch 27), train_loss = 0.034, time/batch = 0.004\n",
      "897/3300 (epoch 27), train_loss = -2.647, time/batch = 0.004\n",
      "898/3300 (epoch 27), train_loss = -1.450, time/batch = 0.004\n",
      "899/3300 (epoch 27), train_loss = -2.697, time/batch = 0.004\n",
      "900/3300 (epoch 27), train_loss = -2.521, time/batch = 0.004\n",
      "901/3300 (epoch 27), train_loss = -3.187, time/batch = 0.005\n",
      "902/3300 (epoch 27), train_loss = -2.394, time/batch = 0.004\n",
      "903/3300 (epoch 27), train_loss = -3.086, time/batch = 0.004\n",
      "904/3300 (epoch 27), train_loss = -1.813, time/batch = 0.004\n",
      "905/3300 (epoch 27), train_loss = -2.800, time/batch = 0.005\n",
      "906/3300 (epoch 27), train_loss = -1.367, time/batch = 0.004\n",
      "907/3300 (epoch 27), train_loss = -3.108, time/batch = 0.004\n",
      "908/3300 (epoch 27), train_loss = -2.204, time/batch = 0.004\n",
      "909/3300 (epoch 27), train_loss = -3.179, time/batch = 0.004\n",
      "910/3300 (epoch 27), train_loss = -2.345, time/batch = 0.005\n",
      "911/3300 (epoch 27), train_loss = -3.200, time/batch = 0.004\n",
      "912/3300 (epoch 27), train_loss = -2.637, time/batch = 0.005\n",
      "913/3300 (epoch 27), train_loss = -3.331, time/batch = 0.004\n",
      "914/3300 (epoch 27), train_loss = -3.170, time/batch = 0.011\n",
      "915/3300 (epoch 27), train_loss = -3.511, time/batch = 0.007\n",
      "916/3300 (epoch 27), train_loss = -3.055, time/batch = 0.004\n",
      "917/3300 (epoch 27), train_loss = -3.425, time/batch = 0.004\n",
      "918/3300 (epoch 27), train_loss = -2.632, time/batch = 0.004\n",
      "919/3300 (epoch 27), train_loss = -2.529, time/batch = 0.004\n",
      "920/3300 (epoch 27), train_loss = -2.726, time/batch = 0.005\n",
      "921/3300 (epoch 27), train_loss = -3.163, time/batch = 0.004\n",
      "922/3300 (epoch 27), train_loss = -2.996, time/batch = 0.004\n",
      "923/3300 (epoch 27), train_loss = -2.449, time/batch = 0.004\n",
      "924/3300 (epoch 28), train_loss = -0.618, time/batch = 0.004\n",
      "925/3300 (epoch 28), train_loss = -1.384, time/batch = 0.004\n",
      "926/3300 (epoch 28), train_loss = -2.459, time/batch = 0.004\n",
      "927/3300 (epoch 28), train_loss = -2.214, time/batch = 0.004\n",
      "928/3300 (epoch 28), train_loss = -2.599, time/batch = 0.005\n",
      "929/3300 (epoch 28), train_loss = 0.042, time/batch = 0.004\n",
      "930/3300 (epoch 28), train_loss = -3.165, time/batch = 0.004\n",
      "931/3300 (epoch 28), train_loss = 0.238, time/batch = 0.004\n",
      "932/3300 (epoch 28), train_loss = -2.920, time/batch = 0.004\n",
      "933/3300 (epoch 28), train_loss = -1.590, time/batch = 0.004\n",
      "934/3300 (epoch 28), train_loss = -3.292, time/batch = 0.004\n",
      "935/3300 (epoch 28), train_loss = -0.952, time/batch = 0.004\n",
      "936/3300 (epoch 28), train_loss = -3.183, time/batch = 0.004\n",
      "937/3300 (epoch 28), train_loss = 0.085, time/batch = 0.004\n",
      "938/3300 (epoch 28), train_loss = -3.589, time/batch = 0.004\n",
      "939/3300 (epoch 28), train_loss = -0.796, time/batch = 0.004\n",
      "940/3300 (epoch 28), train_loss = -3.465, time/batch = 0.004\n",
      "941/3300 (epoch 28), train_loss = -0.540, time/batch = 0.004\n",
      "942/3300 (epoch 28), train_loss = -3.522, time/batch = 0.005\n",
      "943/3300 (epoch 28), train_loss = -1.453, time/batch = 0.005\n",
      "944/3300 (epoch 28), train_loss = -3.394, time/batch = 0.020\n",
      "945/3300 (epoch 28), train_loss = -1.740, time/batch = 0.017\n",
      "946/3300 (epoch 28), train_loss = -3.311, time/batch = 0.005\n",
      "947/3300 (epoch 28), train_loss = -1.967, time/batch = 0.020\n",
      "948/3300 (epoch 28), train_loss = -3.350, time/batch = 0.005\n",
      "949/3300 (epoch 28), train_loss = -2.586, time/batch = 0.005\n",
      "950/3300 (epoch 28), train_loss = -3.544, time/batch = 0.009\n",
      "951/3300 (epoch 28), train_loss = -1.639, time/batch = 0.005\n",
      "952/3300 (epoch 28), train_loss = -3.577, time/batch = 0.005\n",
      "953/3300 (epoch 28), train_loss = -1.596, time/batch = 0.015\n",
      "954/3300 (epoch 28), train_loss = -4.113, time/batch = 0.016\n",
      "955/3300 (epoch 28), train_loss = -0.770, time/batch = 0.004\n",
      "956/3300 (epoch 28), train_loss = -3.616, time/batch = 0.007\n",
      "957/3300 (epoch 29), train_loss = -0.697, time/batch = 0.028\n",
      "958/3300 (epoch 29), train_loss = -1.159, time/batch = 0.005\n",
      "959/3300 (epoch 29), train_loss = -2.522, time/batch = 0.005\n",
      "960/3300 (epoch 29), train_loss = -2.802, time/batch = 0.007\n",
      "961/3300 (epoch 29), train_loss = -0.899, time/batch = 0.004\n",
      "962/3300 (epoch 29), train_loss = -2.666, time/batch = 0.004\n",
      "963/3300 (epoch 29), train_loss = -0.880, time/batch = 0.004\n",
      "964/3300 (epoch 29), train_loss = -1.839, time/batch = 0.004\n",
      "965/3300 (epoch 29), train_loss = -1.180, time/batch = 0.004\n",
      "966/3300 (epoch 29), train_loss = -2.566, time/batch = 0.005\n",
      "967/3300 (epoch 29), train_loss = -2.749, time/batch = 0.004\n",
      "968/3300 (epoch 29), train_loss = -2.207, time/batch = 0.004\n",
      "969/3300 (epoch 29), train_loss = -2.710, time/batch = 0.004\n",
      "970/3300 (epoch 29), train_loss = -3.137, time/batch = 0.005\n",
      "971/3300 (epoch 29), train_loss = -2.089, time/batch = 0.004\n",
      "972/3300 (epoch 29), train_loss = -3.227, time/batch = 0.004\n",
      "973/3300 (epoch 29), train_loss = -1.618, time/batch = 0.004\n",
      "974/3300 (epoch 29), train_loss = -3.031, time/batch = 0.004\n",
      "975/3300 (epoch 29), train_loss = -2.987, time/batch = 0.004\n",
      "976/3300 (epoch 29), train_loss = -3.265, time/batch = 0.004\n",
      "977/3300 (epoch 29), train_loss = -2.574, time/batch = 0.004\n",
      "978/3300 (epoch 29), train_loss = -3.530, time/batch = 0.004\n",
      "979/3300 (epoch 29), train_loss = -3.364, time/batch = 0.004\n",
      "980/3300 (epoch 29), train_loss = -2.606, time/batch = 0.004\n",
      "981/3300 (epoch 29), train_loss = -3.513, time/batch = 0.004\n",
      "982/3300 (epoch 29), train_loss = -3.419, time/batch = 0.004\n",
      "983/3300 (epoch 29), train_loss = -3.650, time/batch = 0.004\n",
      "984/3300 (epoch 29), train_loss = -2.154, time/batch = 0.004\n",
      "985/3300 (epoch 29), train_loss = -3.109, time/batch = 0.004\n",
      "986/3300 (epoch 29), train_loss = -3.508, time/batch = 0.004\n",
      "987/3300 (epoch 29), train_loss = -3.437, time/batch = 0.004\n",
      "988/3300 (epoch 29), train_loss = -2.713, time/batch = 0.004\n",
      "989/3300 (epoch 29), train_loss = -3.359, time/batch = 0.006\n",
      "990/3300 (epoch 30), train_loss = -0.604, time/batch = 0.005\n",
      "991/3300 (epoch 30), train_loss = -1.690, time/batch = 0.004\n",
      "992/3300 (epoch 30), train_loss = -2.380, time/batch = 0.005\n",
      "993/3300 (epoch 30), train_loss = -2.412, time/batch = 0.005\n",
      "994/3300 (epoch 30), train_loss = -2.724, time/batch = 0.005\n",
      "995/3300 (epoch 30), train_loss = -1.709, time/batch = 0.008\n",
      "996/3300 (epoch 30), train_loss = -2.471, time/batch = 0.006\n",
      "997/3300 (epoch 30), train_loss = -1.862, time/batch = 0.005\n",
      "998/3300 (epoch 30), train_loss = -3.497, time/batch = 0.005\n",
      "999/3300 (epoch 30), train_loss = -0.419, time/batch = 0.004\n",
      "1000/3300 (epoch 30), train_loss = -3.217, time/batch = 0.005\n",
      "1001/3300 (epoch 30), train_loss = -0.809, time/batch = 0.004\n",
      "1002/3300 (epoch 30), train_loss = -2.925, time/batch = 0.004\n",
      "1003/3300 (epoch 30), train_loss = -1.920, time/batch = 0.007\n",
      "1004/3300 (epoch 30), train_loss = -2.744, time/batch = 0.008\n",
      "1005/3300 (epoch 30), train_loss = -2.702, time/batch = 0.008\n",
      "1006/3300 (epoch 30), train_loss = -3.070, time/batch = 0.008\n",
      "1007/3300 (epoch 30), train_loss = -2.262, time/batch = 0.005\n",
      "1008/3300 (epoch 30), train_loss = -3.295, time/batch = 0.005\n",
      "1009/3300 (epoch 30), train_loss = -2.312, time/batch = 0.007\n",
      "1010/3300 (epoch 30), train_loss = -3.334, time/batch = 0.008\n",
      "1011/3300 (epoch 30), train_loss = -2.512, time/batch = 0.005\n",
      "1012/3300 (epoch 30), train_loss = -3.086, time/batch = 0.014\n",
      "1013/3300 (epoch 30), train_loss = -3.062, time/batch = 0.010\n",
      "1014/3300 (epoch 30), train_loss = -3.316, time/batch = 0.009\n",
      "1015/3300 (epoch 30), train_loss = -2.958, time/batch = 0.008\n",
      "1016/3300 (epoch 30), train_loss = -3.825, time/batch = 0.013\n",
      "1017/3300 (epoch 30), train_loss = -3.078, time/batch = 0.008\n",
      "1018/3300 (epoch 30), train_loss = -3.763, time/batch = 0.007\n",
      "1019/3300 (epoch 30), train_loss = -2.631, time/batch = 0.009\n",
      "1020/3300 (epoch 30), train_loss = -3.047, time/batch = 0.009\n",
      "1021/3300 (epoch 30), train_loss = -2.002, time/batch = 0.014\n",
      "1022/3300 (epoch 30), train_loss = -3.985, time/batch = 0.006\n",
      "1023/3300 (epoch 31), train_loss = -0.698, time/batch = 0.009\n",
      "1024/3300 (epoch 31), train_loss = -1.529, time/batch = 0.006\n",
      "1025/3300 (epoch 31), train_loss = -2.517, time/batch = 0.014\n",
      "1026/3300 (epoch 31), train_loss = -1.594, time/batch = 0.007\n",
      "1027/3300 (epoch 31), train_loss = -3.035, time/batch = 0.008\n",
      "1028/3300 (epoch 31), train_loss = -1.966, time/batch = 0.007\n",
      "1029/3300 (epoch 31), train_loss = -2.973, time/batch = 0.006\n",
      "1030/3300 (epoch 31), train_loss = -1.058, time/batch = 0.005\n",
      "1031/3300 (epoch 31), train_loss = -3.755, time/batch = 0.008\n",
      "1032/3300 (epoch 31), train_loss = -0.539, time/batch = 0.009\n",
      "1033/3300 (epoch 31), train_loss = -3.090, time/batch = 0.008\n",
      "1034/3300 (epoch 31), train_loss = -1.631, time/batch = 0.009\n",
      "1035/3300 (epoch 31), train_loss = -3.029, time/batch = 0.009\n",
      "1036/3300 (epoch 31), train_loss = -1.049, time/batch = 0.012\n",
      "1037/3300 (epoch 31), train_loss = -3.789, time/batch = 0.009\n",
      "1038/3300 (epoch 31), train_loss = -1.436, time/batch = 0.008\n",
      "1039/3300 (epoch 31), train_loss = -3.657, time/batch = 0.011\n",
      "1040/3300 (epoch 31), train_loss = -1.210, time/batch = 0.010\n",
      "1041/3300 (epoch 31), train_loss = -3.311, time/batch = 0.010\n",
      "1042/3300 (epoch 31), train_loss = -2.894, time/batch = 0.018\n",
      "1043/3300 (epoch 31), train_loss = -2.758, time/batch = 0.009\n",
      "1044/3300 (epoch 31), train_loss = -3.436, time/batch = 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045/3300 (epoch 31), train_loss = -2.213, time/batch = 0.007\n",
      "1046/3300 (epoch 31), train_loss = -3.854, time/batch = 0.007\n",
      "1047/3300 (epoch 31), train_loss = -2.237, time/batch = 0.010\n",
      "1048/3300 (epoch 31), train_loss = -3.626, time/batch = 0.025\n",
      "1049/3300 (epoch 31), train_loss = -2.262, time/batch = 0.011\n",
      "1050/3300 (epoch 31), train_loss = -3.438, time/batch = 0.011\n",
      "1051/3300 (epoch 31), train_loss = -3.532, time/batch = 0.006\n",
      "1052/3300 (epoch 31), train_loss = -3.619, time/batch = 0.007\n",
      "1053/3300 (epoch 31), train_loss = -3.212, time/batch = 0.008\n",
      "1054/3300 (epoch 31), train_loss = -3.184, time/batch = 0.006\n",
      "1055/3300 (epoch 31), train_loss = -3.161, time/batch = 0.008\n",
      "1056/3300 (epoch 32), train_loss = -0.794, time/batch = 0.011\n",
      "1057/3300 (epoch 32), train_loss = -1.797, time/batch = 0.008\n",
      "1058/3300 (epoch 32), train_loss = -2.478, time/batch = 0.007\n",
      "1059/3300 (epoch 32), train_loss = -2.628, time/batch = 0.009\n",
      "1060/3300 (epoch 32), train_loss = -1.998, time/batch = 0.005\n",
      "1061/3300 (epoch 32), train_loss = -2.615, time/batch = 0.007\n",
      "1062/3300 (epoch 32), train_loss = -1.258, time/batch = 0.005\n",
      "1063/3300 (epoch 32), train_loss = -2.973, time/batch = 0.009\n",
      "1064/3300 (epoch 32), train_loss = -2.069, time/batch = 0.008\n",
      "1065/3300 (epoch 32), train_loss = -3.475, time/batch = 0.008\n",
      "1066/3300 (epoch 32), train_loss = 0.039, time/batch = 0.005\n",
      "1067/3300 (epoch 32), train_loss = -3.518, time/batch = 0.008\n",
      "1068/3300 (epoch 32), train_loss = -2.962, time/batch = 0.006\n",
      "1069/3300 (epoch 32), train_loss = -2.682, time/batch = 0.005\n",
      "1070/3300 (epoch 32), train_loss = -2.693, time/batch = 0.004\n",
      "1071/3300 (epoch 32), train_loss = -2.870, time/batch = 0.005\n",
      "1072/3300 (epoch 32), train_loss = -2.755, time/batch = 0.007\n",
      "1073/3300 (epoch 32), train_loss = -3.268, time/batch = 0.005\n",
      "1074/3300 (epoch 32), train_loss = -2.923, time/batch = 0.007\n",
      "1075/3300 (epoch 32), train_loss = -3.089, time/batch = 0.007\n",
      "1076/3300 (epoch 32), train_loss = -2.824, time/batch = 0.007\n",
      "1077/3300 (epoch 32), train_loss = -3.335, time/batch = 0.005\n",
      "1078/3300 (epoch 32), train_loss = -2.402, time/batch = 0.005\n",
      "1079/3300 (epoch 32), train_loss = -3.716, time/batch = 0.005\n",
      "1080/3300 (epoch 32), train_loss = -2.881, time/batch = 0.006\n",
      "1081/3300 (epoch 32), train_loss = -3.982, time/batch = 0.005\n",
      "1082/3300 (epoch 32), train_loss = -2.818, time/batch = 0.005\n",
      "1083/3300 (epoch 32), train_loss = -3.283, time/batch = 0.006\n",
      "1084/3300 (epoch 32), train_loss = -2.993, time/batch = 0.005\n",
      "1085/3300 (epoch 32), train_loss = -4.096, time/batch = 0.009\n",
      "1086/3300 (epoch 32), train_loss = -3.077, time/batch = 0.006\n",
      "1087/3300 (epoch 32), train_loss = -4.173, time/batch = 0.007\n",
      "1088/3300 (epoch 32), train_loss = -1.469, time/batch = 0.006\n",
      "1089/3300 (epoch 33), train_loss = -0.794, time/batch = 0.004\n",
      "1090/3300 (epoch 33), train_loss = -1.810, time/batch = 0.008\n",
      "1091/3300 (epoch 33), train_loss = -2.473, time/batch = 0.006\n",
      "1092/3300 (epoch 33), train_loss = -3.046, time/batch = 0.005\n",
      "1093/3300 (epoch 33), train_loss = -1.977, time/batch = 0.009\n",
      "1094/3300 (epoch 33), train_loss = -2.995, time/batch = 0.009\n",
      "1095/3300 (epoch 33), train_loss = 0.327, time/batch = 0.005\n",
      "1096/3300 (epoch 33), train_loss = -2.986, time/batch = 0.007\n",
      "1097/3300 (epoch 33), train_loss = -0.794, time/batch = 0.006\n",
      "1098/3300 (epoch 33), train_loss = -2.639, time/batch = 0.007\n",
      "1099/3300 (epoch 33), train_loss = -1.517, time/batch = 0.006\n",
      "1100/3300 (epoch 33), train_loss = -3.009, time/batch = 0.005\n",
      "1101/3300 (epoch 33), train_loss = -2.285, time/batch = 0.007\n",
      "1102/3300 (epoch 33), train_loss = -3.184, time/batch = 0.006\n",
      "1103/3300 (epoch 33), train_loss = -1.593, time/batch = 0.007\n",
      "1104/3300 (epoch 33), train_loss = -3.365, time/batch = 0.007\n",
      "1105/3300 (epoch 33), train_loss = -2.617, time/batch = 0.009\n",
      "1106/3300 (epoch 33), train_loss = -2.664, time/batch = 0.005\n",
      "1107/3300 (epoch 33), train_loss = -3.581, time/batch = 0.006\n",
      "1108/3300 (epoch 33), train_loss = -2.171, time/batch = 0.006\n",
      "1109/3300 (epoch 33), train_loss = -3.502, time/batch = 0.008\n",
      "1110/3300 (epoch 33), train_loss = -1.757, time/batch = 0.009\n",
      "1111/3300 (epoch 33), train_loss = -3.674, time/batch = 0.009\n",
      "1112/3300 (epoch 33), train_loss = -0.933, time/batch = 0.008\n",
      "1113/3300 (epoch 33), train_loss = -3.788, time/batch = 0.009\n",
      "1114/3300 (epoch 33), train_loss = -1.233, time/batch = 0.010\n",
      "1115/3300 (epoch 33), train_loss = -3.632, time/batch = 0.008\n",
      "1116/3300 (epoch 33), train_loss = -3.536, time/batch = 0.009\n",
      "1117/3300 (epoch 33), train_loss = -3.679, time/batch = 0.009\n",
      "1118/3300 (epoch 33), train_loss = -2.544, time/batch = 0.009\n",
      "1119/3300 (epoch 33), train_loss = -3.744, time/batch = 0.009\n",
      "1120/3300 (epoch 33), train_loss = -2.421, time/batch = 0.010\n",
      "1121/3300 (epoch 33), train_loss = -3.494, time/batch = 0.010\n",
      "1122/3300 (epoch 34), train_loss = -0.950, time/batch = 0.008\n",
      "1123/3300 (epoch 34), train_loss = -1.022, time/batch = 0.010\n",
      "1124/3300 (epoch 34), train_loss = -2.463, time/batch = 0.009\n",
      "1125/3300 (epoch 34), train_loss = -1.685, time/batch = 0.015\n",
      "1126/3300 (epoch 34), train_loss = -2.630, time/batch = 0.010\n",
      "1127/3300 (epoch 34), train_loss = -0.841, time/batch = 0.010\n",
      "1128/3300 (epoch 34), train_loss = -2.792, time/batch = 0.009\n",
      "1129/3300 (epoch 34), train_loss = -1.299, time/batch = 0.010\n",
      "1130/3300 (epoch 34), train_loss = -3.391, time/batch = 0.008\n",
      "1131/3300 (epoch 34), train_loss = -2.671, time/batch = 0.009\n",
      "1132/3300 (epoch 34), train_loss = -3.198, time/batch = 0.010\n",
      "1133/3300 (epoch 34), train_loss = -2.993, time/batch = 0.008\n",
      "1134/3300 (epoch 34), train_loss = -3.286, time/batch = 0.007\n",
      "1135/3300 (epoch 34), train_loss = -3.315, time/batch = 0.007\n",
      "1136/3300 (epoch 34), train_loss = -2.930, time/batch = 0.010\n",
      "1137/3300 (epoch 34), train_loss = -2.494, time/batch = 0.009\n",
      "1138/3300 (epoch 34), train_loss = -2.912, time/batch = 0.007\n",
      "1139/3300 (epoch 34), train_loss = -3.287, time/batch = 0.006\n",
      "1140/3300 (epoch 34), train_loss = -3.409, time/batch = 0.008\n",
      "1141/3300 (epoch 34), train_loss = -2.913, time/batch = 0.005\n",
      "1142/3300 (epoch 34), train_loss = -2.863, time/batch = 0.007\n",
      "1143/3300 (epoch 34), train_loss = -3.348, time/batch = 0.005\n",
      "1144/3300 (epoch 34), train_loss = -3.529, time/batch = 0.007\n",
      "1145/3300 (epoch 34), train_loss = -3.562, time/batch = 0.005\n",
      "1146/3300 (epoch 34), train_loss = -2.302, time/batch = 0.005\n",
      "1147/3300 (epoch 34), train_loss = -4.165, time/batch = 0.004\n",
      "1148/3300 (epoch 34), train_loss = -2.544, time/batch = 0.005\n",
      "1149/3300 (epoch 34), train_loss = -3.545, time/batch = 0.004\n",
      "1150/3300 (epoch 34), train_loss = -2.022, time/batch = 0.004\n",
      "1151/3300 (epoch 34), train_loss = -3.713, time/batch = 0.005\n",
      "1152/3300 (epoch 34), train_loss = -2.900, time/batch = 0.004\n",
      "1153/3300 (epoch 34), train_loss = -3.110, time/batch = 0.005\n",
      "1154/3300 (epoch 34), train_loss = -3.006, time/batch = 0.004\n",
      "1155/3300 (epoch 35), train_loss = -0.820, time/batch = 0.009\n",
      "1156/3300 (epoch 35), train_loss = -2.017, time/batch = 0.010\n",
      "1157/3300 (epoch 35), train_loss = -2.671, time/batch = 0.009\n",
      "1158/3300 (epoch 35), train_loss = -2.619, time/batch = 0.006\n",
      "1159/3300 (epoch 35), train_loss = -1.862, time/batch = 0.009\n",
      "1160/3300 (epoch 35), train_loss = -2.230, time/batch = 0.005\n",
      "1161/3300 (epoch 35), train_loss = -1.939, time/batch = 0.007\n",
      "1162/3300 (epoch 35), train_loss = -2.276, time/batch = 0.005\n",
      "1163/3300 (epoch 35), train_loss = -3.496, time/batch = 0.007\n",
      "1164/3300 (epoch 35), train_loss = -3.427, time/batch = 0.008\n",
      "1165/3300 (epoch 35), train_loss = -0.764, time/batch = 0.007\n",
      "1166/3300 (epoch 35), train_loss = -2.544, time/batch = 0.005\n",
      "1167/3300 (epoch 35), train_loss = -0.632, time/batch = 0.005\n",
      "1168/3300 (epoch 35), train_loss = -3.587, time/batch = 0.006\n",
      "1169/3300 (epoch 35), train_loss = -2.243, time/batch = 0.008\n",
      "1170/3300 (epoch 35), train_loss = -3.434, time/batch = 0.008\n",
      "1171/3300 (epoch 35), train_loss = -2.125, time/batch = 0.006\n",
      "1172/3300 (epoch 35), train_loss = -3.582, time/batch = 0.009\n",
      "1173/3300 (epoch 35), train_loss = -2.756, time/batch = 0.009\n",
      "1174/3300 (epoch 35), train_loss = -3.120, time/batch = 0.007\n",
      "1175/3300 (epoch 35), train_loss = -2.998, time/batch = 0.006\n",
      "1176/3300 (epoch 35), train_loss = -3.403, time/batch = 0.007\n",
      "1177/3300 (epoch 35), train_loss = -2.963, time/batch = 0.005\n",
      "1178/3300 (epoch 35), train_loss = -3.658, time/batch = 0.012\n",
      "1179/3300 (epoch 35), train_loss = -3.699, time/batch = 0.005\n",
      "1180/3300 (epoch 35), train_loss = -2.417, time/batch = 0.006\n",
      "1181/3300 (epoch 35), train_loss = -3.646, time/batch = 0.005\n",
      "1182/3300 (epoch 35), train_loss = -3.180, time/batch = 0.007\n",
      "1183/3300 (epoch 35), train_loss = -3.396, time/batch = 0.008\n",
      "1184/3300 (epoch 35), train_loss = -3.262, time/batch = 0.005\n",
      "1185/3300 (epoch 35), train_loss = -3.308, time/batch = 0.005\n",
      "1186/3300 (epoch 35), train_loss = -3.426, time/batch = 0.007\n",
      "1187/3300 (epoch 35), train_loss = -3.551, time/batch = 0.006\n",
      "1188/3300 (epoch 36), train_loss = -0.914, time/batch = 0.007\n",
      "1189/3300 (epoch 36), train_loss = -1.493, time/batch = 0.006\n",
      "1190/3300 (epoch 36), train_loss = -2.512, time/batch = 0.007\n",
      "1191/3300 (epoch 36), train_loss = -2.305, time/batch = 0.004\n",
      "1192/3300 (epoch 36), train_loss = -2.736, time/batch = 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193/3300 (epoch 36), train_loss = -3.353, time/batch = 0.005\n",
      "1194/3300 (epoch 36), train_loss = -0.756, time/batch = 0.008\n",
      "1195/3300 (epoch 36), train_loss = -3.096, time/batch = 0.004\n",
      "1196/3300 (epoch 36), train_loss = -3.086, time/batch = 0.005\n",
      "1197/3300 (epoch 36), train_loss = -3.230, time/batch = 0.006\n",
      "1198/3300 (epoch 36), train_loss = -1.047, time/batch = 0.004\n",
      "1199/3300 (epoch 36), train_loss = -3.356, time/batch = 0.004\n",
      "1200/3300 (epoch 36), train_loss = -1.281, time/batch = 0.004\n",
      "1201/3300 (epoch 36), train_loss = -3.435, time/batch = 0.004\n",
      "1202/3300 (epoch 36), train_loss = -2.601, time/batch = 0.004\n",
      "1203/3300 (epoch 36), train_loss = -3.586, time/batch = 0.004\n",
      "1204/3300 (epoch 36), train_loss = -2.016, time/batch = 0.005\n",
      "1205/3300 (epoch 36), train_loss = -3.096, time/batch = 0.004\n",
      "1206/3300 (epoch 36), train_loss = -2.748, time/batch = 0.006\n",
      "1207/3300 (epoch 36), train_loss = -3.370, time/batch = 0.006\n",
      "1208/3300 (epoch 36), train_loss = -3.167, time/batch = 0.005\n",
      "1209/3300 (epoch 36), train_loss = -3.710, time/batch = 0.005\n",
      "1210/3300 (epoch 36), train_loss = -3.400, time/batch = 0.005\n",
      "1211/3300 (epoch 36), train_loss = -3.402, time/batch = 0.005\n",
      "1212/3300 (epoch 36), train_loss = -3.262, time/batch = 0.004\n",
      "1213/3300 (epoch 36), train_loss = -3.636, time/batch = 0.005\n",
      "1214/3300 (epoch 36), train_loss = -3.786, time/batch = 0.005\n",
      "1215/3300 (epoch 36), train_loss = -3.801, time/batch = 0.006\n",
      "1216/3300 (epoch 36), train_loss = -3.409, time/batch = 0.007\n",
      "1217/3300 (epoch 36), train_loss = -3.693, time/batch = 0.005\n",
      "1218/3300 (epoch 36), train_loss = -3.251, time/batch = 0.007\n",
      "1219/3300 (epoch 36), train_loss = -3.795, time/batch = 0.006\n",
      "1220/3300 (epoch 36), train_loss = -3.025, time/batch = 0.006\n",
      "1221/3300 (epoch 37), train_loss = -0.874, time/batch = 0.004\n",
      "1222/3300 (epoch 37), train_loss = -1.756, time/batch = 0.005\n",
      "1223/3300 (epoch 37), train_loss = -2.587, time/batch = 0.005\n",
      "1224/3300 (epoch 37), train_loss = -3.011, time/batch = 0.005\n",
      "1225/3300 (epoch 37), train_loss = -3.116, time/batch = 0.005\n",
      "1226/3300 (epoch 37), train_loss = 1.499, time/batch = 0.005\n",
      "1227/3300 (epoch 37), train_loss = -2.316, time/batch = 0.005\n",
      "1228/3300 (epoch 37), train_loss = -2.576, time/batch = 0.006\n",
      "1229/3300 (epoch 37), train_loss = -3.011, time/batch = 0.005\n",
      "1230/3300 (epoch 37), train_loss = -2.775, time/batch = 0.006\n",
      "1231/3300 (epoch 37), train_loss = -2.186, time/batch = 0.006\n",
      "1232/3300 (epoch 37), train_loss = -3.469, time/batch = 0.008\n",
      "1233/3300 (epoch 37), train_loss = -0.032, time/batch = 0.004\n",
      "1234/3300 (epoch 37), train_loss = -2.702, time/batch = 0.005\n",
      "1235/3300 (epoch 37), train_loss = -2.783, time/batch = 0.005\n",
      "1236/3300 (epoch 37), train_loss = -3.811, time/batch = 0.005\n",
      "1237/3300 (epoch 37), train_loss = -2.674, time/batch = 0.004\n",
      "1238/3300 (epoch 37), train_loss = -3.216, time/batch = 0.004\n",
      "1239/3300 (epoch 37), train_loss = -3.227, time/batch = 0.004\n",
      "1240/3300 (epoch 37), train_loss = -3.763, time/batch = 0.006\n",
      "1241/3300 (epoch 37), train_loss = -2.609, time/batch = 0.007\n",
      "1242/3300 (epoch 37), train_loss = -3.052, time/batch = 0.008\n",
      "1243/3300 (epoch 37), train_loss = -3.683, time/batch = 0.005\n",
      "1244/3300 (epoch 37), train_loss = -2.820, time/batch = 0.005\n",
      "1245/3300 (epoch 37), train_loss = -3.991, time/batch = 0.008\n",
      "1246/3300 (epoch 37), train_loss = -3.143, time/batch = 0.006\n",
      "1247/3300 (epoch 37), train_loss = -3.241, time/batch = 0.007\n",
      "1248/3300 (epoch 37), train_loss = -3.848, time/batch = 0.007\n",
      "1249/3300 (epoch 37), train_loss = -3.873, time/batch = 0.009\n",
      "1250/3300 (epoch 37), train_loss = -3.766, time/batch = 0.006\n",
      "1251/3300 (epoch 37), train_loss = -4.119, time/batch = 0.009\n",
      "1252/3300 (epoch 37), train_loss = -3.782, time/batch = 0.008\n",
      "1253/3300 (epoch 37), train_loss = -3.742, time/batch = 0.009\n",
      "1254/3300 (epoch 38), train_loss = -0.879, time/batch = 0.005\n",
      "1255/3300 (epoch 38), train_loss = -1.825, time/batch = 0.008\n",
      "1256/3300 (epoch 38), train_loss = -2.552, time/batch = 0.007\n",
      "1257/3300 (epoch 38), train_loss = -3.049, time/batch = 0.008\n",
      "1258/3300 (epoch 38), train_loss = -2.447, time/batch = 0.009\n",
      "1259/3300 (epoch 38), train_loss = -3.395, time/batch = 0.006\n",
      "1260/3300 (epoch 38), train_loss = -2.204, time/batch = 0.007\n",
      "1261/3300 (epoch 38), train_loss = -2.442, time/batch = 0.010\n",
      "1262/3300 (epoch 38), train_loss = -2.772, time/batch = 0.009\n",
      "1263/3300 (epoch 38), train_loss = -3.487, time/batch = 0.008\n",
      "1264/3300 (epoch 38), train_loss = -2.851, time/batch = 0.009\n",
      "1265/3300 (epoch 38), train_loss = -1.883, time/batch = 0.007\n",
      "1266/3300 (epoch 38), train_loss = -3.067, time/batch = 0.009\n",
      "1267/3300 (epoch 38), train_loss = -3.643, time/batch = 0.006\n",
      "1268/3300 (epoch 38), train_loss = -3.552, time/batch = 0.008\n",
      "1269/3300 (epoch 38), train_loss = -2.916, time/batch = 0.008\n",
      "1270/3300 (epoch 38), train_loss = -3.169, time/batch = 0.009\n",
      "1271/3300 (epoch 38), train_loss = -2.158, time/batch = 0.010\n",
      "1272/3300 (epoch 38), train_loss = -3.858, time/batch = 0.009\n",
      "1273/3300 (epoch 38), train_loss = -1.202, time/batch = 0.007\n",
      "1274/3300 (epoch 38), train_loss = -3.502, time/batch = 0.010\n",
      "1275/3300 (epoch 38), train_loss = -3.346, time/batch = 0.007\n",
      "1276/3300 (epoch 38), train_loss = -3.738, time/batch = 0.009\n",
      "1277/3300 (epoch 38), train_loss = -3.164, time/batch = 0.006\n",
      "1278/3300 (epoch 38), train_loss = -3.478, time/batch = 0.010\n",
      "1279/3300 (epoch 38), train_loss = -3.898, time/batch = 0.013\n",
      "1280/3300 (epoch 38), train_loss = -4.364, time/batch = 0.013\n",
      "1281/3300 (epoch 38), train_loss = -2.951, time/batch = 0.018\n",
      "1282/3300 (epoch 38), train_loss = -4.289, time/batch = 0.015\n",
      "1283/3300 (epoch 38), train_loss = -3.191, time/batch = 0.009\n",
      "1284/3300 (epoch 38), train_loss = -3.686, time/batch = 0.009\n",
      "1285/3300 (epoch 38), train_loss = -3.108, time/batch = 0.008\n",
      "1286/3300 (epoch 38), train_loss = -3.867, time/batch = 0.031\n",
      "1287/3300 (epoch 39), train_loss = -0.848, time/batch = 0.004\n",
      "1288/3300 (epoch 39), train_loss = -1.995, time/batch = 0.005\n",
      "1289/3300 (epoch 39), train_loss = -2.700, time/batch = 0.004\n",
      "1290/3300 (epoch 39), train_loss = -1.835, time/batch = 0.006\n",
      "1291/3300 (epoch 39), train_loss = -2.952, time/batch = 0.004\n",
      "1292/3300 (epoch 39), train_loss = -3.678, time/batch = 0.005\n",
      "1293/3300 (epoch 39), train_loss = -1.237, time/batch = 0.005\n",
      "1294/3300 (epoch 39), train_loss = -2.821, time/batch = 0.006\n",
      "1295/3300 (epoch 39), train_loss = -3.144, time/batch = 0.015\n",
      "1296/3300 (epoch 39), train_loss = -3.462, time/batch = 0.007\n",
      "1297/3300 (epoch 39), train_loss = -2.558, time/batch = 0.007\n",
      "1298/3300 (epoch 39), train_loss = -3.092, time/batch = 0.005\n",
      "1299/3300 (epoch 39), train_loss = -2.645, time/batch = 0.010\n",
      "1300/3300 (epoch 39), train_loss = -2.871, time/batch = 0.007\n",
      "1301/3300 (epoch 39), train_loss = -3.194, time/batch = 0.008\n",
      "1302/3300 (epoch 39), train_loss = -3.092, time/batch = 0.007\n",
      "1303/3300 (epoch 39), train_loss = -2.477, time/batch = 0.005\n",
      "1304/3300 (epoch 39), train_loss = -3.363, time/batch = 0.007\n",
      "1305/3300 (epoch 39), train_loss = -3.481, time/batch = 0.008\n",
      "1306/3300 (epoch 39), train_loss = -2.884, time/batch = 0.006\n",
      "1307/3300 (epoch 39), train_loss = -3.232, time/batch = 0.005\n",
      "1308/3300 (epoch 39), train_loss = -3.295, time/batch = 0.005\n",
      "1309/3300 (epoch 39), train_loss = -3.319, time/batch = 0.014\n",
      "1310/3300 (epoch 39), train_loss = -3.264, time/batch = 0.008\n",
      "1311/3300 (epoch 39), train_loss = -4.098, time/batch = 0.007\n",
      "1312/3300 (epoch 39), train_loss = -4.176, time/batch = 0.009\n",
      "1313/3300 (epoch 39), train_loss = -3.874, time/batch = 0.015\n",
      "1314/3300 (epoch 39), train_loss = -3.347, time/batch = 0.011\n",
      "1315/3300 (epoch 39), train_loss = -3.369, time/batch = 0.009\n",
      "1316/3300 (epoch 39), train_loss = -3.522, time/batch = 0.007\n",
      "1317/3300 (epoch 39), train_loss = -3.989, time/batch = 0.014\n",
      "1318/3300 (epoch 39), train_loss = -4.307, time/batch = 0.009\n",
      "1319/3300 (epoch 39), train_loss = -4.018, time/batch = 0.007\n",
      "1320/3300 (epoch 40), train_loss = -0.953, time/batch = 0.018\n",
      "1321/3300 (epoch 40), train_loss = -2.067, time/batch = 0.010\n",
      "1322/3300 (epoch 40), train_loss = -2.566, time/batch = 0.005\n",
      "1323/3300 (epoch 40), train_loss = -1.946, time/batch = 0.008\n",
      "1324/3300 (epoch 40), train_loss = -2.859, time/batch = 0.006\n",
      "1325/3300 (epoch 40), train_loss = -0.587, time/batch = 0.008\n",
      "1326/3300 (epoch 40), train_loss = -2.982, time/batch = 0.007\n",
      "1327/3300 (epoch 40), train_loss = -2.674, time/batch = 0.008\n",
      "1328/3300 (epoch 40), train_loss = -3.367, time/batch = 0.006\n",
      "1329/3300 (epoch 40), train_loss = -3.229, time/batch = 0.007\n",
      "1330/3300 (epoch 40), train_loss = -3.679, time/batch = 0.008\n",
      "1331/3300 (epoch 40), train_loss = -2.299, time/batch = 0.008\n",
      "1332/3300 (epoch 40), train_loss = -2.930, time/batch = 0.007\n",
      "1333/3300 (epoch 40), train_loss = -3.377, time/batch = 0.009\n",
      "1334/3300 (epoch 40), train_loss = -2.770, time/batch = 0.009\n",
      "1335/3300 (epoch 40), train_loss = -3.112, time/batch = 0.006\n",
      "1336/3300 (epoch 40), train_loss = -4.169, time/batch = 0.007\n",
      "1337/3300 (epoch 40), train_loss = -2.613, time/batch = 0.007\n",
      "1338/3300 (epoch 40), train_loss = -3.148, time/batch = 0.008\n",
      "1339/3300 (epoch 40), train_loss = -2.741, time/batch = 0.008\n",
      "1340/3300 (epoch 40), train_loss = -3.675, time/batch = 0.006\n",
      "1341/3300 (epoch 40), train_loss = -3.066, time/batch = 0.008\n",
      "1342/3300 (epoch 40), train_loss = -3.937, time/batch = 0.007\n",
      "1343/3300 (epoch 40), train_loss = -3.377, time/batch = 0.009\n",
      "1344/3300 (epoch 40), train_loss = -2.990, time/batch = 0.008\n",
      "1345/3300 (epoch 40), train_loss = -4.349, time/batch = 0.009\n",
      "1346/3300 (epoch 40), train_loss = -4.327, time/batch = 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/3300 (epoch 40), train_loss = -3.618, time/batch = 0.009\n",
      "1348/3300 (epoch 40), train_loss = -3.012, time/batch = 0.006\n",
      "1349/3300 (epoch 40), train_loss = -3.340, time/batch = 0.007\n",
      "1350/3300 (epoch 40), train_loss = -3.797, time/batch = 0.006\n",
      "1351/3300 (epoch 40), train_loss = -2.511, time/batch = 0.009\n",
      "1352/3300 (epoch 40), train_loss = -3.737, time/batch = 0.006\n",
      "1353/3300 (epoch 41), train_loss = -0.875, time/batch = 0.005\n",
      "1354/3300 (epoch 41), train_loss = -2.156, time/batch = 0.006\n",
      "1355/3300 (epoch 41), train_loss = -2.549, time/batch = 0.011\n",
      "1356/3300 (epoch 41), train_loss = -2.889, time/batch = 0.009\n",
      "1357/3300 (epoch 41), train_loss = -2.359, time/batch = 0.008\n",
      "1358/3300 (epoch 41), train_loss = -0.879, time/batch = 0.007\n",
      "1359/3300 (epoch 41), train_loss = -2.895, time/batch = 0.009\n",
      "1360/3300 (epoch 41), train_loss = -1.834, time/batch = 0.008\n",
      "1361/3300 (epoch 41), train_loss = -3.745, time/batch = 0.006\n",
      "1362/3300 (epoch 41), train_loss = -3.622, time/batch = 0.008\n",
      "1363/3300 (epoch 41), train_loss = -3.156, time/batch = 0.010\n",
      "1364/3300 (epoch 41), train_loss = -3.193, time/batch = 0.010\n",
      "1365/3300 (epoch 41), train_loss = -3.359, time/batch = 0.009\n",
      "1366/3300 (epoch 41), train_loss = -2.515, time/batch = 0.010\n",
      "1367/3300 (epoch 41), train_loss = -3.855, time/batch = 0.009\n",
      "1368/3300 (epoch 41), train_loss = -2.358, time/batch = 0.008\n",
      "1369/3300 (epoch 41), train_loss = -3.643, time/batch = 0.007\n",
      "1370/3300 (epoch 41), train_loss = -2.925, time/batch = 0.006\n",
      "1371/3300 (epoch 41), train_loss = -3.819, time/batch = 0.010\n",
      "1372/3300 (epoch 41), train_loss = -3.070, time/batch = 0.007\n",
      "1373/3300 (epoch 41), train_loss = -3.228, time/batch = 0.007\n",
      "1374/3300 (epoch 41), train_loss = -3.580, time/batch = 0.009\n",
      "1375/3300 (epoch 41), train_loss = -3.968, time/batch = 0.009\n",
      "1376/3300 (epoch 41), train_loss = -3.407, time/batch = 0.008\n",
      "1377/3300 (epoch 41), train_loss = -3.905, time/batch = 0.009\n",
      "1378/3300 (epoch 41), train_loss = -4.205, time/batch = 0.009\n",
      "1379/3300 (epoch 41), train_loss = -4.002, time/batch = 0.008\n",
      "1380/3300 (epoch 41), train_loss = -3.537, time/batch = 0.008\n",
      "1381/3300 (epoch 41), train_loss = -3.428, time/batch = 0.008\n",
      "1382/3300 (epoch 41), train_loss = -4.142, time/batch = 0.009\n",
      "1383/3300 (epoch 41), train_loss = -4.337, time/batch = 0.007\n",
      "1384/3300 (epoch 41), train_loss = -4.149, time/batch = 0.007\n",
      "1385/3300 (epoch 41), train_loss = -3.435, time/batch = 0.006\n",
      "1386/3300 (epoch 42), train_loss = -0.988, time/batch = 0.008\n",
      "1387/3300 (epoch 42), train_loss = -2.092, time/batch = 0.007\n",
      "1388/3300 (epoch 42), train_loss = -2.670, time/batch = 0.007\n",
      "1389/3300 (epoch 42), train_loss = -2.662, time/batch = 0.006\n",
      "1390/3300 (epoch 42), train_loss = -2.627, time/batch = 0.008\n",
      "1391/3300 (epoch 42), train_loss = -2.578, time/batch = 0.005\n",
      "1392/3300 (epoch 42), train_loss = -3.402, time/batch = 0.006\n",
      "1393/3300 (epoch 42), train_loss = -1.457, time/batch = 0.009\n",
      "1394/3300 (epoch 42), train_loss = -3.584, time/batch = 0.007\n",
      "1395/3300 (epoch 42), train_loss = -2.633, time/batch = 0.009\n",
      "1396/3300 (epoch 42), train_loss = -3.591, time/batch = 0.008\n",
      "1397/3300 (epoch 42), train_loss = -2.900, time/batch = 0.005\n",
      "1398/3300 (epoch 42), train_loss = -3.520, time/batch = 0.007\n",
      "1399/3300 (epoch 42), train_loss = -3.116, time/batch = 0.008\n",
      "1400/3300 (epoch 42), train_loss = -3.458, time/batch = 0.005\n",
      "1401/3300 (epoch 42), train_loss = -3.047, time/batch = 0.006\n",
      "1402/3300 (epoch 42), train_loss = -3.491, time/batch = 0.008\n",
      "1403/3300 (epoch 42), train_loss = -3.621, time/batch = 0.005\n",
      "1404/3300 (epoch 42), train_loss = -3.369, time/batch = 0.006\n",
      "1405/3300 (epoch 42), train_loss = -3.554, time/batch = 0.009\n",
      "1406/3300 (epoch 42), train_loss = -2.987, time/batch = 0.006\n",
      "1407/3300 (epoch 42), train_loss = -3.644, time/batch = 0.010\n",
      "1408/3300 (epoch 42), train_loss = -3.369, time/batch = 0.010\n",
      "1409/3300 (epoch 42), train_loss = -3.703, time/batch = 0.007\n",
      "1410/3300 (epoch 42), train_loss = -1.730, time/batch = 0.010\n",
      "1411/3300 (epoch 42), train_loss = -3.769, time/batch = 0.007\n",
      "1412/3300 (epoch 42), train_loss = -4.135, time/batch = 0.009\n",
      "1413/3300 (epoch 42), train_loss = -4.061, time/batch = 0.010\n",
      "1414/3300 (epoch 42), train_loss = -3.380, time/batch = 0.010\n",
      "1415/3300 (epoch 42), train_loss = -3.876, time/batch = 0.009\n",
      "1416/3300 (epoch 42), train_loss = -3.155, time/batch = 0.009\n",
      "1417/3300 (epoch 42), train_loss = -3.694, time/batch = 0.009\n",
      "1418/3300 (epoch 42), train_loss = -3.550, time/batch = 0.008\n",
      "1419/3300 (epoch 43), train_loss = -0.987, time/batch = 0.006\n",
      "1420/3300 (epoch 43), train_loss = -1.969, time/batch = 0.010\n",
      "1421/3300 (epoch 43), train_loss = -2.677, time/batch = 0.010\n",
      "1422/3300 (epoch 43), train_loss = -2.436, time/batch = 0.009\n",
      "1423/3300 (epoch 43), train_loss = -2.480, time/batch = 0.008\n",
      "1424/3300 (epoch 43), train_loss = -3.299, time/batch = 0.009\n",
      "1425/3300 (epoch 43), train_loss = -1.628, time/batch = 0.008\n",
      "1426/3300 (epoch 43), train_loss = -3.273, time/batch = 0.008\n",
      "1427/3300 (epoch 43), train_loss = -2.671, time/batch = 0.009\n",
      "1428/3300 (epoch 43), train_loss = -3.844, time/batch = 0.009\n",
      "1429/3300 (epoch 43), train_loss = -2.487, time/batch = 0.010\n",
      "1430/3300 (epoch 43), train_loss = -3.694, time/batch = 0.010\n",
      "1431/3300 (epoch 43), train_loss = -2.671, time/batch = 0.010\n",
      "1432/3300 (epoch 43), train_loss = -3.878, time/batch = 0.010\n",
      "1433/3300 (epoch 43), train_loss = -2.979, time/batch = 0.010\n",
      "1434/3300 (epoch 43), train_loss = -3.594, time/batch = 0.010\n",
      "1435/3300 (epoch 43), train_loss = -3.895, time/batch = 0.010\n",
      "1436/3300 (epoch 43), train_loss = -1.950, time/batch = 0.011\n",
      "1437/3300 (epoch 43), train_loss = -4.006, time/batch = 0.010\n",
      "1438/3300 (epoch 43), train_loss = -1.784, time/batch = 0.010\n",
      "1439/3300 (epoch 43), train_loss = -3.457, time/batch = 0.009\n",
      "1440/3300 (epoch 43), train_loss = -3.371, time/batch = 0.009\n",
      "1441/3300 (epoch 43), train_loss = -3.489, time/batch = 0.010\n",
      "1442/3300 (epoch 43), train_loss = -3.462, time/batch = 0.009\n",
      "1443/3300 (epoch 43), train_loss = -3.727, time/batch = 0.008\n",
      "1444/3300 (epoch 43), train_loss = -3.071, time/batch = 0.009\n",
      "1445/3300 (epoch 43), train_loss = -4.185, time/batch = 0.009\n",
      "1446/3300 (epoch 43), train_loss = -3.228, time/batch = 0.008\n",
      "1447/3300 (epoch 43), train_loss = -3.874, time/batch = 0.008\n",
      "1448/3300 (epoch 43), train_loss = -3.446, time/batch = 0.009\n",
      "1449/3300 (epoch 43), train_loss = -3.810, time/batch = 0.010\n",
      "1450/3300 (epoch 43), train_loss = -3.158, time/batch = 0.007\n",
      "1451/3300 (epoch 43), train_loss = -3.721, time/batch = 0.010\n",
      "1452/3300 (epoch 44), train_loss = -0.994, time/batch = 0.010\n",
      "1453/3300 (epoch 44), train_loss = -2.130, time/batch = 0.011\n",
      "1454/3300 (epoch 44), train_loss = -2.636, time/batch = 0.006\n",
      "1455/3300 (epoch 44), train_loss = -1.879, time/batch = 0.014\n",
      "1456/3300 (epoch 44), train_loss = -2.000, time/batch = 0.008\n",
      "1457/3300 (epoch 44), train_loss = -3.200, time/batch = 0.016\n",
      "1458/3300 (epoch 44), train_loss = -2.536, time/batch = 0.006\n",
      "1459/3300 (epoch 44), train_loss = -2.748, time/batch = 0.008\n",
      "1460/3300 (epoch 44), train_loss = -2.504, time/batch = 0.007\n",
      "1461/3300 (epoch 44), train_loss = -3.291, time/batch = 0.033\n",
      "1462/3300 (epoch 44), train_loss = -3.223, time/batch = 0.005\n",
      "1463/3300 (epoch 44), train_loss = -3.452, time/batch = 0.019\n",
      "1464/3300 (epoch 44), train_loss = -3.323, time/batch = 0.009\n",
      "1465/3300 (epoch 44), train_loss = -4.046, time/batch = 0.008\n",
      "1466/3300 (epoch 44), train_loss = -2.089, time/batch = 0.006\n",
      "1467/3300 (epoch 44), train_loss = -3.787, time/batch = 0.010\n",
      "1468/3300 (epoch 44), train_loss = -2.223, time/batch = 0.005\n",
      "1469/3300 (epoch 44), train_loss = -3.859, time/batch = 0.005\n",
      "1470/3300 (epoch 44), train_loss = -2.457, time/batch = 0.008\n",
      "1471/3300 (epoch 44), train_loss = -3.613, time/batch = 0.017\n",
      "1472/3300 (epoch 44), train_loss = -3.055, time/batch = 0.010\n",
      "1473/3300 (epoch 44), train_loss = -3.924, time/batch = 0.006\n",
      "1474/3300 (epoch 44), train_loss = -2.517, time/batch = 0.007\n",
      "1475/3300 (epoch 44), train_loss = -4.359, time/batch = 0.008\n",
      "1476/3300 (epoch 44), train_loss = -4.052, time/batch = 0.008\n",
      "1477/3300 (epoch 44), train_loss = -3.455, time/batch = 0.007\n",
      "1478/3300 (epoch 44), train_loss = -3.601, time/batch = 0.005\n",
      "1479/3300 (epoch 44), train_loss = -4.178, time/batch = 0.006\n",
      "1480/3300 (epoch 44), train_loss = -4.339, time/batch = 0.006\n",
      "1481/3300 (epoch 44), train_loss = -3.905, time/batch = 0.008\n",
      "1482/3300 (epoch 44), train_loss = -3.469, time/batch = 0.007\n",
      "1483/3300 (epoch 44), train_loss = -3.669, time/batch = 0.007\n",
      "1484/3300 (epoch 44), train_loss = -3.679, time/batch = 0.015\n",
      "1485/3300 (epoch 45), train_loss = -1.003, time/batch = 0.011\n",
      "1486/3300 (epoch 45), train_loss = -1.932, time/batch = 0.007\n",
      "1487/3300 (epoch 45), train_loss = -2.637, time/batch = 0.010\n",
      "1488/3300 (epoch 45), train_loss = -2.375, time/batch = 0.017\n",
      "1489/3300 (epoch 45), train_loss = -3.031, time/batch = 0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490/3300 (epoch 45), train_loss = -2.621, time/batch = 0.008\n",
      "1491/3300 (epoch 45), train_loss = -2.665, time/batch = 0.005\n",
      "1492/3300 (epoch 45), train_loss = -2.643, time/batch = 0.012\n",
      "1493/3300 (epoch 45), train_loss = -3.592, time/batch = 0.007\n",
      "1494/3300 (epoch 45), train_loss = -4.217, time/batch = 0.008\n",
      "1495/3300 (epoch 45), train_loss = -2.034, time/batch = 0.008\n",
      "1496/3300 (epoch 45), train_loss = -3.729, time/batch = 0.009\n",
      "1497/3300 (epoch 45), train_loss = -3.251, time/batch = 0.014\n",
      "1498/3300 (epoch 45), train_loss = -3.720, time/batch = 0.011\n",
      "1499/3300 (epoch 45), train_loss = -3.456, time/batch = 0.006\n",
      "1500/3300 (epoch 45), train_loss = -3.467, time/batch = 0.009\n",
      "1501/3300 (epoch 45), train_loss = -3.516, time/batch = 0.009\n",
      "1502/3300 (epoch 45), train_loss = -3.336, time/batch = 0.007\n",
      "1503/3300 (epoch 45), train_loss = -3.869, time/batch = 0.007\n",
      "1504/3300 (epoch 45), train_loss = -2.277, time/batch = 0.009\n",
      "1505/3300 (epoch 45), train_loss = -3.703, time/batch = 0.008\n",
      "1506/3300 (epoch 45), train_loss = -3.525, time/batch = 0.009\n",
      "1507/3300 (epoch 45), train_loss = -4.237, time/batch = 0.008\n",
      "1508/3300 (epoch 45), train_loss = -3.600, time/batch = 0.008\n",
      "1509/3300 (epoch 45), train_loss = -3.598, time/batch = 0.009\n",
      "1510/3300 (epoch 45), train_loss = -4.254, time/batch = 0.009\n",
      "1511/3300 (epoch 45), train_loss = -4.469, time/batch = 0.008\n",
      "1512/3300 (epoch 45), train_loss = -3.980, time/batch = 0.008\n",
      "1513/3300 (epoch 45), train_loss = -3.952, time/batch = 0.009\n",
      "1514/3300 (epoch 45), train_loss = -3.313, time/batch = 0.010\n",
      "1515/3300 (epoch 45), train_loss = -4.513, time/batch = 0.009\n",
      "1516/3300 (epoch 45), train_loss = -2.986, time/batch = 0.010\n",
      "1517/3300 (epoch 45), train_loss = -3.822, time/batch = 0.008\n",
      "1518/3300 (epoch 46), train_loss = -1.077, time/batch = 0.006\n",
      "1519/3300 (epoch 46), train_loss = -1.918, time/batch = 0.006\n",
      "1520/3300 (epoch 46), train_loss = -2.726, time/batch = 0.007\n",
      "1521/3300 (epoch 46), train_loss = -2.337, time/batch = 0.007\n",
      "1522/3300 (epoch 46), train_loss = -2.375, time/batch = 0.009\n",
      "1523/3300 (epoch 46), train_loss = -2.822, time/batch = 0.007\n",
      "1524/3300 (epoch 46), train_loss = -2.480, time/batch = 0.007\n",
      "1525/3300 (epoch 46), train_loss = -2.631, time/batch = 0.009\n",
      "1526/3300 (epoch 46), train_loss = -3.422, time/batch = 0.008\n",
      "1527/3300 (epoch 46), train_loss = -3.107, time/batch = 0.007\n",
      "1528/3300 (epoch 46), train_loss = -3.663, time/batch = 0.007\n",
      "1529/3300 (epoch 46), train_loss = -2.918, time/batch = 0.007\n",
      "1530/3300 (epoch 46), train_loss = -3.239, time/batch = 0.008\n",
      "1531/3300 (epoch 46), train_loss = -4.187, time/batch = 0.007\n",
      "1532/3300 (epoch 46), train_loss = -3.565, time/batch = 0.007\n",
      "1533/3300 (epoch 46), train_loss = -3.651, time/batch = 0.007\n",
      "1534/3300 (epoch 46), train_loss = -3.815, time/batch = 0.007\n",
      "1535/3300 (epoch 46), train_loss = -2.672, time/batch = 0.007\n",
      "1536/3300 (epoch 46), train_loss = -3.608, time/batch = 0.010\n",
      "1537/3300 (epoch 46), train_loss = -3.282, time/batch = 0.008\n",
      "1538/3300 (epoch 46), train_loss = -3.761, time/batch = 0.013\n",
      "1539/3300 (epoch 46), train_loss = -3.315, time/batch = 0.011\n",
      "1540/3300 (epoch 46), train_loss = -3.606, time/batch = 0.009\n",
      "1541/3300 (epoch 46), train_loss = -4.311, time/batch = 0.009\n",
      "1542/3300 (epoch 46), train_loss = -4.460, time/batch = 0.006\n",
      "1543/3300 (epoch 46), train_loss = -3.463, time/batch = 0.008\n",
      "1544/3300 (epoch 46), train_loss = -3.679, time/batch = 0.007\n",
      "1545/3300 (epoch 46), train_loss = -3.791, time/batch = 0.010\n",
      "1546/3300 (epoch 46), train_loss = -4.096, time/batch = 0.008\n",
      "1547/3300 (epoch 46), train_loss = -4.350, time/batch = 0.009\n",
      "1548/3300 (epoch 46), train_loss = -4.598, time/batch = 0.010\n",
      "1549/3300 (epoch 46), train_loss = -4.537, time/batch = 0.007\n",
      "1550/3300 (epoch 46), train_loss = -4.552, time/batch = 0.009\n",
      "1551/3300 (epoch 47), train_loss = -1.040, time/batch = 0.006\n",
      "1552/3300 (epoch 47), train_loss = -2.094, time/batch = 0.008\n",
      "1553/3300 (epoch 47), train_loss = -2.617, time/batch = 0.008\n",
      "1554/3300 (epoch 47), train_loss = -1.859, time/batch = 0.006\n",
      "1555/3300 (epoch 47), train_loss = -2.353, time/batch = 0.008\n",
      "1556/3300 (epoch 47), train_loss = -2.788, time/batch = 0.009\n",
      "1557/3300 (epoch 47), train_loss = -2.736, time/batch = 0.009\n",
      "1558/3300 (epoch 47), train_loss = -2.690, time/batch = 0.009\n",
      "1559/3300 (epoch 47), train_loss = -3.473, time/batch = 0.009\n",
      "1560/3300 (epoch 47), train_loss = -3.567, time/batch = 0.009\n",
      "1561/3300 (epoch 47), train_loss = -3.557, time/batch = 0.006\n",
      "1562/3300 (epoch 47), train_loss = -2.457, time/batch = 0.009\n",
      "1563/3300 (epoch 47), train_loss = -3.636, time/batch = 0.011\n",
      "1564/3300 (epoch 47), train_loss = -3.633, time/batch = 0.006\n",
      "1565/3300 (epoch 47), train_loss = -3.750, time/batch = 0.009\n",
      "1566/3300 (epoch 47), train_loss = -3.302, time/batch = 0.008\n",
      "1567/3300 (epoch 47), train_loss = -3.311, time/batch = 0.009\n",
      "1568/3300 (epoch 47), train_loss = -3.632, time/batch = 0.006\n",
      "1569/3300 (epoch 47), train_loss = -3.808, time/batch = 0.009\n",
      "1570/3300 (epoch 47), train_loss = -3.432, time/batch = 0.006\n",
      "1571/3300 (epoch 47), train_loss = -4.074, time/batch = 0.008\n",
      "1572/3300 (epoch 47), train_loss = -2.572, time/batch = 0.008\n",
      "1573/3300 (epoch 47), train_loss = -4.519, time/batch = 0.008\n",
      "1574/3300 (epoch 47), train_loss = -4.151, time/batch = 0.009\n",
      "1575/3300 (epoch 47), train_loss = -4.653, time/batch = 0.008\n",
      "1576/3300 (epoch 47), train_loss = -4.222, time/batch = 0.009\n",
      "1577/3300 (epoch 47), train_loss = -3.765, time/batch = 0.006\n",
      "1578/3300 (epoch 47), train_loss = -3.683, time/batch = 0.009\n",
      "1579/3300 (epoch 47), train_loss = -4.690, time/batch = 0.009\n",
      "1580/3300 (epoch 47), train_loss = -4.054, time/batch = 0.007\n",
      "1581/3300 (epoch 47), train_loss = -4.293, time/batch = 0.007\n",
      "1582/3300 (epoch 47), train_loss = -3.745, time/batch = 0.006\n",
      "1583/3300 (epoch 47), train_loss = -3.636, time/batch = 0.007\n",
      "1584/3300 (epoch 48), train_loss = -1.087, time/batch = 0.006\n",
      "1585/3300 (epoch 48), train_loss = -1.992, time/batch = 0.008\n",
      "1586/3300 (epoch 48), train_loss = -2.724, time/batch = 0.008\n",
      "1587/3300 (epoch 48), train_loss = -1.913, time/batch = 0.006\n",
      "1588/3300 (epoch 48), train_loss = -1.281, time/batch = 0.009\n",
      "1589/3300 (epoch 48), train_loss = -1.834, time/batch = 0.006\n",
      "1590/3300 (epoch 48), train_loss = -2.653, time/batch = 0.009\n",
      "1591/3300 (epoch 48), train_loss = -2.571, time/batch = 0.007\n",
      "1592/3300 (epoch 48), train_loss = -3.229, time/batch = 0.007\n",
      "1593/3300 (epoch 48), train_loss = -3.337, time/batch = 0.009\n",
      "1594/3300 (epoch 48), train_loss = -3.899, time/batch = 0.008\n",
      "1595/3300 (epoch 48), train_loss = -3.554, time/batch = 0.008\n",
      "1596/3300 (epoch 48), train_loss = -3.130, time/batch = 0.008\n",
      "1597/3300 (epoch 48), train_loss = -3.872, time/batch = 0.008\n",
      "1598/3300 (epoch 48), train_loss = -3.067, time/batch = 0.008\n",
      "1599/3300 (epoch 48), train_loss = -3.457, time/batch = 0.008\n",
      "1600/3300 (epoch 48), train_loss = -3.465, time/batch = 0.008\n",
      "1601/3300 (epoch 48), train_loss = -3.861, time/batch = 0.007\n",
      "1602/3300 (epoch 48), train_loss = -2.988, time/batch = 0.007\n",
      "1603/3300 (epoch 48), train_loss = -3.440, time/batch = 0.008\n",
      "1604/3300 (epoch 48), train_loss = -3.378, time/batch = 0.006\n",
      "1605/3300 (epoch 48), train_loss = -4.317, time/batch = 0.009\n",
      "1606/3300 (epoch 48), train_loss = -3.692, time/batch = 0.008\n",
      "1607/3300 (epoch 48), train_loss = -4.032, time/batch = 0.007\n",
      "1608/3300 (epoch 48), train_loss = -4.005, time/batch = 0.007\n",
      "1609/3300 (epoch 48), train_loss = -4.065, time/batch = 0.008\n",
      "1610/3300 (epoch 48), train_loss = -3.766, time/batch = 0.008\n",
      "1611/3300 (epoch 48), train_loss = -4.341, time/batch = 0.010\n",
      "1612/3300 (epoch 48), train_loss = -4.065, time/batch = 0.008\n",
      "1613/3300 (epoch 48), train_loss = -3.389, time/batch = 0.010\n",
      "1614/3300 (epoch 48), train_loss = -3.846, time/batch = 0.008\n",
      "1615/3300 (epoch 48), train_loss = -3.890, time/batch = 0.011\n",
      "1616/3300 (epoch 48), train_loss = -4.762, time/batch = 0.019\n",
      "1617/3300 (epoch 49), train_loss = -1.081, time/batch = 0.015\n",
      "1618/3300 (epoch 49), train_loss = -1.838, time/batch = 0.008\n",
      "1619/3300 (epoch 49), train_loss = -2.537, time/batch = 0.005\n",
      "1620/3300 (epoch 49), train_loss = -2.277, time/batch = 0.005\n",
      "1621/3300 (epoch 49), train_loss = -2.518, time/batch = 0.012\n",
      "1622/3300 (epoch 49), train_loss = -2.827, time/batch = 0.014\n",
      "1623/3300 (epoch 49), train_loss = -2.519, time/batch = 0.009\n",
      "1624/3300 (epoch 49), train_loss = -3.517, time/batch = 0.008\n",
      "1625/3300 (epoch 49), train_loss = -4.440, time/batch = 0.005\n",
      "1626/3300 (epoch 49), train_loss = -3.274, time/batch = 0.007\n",
      "1627/3300 (epoch 49), train_loss = -3.776, time/batch = 0.009\n",
      "1628/3300 (epoch 49), train_loss = -2.699, time/batch = 0.008\n",
      "1629/3300 (epoch 49), train_loss = -3.470, time/batch = 0.021\n",
      "1630/3300 (epoch 49), train_loss = -2.880, time/batch = 0.015\n",
      "1631/3300 (epoch 49), train_loss = -2.958, time/batch = 0.007\n",
      "1632/3300 (epoch 49), train_loss = -2.658, time/batch = 0.008\n",
      "1633/3300 (epoch 49), train_loss = -3.709, time/batch = 0.007\n",
      "1634/3300 (epoch 49), train_loss = -3.459, time/batch = 0.008\n",
      "1635/3300 (epoch 49), train_loss = -3.225, time/batch = 0.006\n",
      "1636/3300 (epoch 49), train_loss = -3.543, time/batch = 0.009\n",
      "1637/3300 (epoch 49), train_loss = -4.175, time/batch = 0.007\n",
      "1638/3300 (epoch 49), train_loss = -3.358, time/batch = 0.009\n",
      "1639/3300 (epoch 49), train_loss = -4.089, time/batch = 0.016\n",
      "1640/3300 (epoch 49), train_loss = -4.642, time/batch = 0.011\n",
      "1641/3300 (epoch 49), train_loss = -4.762, time/batch = 0.005\n",
      "1642/3300 (epoch 49), train_loss = -4.360, time/batch = 0.007\n",
      "1643/3300 (epoch 49), train_loss = -3.796, time/batch = 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644/3300 (epoch 49), train_loss = -3.677, time/batch = 0.011\n",
      "1645/3300 (epoch 49), train_loss = -4.926, time/batch = 0.010\n",
      "1646/3300 (epoch 49), train_loss = -3.481, time/batch = 0.007\n",
      "1647/3300 (epoch 49), train_loss = -3.738, time/batch = 0.014\n",
      "1648/3300 (epoch 49), train_loss = -3.470, time/batch = 0.008\n",
      "1649/3300 (epoch 49), train_loss = -4.109, time/batch = 0.005\n",
      "1650/3300 (epoch 50), train_loss = -1.140, time/batch = 0.009\n",
      "1651/3300 (epoch 50), train_loss = -1.892, time/batch = 0.016\n",
      "1652/3300 (epoch 50), train_loss = -2.571, time/batch = 0.011\n",
      "1653/3300 (epoch 50), train_loss = -2.493, time/batch = 0.009\n",
      "1654/3300 (epoch 50), train_loss = -2.159, time/batch = 0.009\n",
      "1655/3300 (epoch 50), train_loss = -2.707, time/batch = 0.005\n",
      "1656/3300 (epoch 50), train_loss = -2.335, time/batch = 0.008\n",
      "1657/3300 (epoch 50), train_loss = -2.465, time/batch = 0.008\n",
      "1658/3300 (epoch 50), train_loss = -3.528, time/batch = 0.008\n",
      "1659/3300 (epoch 50), train_loss = -4.254, time/batch = 0.006\n",
      "1660/3300 (epoch 50), train_loss = -2.441, time/batch = 0.006\n",
      "1661/3300 (epoch 50), train_loss = -3.029, time/batch = 0.008\n",
      "1662/3300 (epoch 50), train_loss = -3.669, time/batch = 0.009\n",
      "1663/3300 (epoch 50), train_loss = -3.272, time/batch = 0.011\n",
      "1664/3300 (epoch 50), train_loss = -3.947, time/batch = 0.010\n",
      "1665/3300 (epoch 50), train_loss = -2.852, time/batch = 0.010\n",
      "1666/3300 (epoch 50), train_loss = -4.009, time/batch = 0.007\n",
      "1667/3300 (epoch 50), train_loss = -2.829, time/batch = 0.010\n",
      "1668/3300 (epoch 50), train_loss = -3.523, time/batch = 0.008\n",
      "1669/3300 (epoch 50), train_loss = -3.520, time/batch = 0.009\n",
      "1670/3300 (epoch 50), train_loss = -4.083, time/batch = 0.011\n",
      "1671/3300 (epoch 50), train_loss = -3.347, time/batch = 0.010\n",
      "1672/3300 (epoch 50), train_loss = -3.692, time/batch = 0.010\n",
      "1673/3300 (epoch 50), train_loss = -3.643, time/batch = 0.014\n",
      "1674/3300 (epoch 50), train_loss = -4.741, time/batch = 0.010\n",
      "1675/3300 (epoch 50), train_loss = -3.409, time/batch = 0.021\n",
      "1676/3300 (epoch 50), train_loss = -4.449, time/batch = 0.019\n",
      "1677/3300 (epoch 50), train_loss = -3.648, time/batch = 0.044\n",
      "1678/3300 (epoch 50), train_loss = -4.109, time/batch = 0.010\n",
      "1679/3300 (epoch 50), train_loss = -3.289, time/batch = 0.010\n",
      "1680/3300 (epoch 50), train_loss = -4.051, time/batch = 0.007\n",
      "1681/3300 (epoch 50), train_loss = -3.758, time/batch = 0.012\n",
      "1682/3300 (epoch 50), train_loss = -3.732, time/batch = 0.008\n",
      "1683/3300 (epoch 51), train_loss = -1.076, time/batch = 0.006\n",
      "1684/3300 (epoch 51), train_loss = -2.100, time/batch = 0.014\n",
      "1685/3300 (epoch 51), train_loss = -2.455, time/batch = 0.008\n",
      "1686/3300 (epoch 51), train_loss = -2.349, time/batch = 0.008\n",
      "1687/3300 (epoch 51), train_loss = -2.350, time/batch = 0.006\n",
      "1688/3300 (epoch 51), train_loss = -2.646, time/batch = 0.008\n",
      "1689/3300 (epoch 51), train_loss = -2.488, time/batch = 0.008\n",
      "1690/3300 (epoch 51), train_loss = -2.529, time/batch = 0.008\n",
      "1691/3300 (epoch 51), train_loss = -3.055, time/batch = 0.010\n",
      "1692/3300 (epoch 51), train_loss = -3.772, time/batch = 0.006\n",
      "1693/3300 (epoch 51), train_loss = -3.114, time/batch = 0.008\n",
      "1694/3300 (epoch 51), train_loss = -3.439, time/batch = 0.008\n",
      "1695/3300 (epoch 51), train_loss = -3.500, time/batch = 0.011\n",
      "1696/3300 (epoch 51), train_loss = -4.045, time/batch = 0.017\n",
      "1697/3300 (epoch 51), train_loss = -3.318, time/batch = 0.017\n",
      "1698/3300 (epoch 51), train_loss = -3.599, time/batch = 0.010\n",
      "1699/3300 (epoch 51), train_loss = -3.565, time/batch = 0.008\n",
      "1700/3300 (epoch 51), train_loss = -3.414, time/batch = 0.013\n",
      "1701/3300 (epoch 51), train_loss = -3.661, time/batch = 0.009\n",
      "1702/3300 (epoch 51), train_loss = -3.443, time/batch = 0.013\n",
      "1703/3300 (epoch 51), train_loss = -4.221, time/batch = 0.008\n",
      "1704/3300 (epoch 51), train_loss = -3.435, time/batch = 0.008\n",
      "1705/3300 (epoch 51), train_loss = -4.684, time/batch = 0.010\n",
      "1706/3300 (epoch 51), train_loss = -4.125, time/batch = 0.014\n",
      "1707/3300 (epoch 51), train_loss = -3.965, time/batch = 0.010\n",
      "1708/3300 (epoch 51), train_loss = -3.858, time/batch = 0.010\n",
      "1709/3300 (epoch 51), train_loss = -4.468, time/batch = 0.015\n",
      "1710/3300 (epoch 51), train_loss = -3.486, time/batch = 0.019\n",
      "1711/3300 (epoch 51), train_loss = -3.984, time/batch = 0.008\n",
      "1712/3300 (epoch 51), train_loss = -3.698, time/batch = 0.007\n",
      "1713/3300 (epoch 51), train_loss = -3.758, time/batch = 0.005\n",
      "1714/3300 (epoch 51), train_loss = -4.001, time/batch = 0.006\n",
      "1715/3300 (epoch 51), train_loss = -4.236, time/batch = 0.004\n",
      "1716/3300 (epoch 52), train_loss = -1.104, time/batch = 0.005\n",
      "1717/3300 (epoch 52), train_loss = -1.831, time/batch = 0.005\n",
      "1718/3300 (epoch 52), train_loss = -2.510, time/batch = 0.005\n",
      "1719/3300 (epoch 52), train_loss = -2.283, time/batch = 0.007\n",
      "1720/3300 (epoch 52), train_loss = -2.063, time/batch = 0.012\n",
      "1721/3300 (epoch 52), train_loss = -2.813, time/batch = 0.019\n",
      "1722/3300 (epoch 52), train_loss = -2.322, time/batch = 0.042\n",
      "1723/3300 (epoch 52), train_loss = -3.360, time/batch = 0.005\n",
      "1724/3300 (epoch 52), train_loss = -3.084, time/batch = 0.004\n",
      "1725/3300 (epoch 52), train_loss = -3.874, time/batch = 0.007\n",
      "1726/3300 (epoch 52), train_loss = -3.378, time/batch = 0.008\n",
      "1727/3300 (epoch 52), train_loss = -3.636, time/batch = 0.008\n",
      "1728/3300 (epoch 52), train_loss = -3.313, time/batch = 0.015\n",
      "1729/3300 (epoch 52), train_loss = -3.976, time/batch = 0.007\n",
      "1730/3300 (epoch 52), train_loss = -2.743, time/batch = 0.006\n",
      "1731/3300 (epoch 52), train_loss = -4.037, time/batch = 0.005\n",
      "1732/3300 (epoch 52), train_loss = -3.292, time/batch = 0.005\n",
      "1733/3300 (epoch 52), train_loss = -3.326, time/batch = 0.005\n",
      "1734/3300 (epoch 52), train_loss = -3.406, time/batch = 0.004\n",
      "1735/3300 (epoch 52), train_loss = -3.906, time/batch = 0.005\n",
      "1736/3300 (epoch 52), train_loss = -4.362, time/batch = 0.006\n",
      "1737/3300 (epoch 52), train_loss = -3.379, time/batch = 0.005\n",
      "1738/3300 (epoch 52), train_loss = -4.600, time/batch = 0.006\n",
      "1739/3300 (epoch 52), train_loss = -4.355, time/batch = 0.005\n",
      "1740/3300 (epoch 52), train_loss = -4.819, time/batch = 0.004\n",
      "1741/3300 (epoch 52), train_loss = -4.456, time/batch = 0.015\n",
      "1742/3300 (epoch 52), train_loss = -4.806, time/batch = 0.012\n",
      "1743/3300 (epoch 52), train_loss = -4.542, time/batch = 0.008\n",
      "1744/3300 (epoch 52), train_loss = -4.839, time/batch = 0.007\n",
      "1745/3300 (epoch 52), train_loss = -3.536, time/batch = 0.005\n",
      "1746/3300 (epoch 52), train_loss = -3.693, time/batch = 0.008\n",
      "1747/3300 (epoch 52), train_loss = -3.858, time/batch = 0.008\n",
      "1748/3300 (epoch 52), train_loss = -4.786, time/batch = 0.007\n",
      "1749/3300 (epoch 53), train_loss = -1.119, time/batch = 0.006\n",
      "1750/3300 (epoch 53), train_loss = -2.010, time/batch = 0.006\n",
      "1751/3300 (epoch 53), train_loss = -2.577, time/batch = 0.008\n",
      "1752/3300 (epoch 53), train_loss = -2.273, time/batch = 0.015\n",
      "1753/3300 (epoch 53), train_loss = -2.337, time/batch = 0.010\n",
      "1754/3300 (epoch 53), train_loss = -2.670, time/batch = 0.007\n",
      "1755/3300 (epoch 53), train_loss = -2.176, time/batch = 0.007\n",
      "1756/3300 (epoch 53), train_loss = -3.169, time/batch = 0.007\n",
      "1757/3300 (epoch 53), train_loss = -3.267, time/batch = 0.007\n",
      "1758/3300 (epoch 53), train_loss = -3.887, time/batch = 0.018\n",
      "1759/3300 (epoch 53), train_loss = -3.920, time/batch = 0.011\n",
      "1760/3300 (epoch 53), train_loss = -3.493, time/batch = 0.009\n",
      "1761/3300 (epoch 53), train_loss = -3.702, time/batch = 0.016\n",
      "1762/3300 (epoch 53), train_loss = -3.921, time/batch = 0.009\n",
      "1763/3300 (epoch 53), train_loss = -3.958, time/batch = 0.007\n",
      "1764/3300 (epoch 53), train_loss = -3.593, time/batch = 0.024\n",
      "1765/3300 (epoch 53), train_loss = -3.759, time/batch = 0.015\n",
      "1766/3300 (epoch 53), train_loss = -3.193, time/batch = 0.008\n",
      "1767/3300 (epoch 53), train_loss = -3.698, time/batch = 0.007\n",
      "1768/3300 (epoch 53), train_loss = -3.063, time/batch = 0.008\n",
      "1769/3300 (epoch 53), train_loss = -3.504, time/batch = 0.006\n",
      "1770/3300 (epoch 53), train_loss = -3.751, time/batch = 0.016\n",
      "1771/3300 (epoch 53), train_loss = -3.865, time/batch = 0.008\n",
      "1772/3300 (epoch 53), train_loss = -3.811, time/batch = 0.008\n",
      "1773/3300 (epoch 53), train_loss = -3.512, time/batch = 0.009\n",
      "1774/3300 (epoch 53), train_loss = -4.744, time/batch = 0.008\n",
      "1775/3300 (epoch 53), train_loss = -4.457, time/batch = 0.010\n",
      "1776/3300 (epoch 53), train_loss = -4.825, time/batch = 0.008\n",
      "1777/3300 (epoch 53), train_loss = -4.501, time/batch = 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1778/3300 (epoch 53), train_loss = -4.873, time/batch = 0.019\n",
      "1779/3300 (epoch 53), train_loss = -3.746, time/batch = 0.016\n",
      "1780/3300 (epoch 53), train_loss = -3.634, time/batch = 0.010\n",
      "1781/3300 (epoch 53), train_loss = -4.053, time/batch = 0.009\n",
      "1782/3300 (epoch 54), train_loss = -1.252, time/batch = 0.016\n",
      "1783/3300 (epoch 54), train_loss = -1.826, time/batch = 0.010\n",
      "1784/3300 (epoch 54), train_loss = -2.625, time/batch = 0.009\n",
      "1785/3300 (epoch 54), train_loss = -2.560, time/batch = 0.009\n",
      "1786/3300 (epoch 54), train_loss = -2.346, time/batch = 0.009\n",
      "1787/3300 (epoch 54), train_loss = -1.904, time/batch = 0.018\n",
      "1788/3300 (epoch 54), train_loss = -2.499, time/batch = 0.010\n",
      "1789/3300 (epoch 54), train_loss = -2.147, time/batch = 0.008\n",
      "1790/3300 (epoch 54), train_loss = -3.631, time/batch = 0.015\n",
      "1791/3300 (epoch 54), train_loss = -2.666, time/batch = 0.013\n",
      "1792/3300 (epoch 54), train_loss = -3.332, time/batch = 0.015\n",
      "1793/3300 (epoch 54), train_loss = -3.773, time/batch = 0.010\n",
      "1794/3300 (epoch 54), train_loss = -3.855, time/batch = 0.013\n",
      "1795/3300 (epoch 54), train_loss = -2.775, time/batch = 0.046\n",
      "1796/3300 (epoch 54), train_loss = -4.320, time/batch = 0.007\n",
      "1797/3300 (epoch 54), train_loss = -2.706, time/batch = 0.011\n",
      "1798/3300 (epoch 54), train_loss = -3.897, time/batch = 0.008\n",
      "1799/3300 (epoch 54), train_loss = -3.413, time/batch = 0.008\n",
      "1800/3300 (epoch 54), train_loss = -3.342, time/batch = 0.007\n",
      "1801/3300 (epoch 54), train_loss = -3.607, time/batch = 0.007\n",
      "1802/3300 (epoch 54), train_loss = -3.644, time/batch = 0.007\n",
      "1803/3300 (epoch 54), train_loss = -4.260, time/batch = 0.011\n",
      "1804/3300 (epoch 54), train_loss = -3.509, time/batch = 0.008\n",
      "1805/3300 (epoch 54), train_loss = -3.592, time/batch = 0.009\n",
      "1806/3300 (epoch 54), train_loss = -4.634, time/batch = 0.008\n",
      "1807/3300 (epoch 54), train_loss = -4.676, time/batch = 0.005\n",
      "1808/3300 (epoch 54), train_loss = -4.676, time/batch = 0.004\n",
      "1809/3300 (epoch 54), train_loss = -4.627, time/batch = 0.013\n",
      "1810/3300 (epoch 54), train_loss = -3.866, time/batch = 0.010\n",
      "1811/3300 (epoch 54), train_loss = -3.796, time/batch = 0.012\n",
      "1812/3300 (epoch 54), train_loss = -4.457, time/batch = 0.012\n",
      "1813/3300 (epoch 54), train_loss = -4.878, time/batch = 0.007\n",
      "1814/3300 (epoch 54), train_loss = -4.899, time/batch = 0.006\n",
      "1815/3300 (epoch 55), train_loss = -1.135, time/batch = 0.013\n",
      "1816/3300 (epoch 55), train_loss = -2.110, time/batch = 0.008\n",
      "1817/3300 (epoch 55), train_loss = -2.526, time/batch = 0.014\n",
      "1818/3300 (epoch 55), train_loss = -2.075, time/batch = 0.010\n",
      "1819/3300 (epoch 55), train_loss = -2.617, time/batch = 0.007\n",
      "1820/3300 (epoch 55), train_loss = -2.294, time/batch = 0.008\n",
      "1821/3300 (epoch 55), train_loss = -2.703, time/batch = 0.014\n",
      "1822/3300 (epoch 55), train_loss = -3.055, time/batch = 0.007\n",
      "1823/3300 (epoch 55), train_loss = -3.139, time/batch = 0.006\n",
      "1824/3300 (epoch 55), train_loss = -3.448, time/batch = 0.007\n",
      "1825/3300 (epoch 55), train_loss = -3.560, time/batch = 0.009\n",
      "1826/3300 (epoch 55), train_loss = -3.314, time/batch = 0.013\n",
      "1827/3300 (epoch 55), train_loss = -4.059, time/batch = 0.008\n",
      "1828/3300 (epoch 55), train_loss = -3.413, time/batch = 0.010\n",
      "1829/3300 (epoch 55), train_loss = -3.818, time/batch = 0.010\n",
      "1830/3300 (epoch 55), train_loss = -3.659, time/batch = 0.016\n",
      "1831/3300 (epoch 55), train_loss = -3.646, time/batch = 0.008\n",
      "1832/3300 (epoch 55), train_loss = -2.897, time/batch = 0.010\n",
      "1833/3300 (epoch 55), train_loss = -3.629, time/batch = 0.007\n",
      "1834/3300 (epoch 55), train_loss = -3.946, time/batch = 0.012\n",
      "1835/3300 (epoch 55), train_loss = -4.152, time/batch = 0.010\n",
      "1836/3300 (epoch 55), train_loss = -3.944, time/batch = 0.010\n",
      "1837/3300 (epoch 55), train_loss = -4.004, time/batch = 0.009\n",
      "1838/3300 (epoch 55), train_loss = -4.335, time/batch = 0.010\n",
      "1839/3300 (epoch 55), train_loss = -4.671, time/batch = 0.007\n",
      "1840/3300 (epoch 55), train_loss = -4.101, time/batch = 0.009\n",
      "1841/3300 (epoch 55), train_loss = -3.859, time/batch = 0.010\n",
      "1842/3300 (epoch 55), train_loss = -4.005, time/batch = 0.009\n",
      "1843/3300 (epoch 55), train_loss = -3.728, time/batch = 0.008\n",
      "1844/3300 (epoch 55), train_loss = -4.196, time/batch = 0.011\n",
      "1845/3300 (epoch 55), train_loss = -3.869, time/batch = 0.009\n",
      "1846/3300 (epoch 55), train_loss = -3.793, time/batch = 0.009\n",
      "1847/3300 (epoch 55), train_loss = -4.050, time/batch = 0.022\n",
      "1848/3300 (epoch 56), train_loss = -1.133, time/batch = 0.012\n",
      "1849/3300 (epoch 56), train_loss = -2.008, time/batch = 0.007\n",
      "1850/3300 (epoch 56), train_loss = -2.520, time/batch = 0.006\n",
      "1851/3300 (epoch 56), train_loss = -2.215, time/batch = 0.007\n",
      "1852/3300 (epoch 56), train_loss = -1.787, time/batch = 0.015\n",
      "1853/3300 (epoch 56), train_loss = -2.209, time/batch = 0.010\n",
      "1854/3300 (epoch 56), train_loss = -2.692, time/batch = 0.009\n",
      "1855/3300 (epoch 56), train_loss = -3.017, time/batch = 0.007\n",
      "1856/3300 (epoch 56), train_loss = -3.877, time/batch = 0.006\n",
      "1857/3300 (epoch 56), train_loss = -3.843, time/batch = 0.006\n",
      "1858/3300 (epoch 56), train_loss = -3.753, time/batch = 0.008\n",
      "1859/3300 (epoch 56), train_loss = -3.465, time/batch = 0.005\n",
      "1860/3300 (epoch 56), train_loss = -3.962, time/batch = 0.005\n",
      "1861/3300 (epoch 56), train_loss = -4.029, time/batch = 0.005\n",
      "1862/3300 (epoch 56), train_loss = -4.006, time/batch = 0.018\n",
      "1863/3300 (epoch 56), train_loss = -3.669, time/batch = 0.011\n",
      "1864/3300 (epoch 56), train_loss = -3.938, time/batch = 0.005\n",
      "1865/3300 (epoch 56), train_loss = -3.187, time/batch = 0.009\n",
      "1866/3300 (epoch 56), train_loss = -3.837, time/batch = 0.009\n",
      "1867/3300 (epoch 56), train_loss = -3.813, time/batch = 0.007\n",
      "1868/3300 (epoch 56), train_loss = -3.268, time/batch = 0.007\n",
      "1869/3300 (epoch 56), train_loss = -4.012, time/batch = 0.006\n",
      "1870/3300 (epoch 56), train_loss = -3.724, time/batch = 0.006\n",
      "1871/3300 (epoch 56), train_loss = -4.711, time/batch = 0.006\n",
      "1872/3300 (epoch 56), train_loss = -4.688, time/batch = 0.007\n",
      "1873/3300 (epoch 56), train_loss = -4.795, time/batch = 0.006\n",
      "1874/3300 (epoch 56), train_loss = -4.674, time/batch = 0.007\n",
      "1875/3300 (epoch 56), train_loss = -4.802, time/batch = 0.015\n",
      "1876/3300 (epoch 56), train_loss = -4.758, time/batch = 0.007\n",
      "1877/3300 (epoch 56), train_loss = -4.504, time/batch = 0.007\n",
      "1878/3300 (epoch 56), train_loss = -4.055, time/batch = 0.009\n",
      "1879/3300 (epoch 56), train_loss = -3.585, time/batch = 0.005\n",
      "1880/3300 (epoch 56), train_loss = -4.806, time/batch = 0.011\n",
      "1881/3300 (epoch 57), train_loss = -1.156, time/batch = 0.007\n",
      "1882/3300 (epoch 57), train_loss = -2.168, time/batch = 0.008\n",
      "1883/3300 (epoch 57), train_loss = -2.389, time/batch = 0.008\n",
      "1884/3300 (epoch 57), train_loss = -2.380, time/batch = 0.016\n",
      "1885/3300 (epoch 57), train_loss = -2.161, time/batch = 0.008\n",
      "1886/3300 (epoch 57), train_loss = -2.764, time/batch = 0.007\n",
      "1887/3300 (epoch 57), train_loss = -2.867, time/batch = 0.007\n",
      "1888/3300 (epoch 57), train_loss = -2.647, time/batch = 0.007\n",
      "1889/3300 (epoch 57), train_loss = -3.152, time/batch = 0.006\n",
      "1890/3300 (epoch 57), train_loss = -3.743, time/batch = 0.014\n",
      "1891/3300 (epoch 57), train_loss = -3.473, time/batch = 0.006\n",
      "1892/3300 (epoch 57), train_loss = -3.915, time/batch = 0.004\n",
      "1893/3300 (epoch 57), train_loss = -2.967, time/batch = 0.006\n",
      "1894/3300 (epoch 57), train_loss = -4.314, time/batch = 0.006\n",
      "1895/3300 (epoch 57), train_loss = -2.803, time/batch = 0.008\n",
      "1896/3300 (epoch 57), train_loss = -3.693, time/batch = 0.006\n",
      "1897/3300 (epoch 57), train_loss = -3.456, time/batch = 0.006\n",
      "1898/3300 (epoch 57), train_loss = -3.512, time/batch = 0.007\n",
      "1899/3300 (epoch 57), train_loss = -3.524, time/batch = 0.007\n",
      "1900/3300 (epoch 57), train_loss = -3.630, time/batch = 0.010\n",
      "1901/3300 (epoch 57), train_loss = -4.191, time/batch = 0.010\n",
      "1902/3300 (epoch 57), train_loss = -3.860, time/batch = 0.007\n",
      "1903/3300 (epoch 57), train_loss = -4.268, time/batch = 0.009\n",
      "1904/3300 (epoch 57), train_loss = -3.304, time/batch = 0.006\n",
      "1905/3300 (epoch 57), train_loss = -4.114, time/batch = 0.007\n",
      "1906/3300 (epoch 57), train_loss = -4.796, time/batch = 0.008\n",
      "1907/3300 (epoch 57), train_loss = -4.756, time/batch = 0.010\n",
      "1908/3300 (epoch 57), train_loss = -3.581, time/batch = 0.009\n",
      "1909/3300 (epoch 57), train_loss = -4.186, time/batch = 0.006\n",
      "1910/3300 (epoch 57), train_loss = -3.843, time/batch = 0.009\n",
      "1911/3300 (epoch 57), train_loss = -3.818, time/batch = 0.010\n",
      "1912/3300 (epoch 57), train_loss = -3.794, time/batch = 0.008\n",
      "1913/3300 (epoch 57), train_loss = -4.182, time/batch = 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1914/3300 (epoch 58), train_loss = -1.185, time/batch = 0.017\n",
      "1915/3300 (epoch 58), train_loss = -1.959, time/batch = 0.030\n",
      "1916/3300 (epoch 58), train_loss = -2.376, time/batch = 0.015\n",
      "1917/3300 (epoch 58), train_loss = -2.686, time/batch = 0.041\n",
      "1918/3300 (epoch 58), train_loss = -2.388, time/batch = 0.014\n",
      "1919/3300 (epoch 58), train_loss = -2.989, time/batch = 0.017\n",
      "1920/3300 (epoch 58), train_loss = -1.716, time/batch = 0.017\n",
      "1921/3300 (epoch 58), train_loss = -3.019, time/batch = 0.017\n",
      "1922/3300 (epoch 58), train_loss = -3.531, time/batch = 0.016\n",
      "1923/3300 (epoch 58), train_loss = -3.863, time/batch = 0.017\n",
      "1924/3300 (epoch 58), train_loss = -4.167, time/batch = 0.017\n",
      "1925/3300 (epoch 58), train_loss = -3.611, time/batch = 0.016\n",
      "1926/3300 (epoch 58), train_loss = -3.998, time/batch = 0.016\n",
      "1927/3300 (epoch 58), train_loss = -4.140, time/batch = 0.006\n",
      "1928/3300 (epoch 58), train_loss = -3.786, time/batch = 0.005\n",
      "1929/3300 (epoch 58), train_loss = -3.430, time/batch = 0.004\n",
      "1930/3300 (epoch 58), train_loss = -3.671, time/batch = 0.007\n",
      "1931/3300 (epoch 58), train_loss = -3.510, time/batch = 0.004\n",
      "1932/3300 (epoch 58), train_loss = -3.842, time/batch = 0.004\n",
      "1933/3300 (epoch 58), train_loss = -3.792, time/batch = 0.005\n",
      "1934/3300 (epoch 58), train_loss = -4.283, time/batch = 0.007\n",
      "1935/3300 (epoch 58), train_loss = -4.266, time/batch = 0.007\n",
      "1936/3300 (epoch 58), train_loss = -4.141, time/batch = 0.006\n",
      "1937/3300 (epoch 58), train_loss = -3.855, time/batch = 0.013\n",
      "1938/3300 (epoch 58), train_loss = -4.788, time/batch = 0.007\n",
      "1939/3300 (epoch 58), train_loss = -4.626, time/batch = 0.005\n",
      "1940/3300 (epoch 58), train_loss = -3.538, time/batch = 0.005\n",
      "1941/3300 (epoch 58), train_loss = -3.998, time/batch = 0.004\n",
      "1942/3300 (epoch 58), train_loss = -4.078, time/batch = 0.005\n",
      "1943/3300 (epoch 58), train_loss = -4.911, time/batch = 0.004\n",
      "1944/3300 (epoch 58), train_loss = -4.327, time/batch = 0.004\n",
      "1945/3300 (epoch 58), train_loss = -3.720, time/batch = 0.005\n",
      "1946/3300 (epoch 58), train_loss = -3.676, time/batch = 0.005\n",
      "1947/3300 (epoch 59), train_loss = -1.162, time/batch = 0.004\n",
      "1948/3300 (epoch 59), train_loss = -1.993, time/batch = 0.006\n",
      "1949/3300 (epoch 59), train_loss = -2.478, time/batch = 0.006\n",
      "1950/3300 (epoch 59), train_loss = -3.193, time/batch = 0.004\n",
      "1951/3300 (epoch 59), train_loss = -2.205, time/batch = 0.009\n",
      "1952/3300 (epoch 59), train_loss = -1.681, time/batch = 0.010\n",
      "1953/3300 (epoch 59), train_loss = -2.237, time/batch = 0.006\n",
      "1954/3300 (epoch 59), train_loss = -2.439, time/batch = 0.006\n",
      "1955/3300 (epoch 59), train_loss = -2.819, time/batch = 0.008\n",
      "1956/3300 (epoch 59), train_loss = -3.983, time/batch = 0.007\n",
      "1957/3300 (epoch 59), train_loss = -3.614, time/batch = 0.006\n",
      "1958/3300 (epoch 59), train_loss = -3.625, time/batch = 0.006\n",
      "1959/3300 (epoch 59), train_loss = -3.152, time/batch = 0.011\n",
      "1960/3300 (epoch 59), train_loss = -3.934, time/batch = 0.014\n",
      "1961/3300 (epoch 59), train_loss = -3.691, time/batch = 0.018\n",
      "1962/3300 (epoch 59), train_loss = -4.188, time/batch = 0.006\n",
      "1963/3300 (epoch 59), train_loss = -3.720, time/batch = 0.013\n",
      "1964/3300 (epoch 59), train_loss = -3.564, time/batch = 0.007\n",
      "1965/3300 (epoch 59), train_loss = -3.819, time/batch = 0.006\n",
      "1966/3300 (epoch 59), train_loss = -3.912, time/batch = 0.007\n",
      "1967/3300 (epoch 59), train_loss = -3.380, time/batch = 0.007\n",
      "1968/3300 (epoch 59), train_loss = -3.884, time/batch = 0.005\n",
      "1969/3300 (epoch 59), train_loss = -3.469, time/batch = 0.005\n",
      "1970/3300 (epoch 59), train_loss = -3.613, time/batch = 0.004\n",
      "1971/3300 (epoch 59), train_loss = -4.423, time/batch = 0.004\n",
      "1972/3300 (epoch 59), train_loss = -4.729, time/batch = 0.006\n",
      "1973/3300 (epoch 59), train_loss = -4.847, time/batch = 0.005\n",
      "1974/3300 (epoch 59), train_loss = -4.552, time/batch = 0.004\n",
      "1975/3300 (epoch 59), train_loss = -4.460, time/batch = 0.004\n",
      "1976/3300 (epoch 59), train_loss = -2.803, time/batch = 0.004\n",
      "1977/3300 (epoch 59), train_loss = -4.161, time/batch = 0.005\n",
      "1978/3300 (epoch 59), train_loss = -3.920, time/batch = 0.005\n",
      "1979/3300 (epoch 59), train_loss = -3.617, time/batch = 0.005\n",
      "1980/3300 (epoch 60), train_loss = -1.142, time/batch = 0.006\n",
      "1981/3300 (epoch 60), train_loss = -1.961, time/batch = 0.006\n",
      "1982/3300 (epoch 60), train_loss = -2.554, time/batch = 0.005\n",
      "1983/3300 (epoch 60), train_loss = -3.128, time/batch = 0.013\n",
      "1984/3300 (epoch 60), train_loss = -2.382, time/batch = 0.006\n",
      "1985/3300 (epoch 60), train_loss = -2.270, time/batch = 0.006\n",
      "1986/3300 (epoch 60), train_loss = -1.811, time/batch = 0.005\n",
      "1987/3300 (epoch 60), train_loss = -2.024, time/batch = 0.006\n",
      "1988/3300 (epoch 60), train_loss = -3.257, time/batch = 0.009\n",
      "1989/3300 (epoch 60), train_loss = -4.109, time/batch = 0.008\n",
      "1990/3300 (epoch 60), train_loss = -3.503, time/batch = 0.006\n",
      "1991/3300 (epoch 60), train_loss = -4.069, time/batch = 0.008\n",
      "1992/3300 (epoch 60), train_loss = -3.117, time/batch = 0.005\n",
      "1993/3300 (epoch 60), train_loss = -3.959, time/batch = 0.006\n",
      "1994/3300 (epoch 60), train_loss = -3.564, time/batch = 0.008\n",
      "1995/3300 (epoch 60), train_loss = -4.259, time/batch = 0.009\n",
      "1996/3300 (epoch 60), train_loss = -3.514, time/batch = 0.005\n",
      "1997/3300 (epoch 60), train_loss = -3.632, time/batch = 0.006\n",
      "1998/3300 (epoch 60), train_loss = -3.751, time/batch = 0.005\n",
      "1999/3300 (epoch 60), train_loss = -3.363, time/batch = 0.008\n",
      "2000/3300 (epoch 60), train_loss = -3.942, time/batch = 0.006\n",
      "2001/3300 (epoch 60), train_loss = -3.900, time/batch = 0.008\n",
      "2002/3300 (epoch 60), train_loss = -3.988, time/batch = 0.007\n",
      "2003/3300 (epoch 60), train_loss = -4.274, time/batch = 0.007\n",
      "2004/3300 (epoch 60), train_loss = -4.181, time/batch = 0.009\n",
      "2005/3300 (epoch 60), train_loss = -4.175, time/batch = 0.006\n",
      "2006/3300 (epoch 60), train_loss = -3.855, time/batch = 0.008\n",
      "2007/3300 (epoch 60), train_loss = -4.859, time/batch = 0.010\n",
      "2008/3300 (epoch 60), train_loss = -4.759, time/batch = 0.008\n",
      "2009/3300 (epoch 60), train_loss = -4.968, time/batch = 0.008\n",
      "2010/3300 (epoch 60), train_loss = -4.757, time/batch = 0.010\n",
      "2011/3300 (epoch 60), train_loss = -4.791, time/batch = 0.005\n",
      "2012/3300 (epoch 60), train_loss = -3.892, time/batch = 0.014\n",
      "2013/3300 (epoch 61), train_loss = -1.250, time/batch = 0.006\n",
      "2014/3300 (epoch 61), train_loss = -2.299, time/batch = 0.007\n",
      "2015/3300 (epoch 61), train_loss = -2.279, time/batch = 0.006\n",
      "2016/3300 (epoch 61), train_loss = -2.104, time/batch = 0.008\n",
      "2017/3300 (epoch 61), train_loss = -2.343, time/batch = 0.009\n",
      "2018/3300 (epoch 61), train_loss = -2.010, time/batch = 0.007\n",
      "2019/3300 (epoch 61), train_loss = -2.235, time/batch = 0.007\n",
      "2020/3300 (epoch 61), train_loss = -2.614, time/batch = 0.007\n",
      "2021/3300 (epoch 61), train_loss = -3.062, time/batch = 0.009\n",
      "2022/3300 (epoch 61), train_loss = -3.151, time/batch = 0.010\n",
      "2023/3300 (epoch 61), train_loss = -4.050, time/batch = 0.006\n",
      "2024/3300 (epoch 61), train_loss = -3.808, time/batch = 0.006\n",
      "2025/3300 (epoch 61), train_loss = -3.395, time/batch = 0.008\n",
      "2026/3300 (epoch 61), train_loss = -4.095, time/batch = 0.007\n",
      "2027/3300 (epoch 61), train_loss = -4.154, time/batch = 0.009\n",
      "2028/3300 (epoch 61), train_loss = -4.050, time/batch = 0.007\n",
      "2029/3300 (epoch 61), train_loss = -3.771, time/batch = 0.006\n",
      "2030/3300 (epoch 61), train_loss = -3.859, time/batch = 0.008\n",
      "2031/3300 (epoch 61), train_loss = -3.725, time/batch = 0.007\n",
      "2032/3300 (epoch 61), train_loss = -3.820, time/batch = 0.009\n",
      "2033/3300 (epoch 61), train_loss = -3.871, time/batch = 0.008\n",
      "2034/3300 (epoch 61), train_loss = -4.510, time/batch = 0.009\n",
      "2035/3300 (epoch 61), train_loss = -4.354, time/batch = 0.007\n",
      "2036/3300 (epoch 61), train_loss = -2.959, time/batch = 0.008\n",
      "2037/3300 (epoch 61), train_loss = -4.490, time/batch = 0.007\n",
      "2038/3300 (epoch 61), train_loss = -4.601, time/batch = 0.007\n",
      "2039/3300 (epoch 61), train_loss = -4.046, time/batch = 0.009\n",
      "2040/3300 (epoch 61), train_loss = -3.938, time/batch = 0.008\n",
      "2041/3300 (epoch 61), train_loss = -4.852, time/batch = 0.009\n",
      "2042/3300 (epoch 61), train_loss = -4.794, time/batch = 0.010\n",
      "2043/3300 (epoch 61), train_loss = -3.671, time/batch = 0.010\n",
      "2044/3300 (epoch 61), train_loss = -3.924, time/batch = 0.010\n",
      "2045/3300 (epoch 61), train_loss = -3.777, time/batch = 0.007\n",
      "2046/3300 (epoch 62), train_loss = -1.171, time/batch = 0.009\n",
      "2047/3300 (epoch 62), train_loss = -2.108, time/batch = 0.009\n",
      "2048/3300 (epoch 62), train_loss = -2.471, time/batch = 0.009\n",
      "2049/3300 (epoch 62), train_loss = -2.553, time/batch = 0.007\n",
      "2050/3300 (epoch 62), train_loss = -2.337, time/batch = 0.006\n",
      "2051/3300 (epoch 62), train_loss = -2.205, time/batch = 0.006\n",
      "2052/3300 (epoch 62), train_loss = -2.442, time/batch = 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/3300 (epoch 62), train_loss = -2.785, time/batch = 0.010\n",
      "2054/3300 (epoch 62), train_loss = -3.980, time/batch = 0.010\n",
      "2055/3300 (epoch 62), train_loss = -3.891, time/batch = 0.010\n",
      "2056/3300 (epoch 62), train_loss = -3.877, time/batch = 0.010\n",
      "2057/3300 (epoch 62), train_loss = -3.820, time/batch = 0.015\n",
      "2058/3300 (epoch 62), train_loss = -3.535, time/batch = 0.011\n",
      "2059/3300 (epoch 62), train_loss = -4.059, time/batch = 0.013\n",
      "2060/3300 (epoch 62), train_loss = -3.871, time/batch = 0.010\n",
      "2061/3300 (epoch 62), train_loss = -3.754, time/batch = 0.010\n",
      "2062/3300 (epoch 62), train_loss = -4.197, time/batch = 0.013\n",
      "2063/3300 (epoch 62), train_loss = -3.038, time/batch = 0.011\n",
      "2064/3300 (epoch 62), train_loss = -3.832, time/batch = 0.009\n",
      "2065/3300 (epoch 62), train_loss = -3.854, time/batch = 0.008\n",
      "2066/3300 (epoch 62), train_loss = -4.219, time/batch = 0.008\n",
      "2067/3300 (epoch 62), train_loss = -4.311, time/batch = 0.009\n",
      "2068/3300 (epoch 62), train_loss = -3.942, time/batch = 0.008\n",
      "2069/3300 (epoch 62), train_loss = -4.702, time/batch = 0.009\n",
      "2070/3300 (epoch 62), train_loss = -4.808, time/batch = 0.008\n",
      "2071/3300 (epoch 62), train_loss = -4.459, time/batch = 0.015\n",
      "2072/3300 (epoch 62), train_loss = -4.130, time/batch = 0.010\n",
      "2073/3300 (epoch 62), train_loss = -3.651, time/batch = 0.008\n",
      "2074/3300 (epoch 62), train_loss = -4.159, time/batch = 0.008\n",
      "2075/3300 (epoch 62), train_loss = -4.430, time/batch = 0.008\n",
      "2076/3300 (epoch 62), train_loss = -3.846, time/batch = 0.008\n",
      "2077/3300 (epoch 62), train_loss = -4.378, time/batch = 0.010\n",
      "2078/3300 (epoch 62), train_loss = -3.903, time/batch = 0.009\n",
      "2079/3300 (epoch 63), train_loss = -1.158, time/batch = 0.008\n",
      "2080/3300 (epoch 63), train_loss = -2.224, time/batch = 0.009\n",
      "2081/3300 (epoch 63), train_loss = -2.534, time/batch = 0.008\n",
      "2082/3300 (epoch 63), train_loss = -2.313, time/batch = 0.010\n",
      "2083/3300 (epoch 63), train_loss = -2.289, time/batch = 0.010\n",
      "2084/3300 (epoch 63), train_loss = -2.590, time/batch = 0.010\n",
      "2085/3300 (epoch 63), train_loss = -2.477, time/batch = 0.008\n",
      "2086/3300 (epoch 63), train_loss = -2.986, time/batch = 0.008\n",
      "2087/3300 (epoch 63), train_loss = -3.650, time/batch = 0.010\n",
      "2088/3300 (epoch 63), train_loss = -3.699, time/batch = 0.010\n",
      "2089/3300 (epoch 63), train_loss = -4.203, time/batch = 0.009\n",
      "2090/3300 (epoch 63), train_loss = -4.054, time/batch = 0.009\n",
      "2091/3300 (epoch 63), train_loss = -3.538, time/batch = 0.009\n",
      "2092/3300 (epoch 63), train_loss = -4.073, time/batch = 0.010\n",
      "2093/3300 (epoch 63), train_loss = -3.718, time/batch = 0.007\n",
      "2094/3300 (epoch 63), train_loss = -3.911, time/batch = 0.010\n",
      "2095/3300 (epoch 63), train_loss = -3.996, time/batch = 0.010\n",
      "2096/3300 (epoch 63), train_loss = -3.468, time/batch = 0.010\n",
      "2097/3300 (epoch 63), train_loss = -3.820, time/batch = 0.009\n",
      "2098/3300 (epoch 63), train_loss = -3.990, time/batch = 0.009\n",
      "2099/3300 (epoch 63), train_loss = -4.269, time/batch = 0.009\n",
      "2100/3300 (epoch 63), train_loss = -3.937, time/batch = 0.008\n",
      "2101/3300 (epoch 63), train_loss = -4.714, time/batch = 0.010\n",
      "2102/3300 (epoch 63), train_loss = -4.662, time/batch = 0.009\n",
      "2103/3300 (epoch 63), train_loss = -4.510, time/batch = 0.009\n",
      "2104/3300 (epoch 63), train_loss = -3.977, time/batch = 0.009\n",
      "2105/3300 (epoch 63), train_loss = -3.884, time/batch = 0.009\n",
      "2106/3300 (epoch 63), train_loss = -4.808, time/batch = 0.009\n",
      "2107/3300 (epoch 63), train_loss = -5.053, time/batch = 0.010\n",
      "2108/3300 (epoch 63), train_loss = -5.098, time/batch = 0.010\n",
      "2109/3300 (epoch 63), train_loss = -5.076, time/batch = 0.009\n",
      "2110/3300 (epoch 63), train_loss = -5.041, time/batch = 0.010\n",
      "2111/3300 (epoch 63), train_loss = -4.017, time/batch = 0.008\n",
      "2112/3300 (epoch 64), train_loss = -1.192, time/batch = 0.008\n",
      "2113/3300 (epoch 64), train_loss = -2.176, time/batch = 0.009\n",
      "2114/3300 (epoch 64), train_loss = -2.499, time/batch = 0.008\n",
      "2115/3300 (epoch 64), train_loss = -1.957, time/batch = 0.008\n",
      "2116/3300 (epoch 64), train_loss = -1.962, time/batch = 0.008\n",
      "2117/3300 (epoch 64), train_loss = -1.687, time/batch = 0.007\n",
      "2118/3300 (epoch 64), train_loss = -1.981, time/batch = 0.007\n",
      "2119/3300 (epoch 64), train_loss = -2.892, time/batch = 0.008\n",
      "2120/3300 (epoch 64), train_loss = -3.261, time/batch = 0.007\n",
      "2121/3300 (epoch 64), train_loss = -3.875, time/batch = 0.007\n",
      "2122/3300 (epoch 64), train_loss = -3.774, time/batch = 0.009\n",
      "2123/3300 (epoch 64), train_loss = -3.362, time/batch = 0.006\n",
      "2124/3300 (epoch 64), train_loss = -3.849, time/batch = 0.008\n",
      "2125/3300 (epoch 64), train_loss = -3.736, time/batch = 0.007\n",
      "2126/3300 (epoch 64), train_loss = -3.747, time/batch = 0.008\n",
      "2127/3300 (epoch 64), train_loss = -3.854, time/batch = 0.009\n",
      "2128/3300 (epoch 64), train_loss = -3.880, time/batch = 0.009\n",
      "2129/3300 (epoch 64), train_loss = -3.412, time/batch = 0.009\n",
      "2130/3300 (epoch 64), train_loss = -3.985, time/batch = 0.009\n",
      "2131/3300 (epoch 64), train_loss = -3.413, time/batch = 0.008\n",
      "2132/3300 (epoch 64), train_loss = -3.916, time/batch = 0.009\n",
      "2133/3300 (epoch 64), train_loss = -3.806, time/batch = 0.008\n",
      "2134/3300 (epoch 64), train_loss = -4.328, time/batch = 0.008\n",
      "2135/3300 (epoch 64), train_loss = -3.922, time/batch = 0.008\n",
      "2136/3300 (epoch 64), train_loss = -3.594, time/batch = 0.007\n",
      "2137/3300 (epoch 64), train_loss = -4.647, time/batch = 0.007\n",
      "2138/3300 (epoch 64), train_loss = -4.844, time/batch = 0.009\n",
      "2139/3300 (epoch 64), train_loss = -4.735, time/batch = 0.010\n",
      "2140/3300 (epoch 64), train_loss = -4.581, time/batch = 0.009\n",
      "2141/3300 (epoch 64), train_loss = -4.008, time/batch = 0.008\n",
      "2142/3300 (epoch 64), train_loss = -4.239, time/batch = 0.009\n",
      "2143/3300 (epoch 64), train_loss = -4.414, time/batch = 0.009\n",
      "2144/3300 (epoch 64), train_loss = -4.063, time/batch = 0.007\n",
      "2145/3300 (epoch 65), train_loss = -1.228, time/batch = 0.009\n",
      "2146/3300 (epoch 65), train_loss = -2.169, time/batch = 0.010\n",
      "2147/3300 (epoch 65), train_loss = -2.551, time/batch = 0.006\n",
      "2148/3300 (epoch 65), train_loss = -1.973, time/batch = 0.009\n",
      "2149/3300 (epoch 65), train_loss = -2.086, time/batch = 0.008\n",
      "2150/3300 (epoch 65), train_loss = -2.149, time/batch = 0.008\n",
      "2151/3300 (epoch 65), train_loss = -2.059, time/batch = 0.009\n",
      "2152/3300 (epoch 65), train_loss = -2.462, time/batch = 0.009\n",
      "2153/3300 (epoch 65), train_loss = -3.271, time/batch = 0.008\n",
      "2154/3300 (epoch 65), train_loss = -3.933, time/batch = 0.008\n",
      "2155/3300 (epoch 65), train_loss = -3.946, time/batch = 0.008\n",
      "2156/3300 (epoch 65), train_loss = -3.904, time/batch = 0.009\n",
      "2157/3300 (epoch 65), train_loss = -3.619, time/batch = 0.008\n",
      "2158/3300 (epoch 65), train_loss = -3.939, time/batch = 0.009\n",
      "2159/3300 (epoch 65), train_loss = -4.387, time/batch = 0.010\n",
      "2160/3300 (epoch 65), train_loss = -4.003, time/batch = 0.010\n",
      "2161/3300 (epoch 65), train_loss = -3.484, time/batch = 0.010\n",
      "2162/3300 (epoch 65), train_loss = -3.756, time/batch = 0.009\n",
      "2163/3300 (epoch 65), train_loss = -4.210, time/batch = 0.008\n",
      "2164/3300 (epoch 65), train_loss = -4.266, time/batch = 0.009\n",
      "2165/3300 (epoch 65), train_loss = -3.316, time/batch = 0.009\n",
      "2166/3300 (epoch 65), train_loss = -3.834, time/batch = 0.008\n",
      "2167/3300 (epoch 65), train_loss = -3.937, time/batch = 0.009\n",
      "2168/3300 (epoch 65), train_loss = -4.071, time/batch = 0.014\n",
      "2169/3300 (epoch 65), train_loss = -4.454, time/batch = 0.011\n",
      "2170/3300 (epoch 65), train_loss = -4.890, time/batch = 0.008\n",
      "2171/3300 (epoch 65), train_loss = -4.858, time/batch = 0.007\n",
      "2172/3300 (epoch 65), train_loss = -4.571, time/batch = 0.007\n",
      "2173/3300 (epoch 65), train_loss = -4.252, time/batch = 0.007\n",
      "2174/3300 (epoch 65), train_loss = -3.577, time/batch = 0.008\n",
      "2175/3300 (epoch 65), train_loss = -4.213, time/batch = 0.010\n",
      "2176/3300 (epoch 65), train_loss = -4.929, time/batch = 0.008\n",
      "2177/3300 (epoch 65), train_loss = -4.980, time/batch = 0.010\n",
      "2178/3300 (epoch 66), train_loss = -1.211, time/batch = 0.009\n",
      "2179/3300 (epoch 66), train_loss = -2.031, time/batch = 0.010\n",
      "2180/3300 (epoch 66), train_loss = -2.676, time/batch = 0.009\n",
      "2181/3300 (epoch 66), train_loss = -2.205, time/batch = 0.009\n",
      "2182/3300 (epoch 66), train_loss = -2.070, time/batch = 0.013\n",
      "2183/3300 (epoch 66), train_loss = -2.159, time/batch = 0.010\n",
      "2184/3300 (epoch 66), train_loss = -2.157, time/batch = 0.010\n",
      "2185/3300 (epoch 66), train_loss = -2.526, time/batch = 0.010\n",
      "2186/3300 (epoch 66), train_loss = -2.920, time/batch = 0.013\n",
      "2187/3300 (epoch 66), train_loss = -4.280, time/batch = 0.008\n",
      "2188/3300 (epoch 66), train_loss = -3.386, time/batch = 0.008\n",
      "2189/3300 (epoch 66), train_loss = -3.772, time/batch = 0.010\n",
      "2190/3300 (epoch 66), train_loss = -3.830, time/batch = 0.015\n",
      "2191/3300 (epoch 66), train_loss = -4.152, time/batch = 0.010\n",
      "2192/3300 (epoch 66), train_loss = -4.044, time/batch = 0.009\n",
      "2193/3300 (epoch 66), train_loss = -3.674, time/batch = 0.008\n",
      "2194/3300 (epoch 66), train_loss = -4.099, time/batch = 0.008\n",
      "2195/3300 (epoch 66), train_loss = -3.373, time/batch = 0.012\n",
      "2196/3300 (epoch 66), train_loss = -3.860, time/batch = 0.007\n",
      "2197/3300 (epoch 66), train_loss = -3.919, time/batch = 0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2198/3300 (epoch 66), train_loss = -3.830, time/batch = 0.013\n",
      "2199/3300 (epoch 66), train_loss = -4.442, time/batch = 0.008\n",
      "2200/3300 (epoch 66), train_loss = -4.790, time/batch = 0.005\n",
      "2201/3300 (epoch 66), train_loss = -4.022, time/batch = 0.006\n",
      "2202/3300 (epoch 66), train_loss = -4.022, time/batch = 0.005\n",
      "2203/3300 (epoch 66), train_loss = -4.400, time/batch = 0.006\n",
      "2204/3300 (epoch 66), train_loss = -4.970, time/batch = 0.005\n",
      "2205/3300 (epoch 66), train_loss = -4.182, time/batch = 0.005\n",
      "2206/3300 (epoch 66), train_loss = -3.998, time/batch = 0.006\n",
      "2207/3300 (epoch 66), train_loss = -3.996, time/batch = 0.004\n",
      "2208/3300 (epoch 66), train_loss = -4.156, time/batch = 0.005\n",
      "2209/3300 (epoch 66), train_loss = -3.407, time/batch = 0.007\n",
      "2210/3300 (epoch 66), train_loss = -4.365, time/batch = 0.005\n",
      "2211/3300 (epoch 67), train_loss = -1.266, time/batch = 0.007\n",
      "2212/3300 (epoch 67), train_loss = -2.065, time/batch = 0.009\n",
      "2213/3300 (epoch 67), train_loss = -2.597, time/batch = 0.005\n",
      "2214/3300 (epoch 67), train_loss = -2.328, time/batch = 0.005\n",
      "2215/3300 (epoch 67), train_loss = -1.700, time/batch = 0.006\n",
      "2216/3300 (epoch 67), train_loss = -1.997, time/batch = 0.006\n",
      "2217/3300 (epoch 67), train_loss = -2.189, time/batch = 0.004\n",
      "2218/3300 (epoch 67), train_loss = -2.367, time/batch = 0.005\n",
      "2219/3300 (epoch 67), train_loss = -3.509, time/batch = 0.006\n",
      "2220/3300 (epoch 67), train_loss = -4.475, time/batch = 0.005\n",
      "2221/3300 (epoch 67), train_loss = -4.027, time/batch = 0.012\n",
      "2222/3300 (epoch 67), train_loss = -3.793, time/batch = 0.007\n",
      "2223/3300 (epoch 67), train_loss = -3.969, time/batch = 0.004\n",
      "2224/3300 (epoch 67), train_loss = -3.395, time/batch = 0.005\n",
      "2225/3300 (epoch 67), train_loss = -4.243, time/batch = 0.010\n",
      "2226/3300 (epoch 67), train_loss = -4.178, time/batch = 0.006\n",
      "2227/3300 (epoch 67), train_loss = -3.915, time/batch = 0.005\n",
      "2228/3300 (epoch 67), train_loss = -4.091, time/batch = 0.006\n",
      "2229/3300 (epoch 67), train_loss = -3.712, time/batch = 0.006\n",
      "2230/3300 (epoch 67), train_loss = -3.635, time/batch = 0.005\n",
      "2231/3300 (epoch 67), train_loss = -4.044, time/batch = 0.005\n",
      "2232/3300 (epoch 67), train_loss = -4.006, time/batch = 0.006\n",
      "2233/3300 (epoch 67), train_loss = -3.919, time/batch = 0.005\n",
      "2234/3300 (epoch 67), train_loss = -4.084, time/batch = 0.005\n",
      "2235/3300 (epoch 67), train_loss = -4.840, time/batch = 0.005\n",
      "2236/3300 (epoch 67), train_loss = -4.913, time/batch = 0.005\n",
      "2237/3300 (epoch 67), train_loss = -4.887, time/batch = 0.005\n",
      "2238/3300 (epoch 67), train_loss = -4.711, time/batch = 0.005\n",
      "2239/3300 (epoch 67), train_loss = -4.200, time/batch = 0.005\n",
      "2240/3300 (epoch 67), train_loss = -3.989, time/batch = 0.006\n",
      "2241/3300 (epoch 67), train_loss = -5.191, time/batch = 0.006\n",
      "2242/3300 (epoch 67), train_loss = -5.074, time/batch = 0.005\n",
      "2243/3300 (epoch 67), train_loss = -5.251, time/batch = 0.005\n",
      "2244/3300 (epoch 68), train_loss = -1.229, time/batch = 0.005\n",
      "2245/3300 (epoch 68), train_loss = -2.334, time/batch = 0.007\n",
      "2246/3300 (epoch 68), train_loss = -2.511, time/batch = 0.007\n",
      "2247/3300 (epoch 68), train_loss = -2.153, time/batch = 0.011\n",
      "2248/3300 (epoch 68), train_loss = -1.516, time/batch = 0.009\n",
      "2249/3300 (epoch 68), train_loss = -1.806, time/batch = 0.005\n",
      "2250/3300 (epoch 68), train_loss = -1.610, time/batch = 0.006\n",
      "2251/3300 (epoch 68), train_loss = -2.162, time/batch = 0.012\n",
      "2252/3300 (epoch 68), train_loss = -3.563, time/batch = 0.008\n",
      "2253/3300 (epoch 68), train_loss = -3.905, time/batch = 0.006\n",
      "2254/3300 (epoch 68), train_loss = -3.673, time/batch = 0.005\n",
      "2255/3300 (epoch 68), train_loss = -4.022, time/batch = 0.005\n",
      "2256/3300 (epoch 68), train_loss = -4.291, time/batch = 0.006\n",
      "2257/3300 (epoch 68), train_loss = -4.256, time/batch = 0.022\n",
      "2258/3300 (epoch 68), train_loss = -3.735, time/batch = 0.013\n",
      "2259/3300 (epoch 68), train_loss = -4.314, time/batch = 0.008\n",
      "2260/3300 (epoch 68), train_loss = -4.073, time/batch = 0.005\n",
      "2261/3300 (epoch 68), train_loss = -2.962, time/batch = 0.008\n",
      "2262/3300 (epoch 68), train_loss = -4.011, time/batch = 0.010\n",
      "2263/3300 (epoch 68), train_loss = -3.822, time/batch = 0.016\n",
      "2264/3300 (epoch 68), train_loss = -4.387, time/batch = 0.010\n",
      "2265/3300 (epoch 68), train_loss = -4.051, time/batch = 0.009\n",
      "2266/3300 (epoch 68), train_loss = -4.266, time/batch = 0.014\n",
      "2267/3300 (epoch 68), train_loss = -4.990, time/batch = 0.008\n",
      "2268/3300 (epoch 68), train_loss = -4.424, time/batch = 0.008\n",
      "2269/3300 (epoch 68), train_loss = -3.566, time/batch = 0.008\n",
      "2270/3300 (epoch 68), train_loss = -4.461, time/batch = 0.011\n",
      "2271/3300 (epoch 68), train_loss = -5.222, time/batch = 0.009\n",
      "2272/3300 (epoch 68), train_loss = -5.097, time/batch = 0.005\n",
      "2273/3300 (epoch 68), train_loss = -5.271, time/batch = 0.005\n",
      "2274/3300 (epoch 68), train_loss = -3.686, time/batch = 0.013\n",
      "2275/3300 (epoch 68), train_loss = -4.171, time/batch = 0.008\n",
      "2276/3300 (epoch 68), train_loss = -4.191, time/batch = 0.009\n",
      "2277/3300 (epoch 69), train_loss = -1.220, time/batch = 0.005\n",
      "2278/3300 (epoch 69), train_loss = -2.076, time/batch = 0.012\n",
      "2279/3300 (epoch 69), train_loss = -2.499, time/batch = 0.008\n",
      "2280/3300 (epoch 69), train_loss = -2.301, time/batch = 0.009\n",
      "2281/3300 (epoch 69), train_loss = -1.611, time/batch = 0.008\n",
      "2282/3300 (epoch 69), train_loss = -1.837, time/batch = 0.006\n",
      "2283/3300 (epoch 69), train_loss = -2.120, time/batch = 0.009\n",
      "2284/3300 (epoch 69), train_loss = -3.672, time/batch = 0.013\n",
      "2285/3300 (epoch 69), train_loss = -3.994, time/batch = 0.006\n",
      "2286/3300 (epoch 69), train_loss = -3.980, time/batch = 0.006\n",
      "2287/3300 (epoch 69), train_loss = -3.885, time/batch = 0.005\n",
      "2288/3300 (epoch 69), train_loss = -4.012, time/batch = 0.013\n",
      "2289/3300 (epoch 69), train_loss = -3.923, time/batch = 0.010\n",
      "2290/3300 (epoch 69), train_loss = -4.053, time/batch = 0.005\n",
      "2291/3300 (epoch 69), train_loss = -4.303, time/batch = 0.006\n",
      "2292/3300 (epoch 69), train_loss = -2.922, time/batch = 0.011\n",
      "2293/3300 (epoch 69), train_loss = -3.146, time/batch = 0.007\n",
      "2294/3300 (epoch 69), train_loss = -3.909, time/batch = 0.011\n",
      "2295/3300 (epoch 69), train_loss = -3.984, time/batch = 0.008\n",
      "2296/3300 (epoch 69), train_loss = -4.515, time/batch = 0.007\n",
      "2297/3300 (epoch 69), train_loss = -4.429, time/batch = 0.012\n",
      "2298/3300 (epoch 69), train_loss = -3.840, time/batch = 0.011\n",
      "2299/3300 (epoch 69), train_loss = -4.869, time/batch = 0.009\n",
      "2300/3300 (epoch 69), train_loss = -4.742, time/batch = 0.009\n",
      "2301/3300 (epoch 69), train_loss = -3.988, time/batch = 0.025\n",
      "2302/3300 (epoch 69), train_loss = -4.219, time/batch = 0.010\n",
      "2303/3300 (epoch 69), train_loss = -4.964, time/batch = 0.010\n",
      "2304/3300 (epoch 69), train_loss = -4.507, time/batch = 0.014\n",
      "2305/3300 (epoch 69), train_loss = -3.891, time/batch = 0.017\n",
      "2306/3300 (epoch 69), train_loss = -4.274, time/batch = 0.016\n",
      "2307/3300 (epoch 69), train_loss = -4.115, time/batch = 0.010\n",
      "2308/3300 (epoch 69), train_loss = -3.611, time/batch = 0.009\n",
      "2309/3300 (epoch 69), train_loss = -4.235, time/batch = 0.008\n",
      "2310/3300 (epoch 70), train_loss = -1.224, time/batch = 0.009\n",
      "2311/3300 (epoch 70), train_loss = -2.377, time/batch = 0.007\n",
      "2312/3300 (epoch 70), train_loss = -2.651, time/batch = 0.008\n",
      "2313/3300 (epoch 70), train_loss = -2.279, time/batch = 0.007\n",
      "2314/3300 (epoch 70), train_loss = -2.223, time/batch = 0.009\n",
      "2315/3300 (epoch 70), train_loss = -0.946, time/batch = 0.007\n",
      "2316/3300 (epoch 70), train_loss = -1.734, time/batch = 0.007\n",
      "2317/3300 (epoch 70), train_loss = -1.677, time/batch = 0.009\n",
      "2318/3300 (epoch 70), train_loss = -3.468, time/batch = 0.009\n",
      "2319/3300 (epoch 70), train_loss = -3.813, time/batch = 0.007\n",
      "2320/3300 (epoch 70), train_loss = -3.995, time/batch = 0.012\n",
      "2321/3300 (epoch 70), train_loss = -4.392, time/batch = 0.009\n",
      "2322/3300 (epoch 70), train_loss = -3.960, time/batch = 0.007\n",
      "2323/3300 (epoch 70), train_loss = -3.867, time/batch = 0.007\n",
      "2324/3300 (epoch 70), train_loss = -4.485, time/batch = 0.007\n",
      "2325/3300 (epoch 70), train_loss = -3.876, time/batch = 0.010\n",
      "2326/3300 (epoch 70), train_loss = -3.639, time/batch = 0.009\n",
      "2327/3300 (epoch 70), train_loss = -3.999, time/batch = 0.005\n",
      "2328/3300 (epoch 70), train_loss = -3.900, time/batch = 0.010\n",
      "2329/3300 (epoch 70), train_loss = -3.700, time/batch = 0.010\n",
      "2330/3300 (epoch 70), train_loss = -3.410, time/batch = 0.008\n",
      "2331/3300 (epoch 70), train_loss = -4.200, time/batch = 0.012\n",
      "2332/3300 (epoch 70), train_loss = -4.342, time/batch = 0.011\n",
      "2333/3300 (epoch 70), train_loss = -3.817, time/batch = 0.009\n",
      "2334/3300 (epoch 70), train_loss = -4.925, time/batch = 0.013\n",
      "2335/3300 (epoch 70), train_loss = -4.997, time/batch = 0.018\n",
      "2336/3300 (epoch 70), train_loss = -4.462, time/batch = 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337/3300 (epoch 70), train_loss = -3.792, time/batch = 0.007\n",
      "2338/3300 (epoch 70), train_loss = -4.597, time/batch = 0.015\n",
      "2339/3300 (epoch 70), train_loss = -4.640, time/batch = 0.010\n",
      "2340/3300 (epoch 70), train_loss = -3.852, time/batch = 0.009\n",
      "2341/3300 (epoch 70), train_loss = -4.307, time/batch = 0.009\n",
      "2342/3300 (epoch 70), train_loss = -4.184, time/batch = 0.012\n",
      "2343/3300 (epoch 71), train_loss = -1.161, time/batch = 0.008\n",
      "2344/3300 (epoch 71), train_loss = -2.250, time/batch = 0.008\n",
      "2345/3300 (epoch 71), train_loss = -2.437, time/batch = 0.012\n",
      "2346/3300 (epoch 71), train_loss = -2.012, time/batch = 0.007\n",
      "2347/3300 (epoch 71), train_loss = -1.926, time/batch = 0.006\n",
      "2348/3300 (epoch 71), train_loss = -1.476, time/batch = 0.005\n",
      "2349/3300 (epoch 71), train_loss = -1.918, time/batch = 0.007\n",
      "2350/3300 (epoch 71), train_loss = -2.684, time/batch = 0.006\n",
      "2351/3300 (epoch 71), train_loss = -3.231, time/batch = 0.007\n",
      "2352/3300 (epoch 71), train_loss = -3.772, time/batch = 0.006\n",
      "2353/3300 (epoch 71), train_loss = -4.027, time/batch = 0.005\n",
      "2354/3300 (epoch 71), train_loss = -3.737, time/batch = 0.006\n",
      "2355/3300 (epoch 71), train_loss = -4.217, time/batch = 0.007\n",
      "2356/3300 (epoch 71), train_loss = -4.445, time/batch = 0.006\n",
      "2357/3300 (epoch 71), train_loss = -3.834, time/batch = 0.012\n",
      "2358/3300 (epoch 71), train_loss = -4.042, time/batch = 0.016\n",
      "2359/3300 (epoch 71), train_loss = -3.943, time/batch = 0.007\n",
      "2360/3300 (epoch 71), train_loss = -3.205, time/batch = 0.004\n",
      "2361/3300 (epoch 71), train_loss = -3.858, time/batch = 0.006\n",
      "2362/3300 (epoch 71), train_loss = -4.062, time/batch = 0.005\n",
      "2363/3300 (epoch 71), train_loss = -3.926, time/batch = 0.005\n",
      "2364/3300 (epoch 71), train_loss = -4.334, time/batch = 0.005\n",
      "2365/3300 (epoch 71), train_loss = -4.392, time/batch = 0.005\n",
      "2366/3300 (epoch 71), train_loss = -4.456, time/batch = 0.006\n",
      "2367/3300 (epoch 71), train_loss = -3.731, time/batch = 0.005\n",
      "2368/3300 (epoch 71), train_loss = -4.401, time/batch = 0.006\n",
      "2369/3300 (epoch 71), train_loss = -4.303, time/batch = 0.005\n",
      "2370/3300 (epoch 71), train_loss = -4.061, time/batch = 0.004\n",
      "2371/3300 (epoch 71), train_loss = -5.022, time/batch = 0.006\n",
      "2372/3300 (epoch 71), train_loss = -5.248, time/batch = 0.007\n",
      "2373/3300 (epoch 71), train_loss = -4.171, time/batch = 0.006\n",
      "2374/3300 (epoch 71), train_loss = -4.667, time/batch = 0.006\n",
      "2375/3300 (epoch 71), train_loss = -3.611, time/batch = 0.005\n",
      "2376/3300 (epoch 72), train_loss = -1.181, time/batch = 0.006\n",
      "2377/3300 (epoch 72), train_loss = -2.151, time/batch = 0.008\n",
      "2378/3300 (epoch 72), train_loss = -2.513, time/batch = 0.006\n",
      "2379/3300 (epoch 72), train_loss = -2.256, time/batch = 0.007\n",
      "2380/3300 (epoch 72), train_loss = -1.290, time/batch = 0.006\n",
      "2381/3300 (epoch 72), train_loss = -1.638, time/batch = 0.005\n",
      "2382/3300 (epoch 72), train_loss = -1.727, time/batch = 0.007\n",
      "2383/3300 (epoch 72), train_loss = -2.935, time/batch = 0.007\n",
      "2384/3300 (epoch 72), train_loss = -4.210, time/batch = 0.012\n",
      "2385/3300 (epoch 72), train_loss = -4.116, time/batch = 0.007\n",
      "2386/3300 (epoch 72), train_loss = -3.903, time/batch = 0.007\n",
      "2387/3300 (epoch 72), train_loss = -3.919, time/batch = 0.013\n",
      "2388/3300 (epoch 72), train_loss = -3.976, time/batch = 0.009\n",
      "2389/3300 (epoch 72), train_loss = -4.388, time/batch = 0.012\n",
      "2390/3300 (epoch 72), train_loss = -3.983, time/batch = 0.008\n",
      "2391/3300 (epoch 72), train_loss = -3.927, time/batch = 0.010\n",
      "2392/3300 (epoch 72), train_loss = -4.101, time/batch = 0.006\n",
      "2393/3300 (epoch 72), train_loss = -3.796, time/batch = 0.013\n",
      "2394/3300 (epoch 72), train_loss = -3.418, time/batch = 0.016\n",
      "2395/3300 (epoch 72), train_loss = -3.813, time/batch = 0.005\n",
      "2396/3300 (epoch 72), train_loss = -3.830, time/batch = 0.014\n",
      "2397/3300 (epoch 72), train_loss = -4.019, time/batch = 0.014\n",
      "2398/3300 (epoch 72), train_loss = -5.012, time/batch = 0.005\n",
      "2399/3300 (epoch 72), train_loss = -5.004, time/batch = 0.004\n",
      "2400/3300 (epoch 72), train_loss = -5.140, time/batch = 0.006\n",
      "2401/3300 (epoch 72), train_loss = -5.054, time/batch = 0.004\n",
      "2402/3300 (epoch 72), train_loss = -5.165, time/batch = 0.004\n",
      "2403/3300 (epoch 72), train_loss = -4.992, time/batch = 0.008\n",
      "2404/3300 (epoch 72), train_loss = -4.386, time/batch = 0.004\n",
      "2405/3300 (epoch 72), train_loss = -3.986, time/batch = 0.008\n",
      "2406/3300 (epoch 72), train_loss = -4.234, time/batch = 0.008\n",
      "2407/3300 (epoch 72), train_loss = -5.069, time/batch = 0.006\n",
      "2408/3300 (epoch 72), train_loss = -5.353, time/batch = 0.008\n",
      "2409/3300 (epoch 73), train_loss = -1.184, time/batch = 0.008\n",
      "2410/3300 (epoch 73), train_loss = -2.133, time/batch = 0.012\n",
      "2411/3300 (epoch 73), train_loss = -2.196, time/batch = 0.010\n",
      "2412/3300 (epoch 73), train_loss = -2.728, time/batch = 0.007\n",
      "2413/3300 (epoch 73), train_loss = -1.926, time/batch = 0.012\n",
      "2414/3300 (epoch 73), train_loss = -1.761, time/batch = 0.008\n",
      "2415/3300 (epoch 73), train_loss = -1.715, time/batch = 0.011\n",
      "2416/3300 (epoch 73), train_loss = -2.022, time/batch = 0.007\n",
      "2417/3300 (epoch 73), train_loss = -4.289, time/batch = 0.005\n",
      "2418/3300 (epoch 73), train_loss = -4.162, time/batch = 0.012\n",
      "2419/3300 (epoch 73), train_loss = -3.866, time/batch = 0.007\n",
      "2420/3300 (epoch 73), train_loss = -3.903, time/batch = 0.008\n",
      "2421/3300 (epoch 73), train_loss = -3.821, time/batch = 0.006\n",
      "2422/3300 (epoch 73), train_loss = -4.274, time/batch = 0.013\n",
      "2423/3300 (epoch 73), train_loss = -4.399, time/batch = 0.005\n",
      "2424/3300 (epoch 73), train_loss = -3.492, time/batch = 0.008\n",
      "2425/3300 (epoch 73), train_loss = -3.827, time/batch = 0.006\n",
      "2426/3300 (epoch 73), train_loss = -3.744, time/batch = 0.007\n",
      "2427/3300 (epoch 73), train_loss = -3.797, time/batch = 0.005\n",
      "2428/3300 (epoch 73), train_loss = -4.285, time/batch = 0.005\n",
      "2429/3300 (epoch 73), train_loss = -4.476, time/batch = 0.009\n",
      "2430/3300 (epoch 73), train_loss = -4.585, time/batch = 0.007\n",
      "2431/3300 (epoch 73), train_loss = -4.025, time/batch = 0.005\n",
      "2432/3300 (epoch 73), train_loss = -4.995, time/batch = 0.004\n",
      "2433/3300 (epoch 73), train_loss = -4.984, time/batch = 0.004\n",
      "2434/3300 (epoch 73), train_loss = -5.197, time/batch = 0.006\n",
      "2435/3300 (epoch 73), train_loss = -5.111, time/batch = 0.009\n",
      "2436/3300 (epoch 73), train_loss = -4.083, time/batch = 0.008\n",
      "2437/3300 (epoch 73), train_loss = -4.025, time/batch = 0.014\n",
      "2438/3300 (epoch 73), train_loss = -4.189, time/batch = 0.008\n",
      "2439/3300 (epoch 73), train_loss = -4.710, time/batch = 0.007\n",
      "2440/3300 (epoch 73), train_loss = -4.933, time/batch = 0.010\n",
      "2441/3300 (epoch 73), train_loss = -3.904, time/batch = 0.007\n",
      "2442/3300 (epoch 74), train_loss = -1.225, time/batch = 0.004\n",
      "2443/3300 (epoch 74), train_loss = -2.414, time/batch = 0.004\n",
      "2444/3300 (epoch 74), train_loss = -2.219, time/batch = 0.004\n",
      "2445/3300 (epoch 74), train_loss = -1.948, time/batch = 0.004\n",
      "2446/3300 (epoch 74), train_loss = -1.757, time/batch = 0.004\n",
      "2447/3300 (epoch 74), train_loss = -1.388, time/batch = 0.004\n",
      "2448/3300 (epoch 74), train_loss = -1.832, time/batch = 0.004\n",
      "2449/3300 (epoch 74), train_loss = -1.636, time/batch = 0.004\n",
      "2450/3300 (epoch 74), train_loss = -3.863, time/batch = 0.004\n",
      "2451/3300 (epoch 74), train_loss = -3.653, time/batch = 0.004\n",
      "2452/3300 (epoch 74), train_loss = -3.745, time/batch = 0.004\n",
      "2453/3300 (epoch 74), train_loss = -4.041, time/batch = 0.004\n",
      "2454/3300 (epoch 74), train_loss = -4.157, time/batch = 0.004\n",
      "2455/3300 (epoch 74), train_loss = -4.269, time/batch = 0.004\n",
      "2456/3300 (epoch 74), train_loss = -4.119, time/batch = 0.004\n",
      "2457/3300 (epoch 74), train_loss = -3.784, time/batch = 0.004\n",
      "2458/3300 (epoch 74), train_loss = -4.240, time/batch = 0.004\n",
      "2459/3300 (epoch 74), train_loss = -3.727, time/batch = 0.004\n",
      "2460/3300 (epoch 74), train_loss = -4.001, time/batch = 0.005\n",
      "2461/3300 (epoch 74), train_loss = -3.883, time/batch = 0.004\n",
      "2462/3300 (epoch 74), train_loss = -4.189, time/batch = 0.004\n",
      "2463/3300 (epoch 74), train_loss = -4.473, time/batch = 0.005\n",
      "2464/3300 (epoch 74), train_loss = -4.608, time/batch = 0.004\n",
      "2465/3300 (epoch 74), train_loss = -4.274, time/batch = 0.004\n",
      "2466/3300 (epoch 74), train_loss = -4.087, time/batch = 0.004\n",
      "2467/3300 (epoch 74), train_loss = -5.089, time/batch = 0.004\n",
      "2468/3300 (epoch 74), train_loss = -5.024, time/batch = 0.005\n",
      "2469/3300 (epoch 74), train_loss = -5.141, time/batch = 0.005\n",
      "2470/3300 (epoch 74), train_loss = -5.120, time/batch = 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2471/3300 (epoch 74), train_loss = -4.741, time/batch = 0.006\n",
      "2472/3300 (epoch 74), train_loss = -3.873, time/batch = 0.008\n",
      "2473/3300 (epoch 74), train_loss = -4.341, time/batch = 0.008\n",
      "2474/3300 (epoch 74), train_loss = -4.442, time/batch = 0.005\n",
      "2475/3300 (epoch 75), train_loss = -1.263, time/batch = 0.008\n",
      "2476/3300 (epoch 75), train_loss = -2.127, time/batch = 0.005\n",
      "2477/3300 (epoch 75), train_loss = -2.396, time/batch = 0.006\n",
      "2478/3300 (epoch 75), train_loss = -2.742, time/batch = 0.009\n",
      "2479/3300 (epoch 75), train_loss = -1.615, time/batch = 0.007\n",
      "2480/3300 (epoch 75), train_loss = -1.153, time/batch = 0.009\n",
      "2481/3300 (epoch 75), train_loss = -1.820, time/batch = 0.006\n",
      "2482/3300 (epoch 75), train_loss = -1.637, time/batch = 0.007\n",
      "2483/3300 (epoch 75), train_loss = -3.702, time/batch = 0.007\n",
      "2484/3300 (epoch 75), train_loss = -4.011, time/batch = 0.006\n",
      "2485/3300 (epoch 75), train_loss = -4.038, time/batch = 0.007\n",
      "2486/3300 (epoch 75), train_loss = -3.736, time/batch = 0.006\n",
      "2487/3300 (epoch 75), train_loss = -4.427, time/batch = 0.008\n",
      "2488/3300 (epoch 75), train_loss = -4.609, time/batch = 0.010\n",
      "2489/3300 (epoch 75), train_loss = -3.995, time/batch = 0.009\n",
      "2490/3300 (epoch 75), train_loss = -4.014, time/batch = 0.010\n",
      "2491/3300 (epoch 75), train_loss = -4.204, time/batch = 0.008\n",
      "2492/3300 (epoch 75), train_loss = -3.368, time/batch = 0.007\n",
      "2493/3300 (epoch 75), train_loss = -3.730, time/batch = 0.015\n",
      "2494/3300 (epoch 75), train_loss = -3.967, time/batch = 0.013\n",
      "2495/3300 (epoch 75), train_loss = -3.953, time/batch = 0.009\n",
      "2496/3300 (epoch 75), train_loss = -4.383, time/batch = 0.009\n",
      "2497/3300 (epoch 75), train_loss = -4.289, time/batch = 0.006\n",
      "2498/3300 (epoch 75), train_loss = -4.641, time/batch = 0.013\n",
      "2499/3300 (epoch 75), train_loss = -5.145, time/batch = 0.016\n",
      "2500/3300 (epoch 75), train_loss = -5.126, time/batch = 0.022\n",
      "2501/3300 (epoch 75), train_loss = -4.776, time/batch = 0.013\n",
      "2502/3300 (epoch 75), train_loss = -3.128, time/batch = 0.016\n",
      "2503/3300 (epoch 75), train_loss = -4.085, time/batch = 0.016\n",
      "2504/3300 (epoch 75), train_loss = -4.760, time/batch = 0.016\n",
      "2505/3300 (epoch 75), train_loss = -4.103, time/batch = 0.016\n",
      "2506/3300 (epoch 75), train_loss = -4.512, time/batch = 0.012\n",
      "2507/3300 (epoch 75), train_loss = -3.483, time/batch = 0.008\n",
      "2508/3300 (epoch 76), train_loss = -1.220, time/batch = 0.008\n",
      "2509/3300 (epoch 76), train_loss = -1.927, time/batch = 0.009\n",
      "2510/3300 (epoch 76), train_loss = -2.278, time/batch = 0.009\n",
      "2511/3300 (epoch 76), train_loss = -2.324, time/batch = 0.005\n",
      "2512/3300 (epoch 76), train_loss = -1.797, time/batch = 0.006\n",
      "2513/3300 (epoch 76), train_loss = -1.510, time/batch = 0.008\n",
      "2514/3300 (epoch 76), train_loss = -2.107, time/batch = 0.032\n",
      "2515/3300 (epoch 76), train_loss = -2.346, time/batch = 0.007\n",
      "2516/3300 (epoch 76), train_loss = -3.328, time/batch = 0.008\n",
      "2517/3300 (epoch 76), train_loss = -3.728, time/batch = 0.010\n",
      "2518/3300 (epoch 76), train_loss = -4.183, time/batch = 0.010\n",
      "2519/3300 (epoch 76), train_loss = -4.045, time/batch = 0.006\n",
      "2520/3300 (epoch 76), train_loss = -3.984, time/batch = 0.013\n",
      "2521/3300 (epoch 76), train_loss = -4.320, time/batch = 0.010\n",
      "2522/3300 (epoch 76), train_loss = -4.013, time/batch = 0.017\n",
      "2523/3300 (epoch 76), train_loss = -3.790, time/batch = 0.009\n",
      "2524/3300 (epoch 76), train_loss = -4.057, time/batch = 0.011\n",
      "2525/3300 (epoch 76), train_loss = -3.852, time/batch = 0.008\n",
      "2526/3300 (epoch 76), train_loss = -3.856, time/batch = 0.015\n",
      "2527/3300 (epoch 76), train_loss = -3.355, time/batch = 0.008\n",
      "2528/3300 (epoch 76), train_loss = -4.117, time/batch = 0.011\n",
      "2529/3300 (epoch 76), train_loss = -3.766, time/batch = 0.007\n",
      "2530/3300 (epoch 76), train_loss = -3.947, time/batch = 0.006\n",
      "2531/3300 (epoch 76), train_loss = -4.778, time/batch = 0.012\n",
      "2532/3300 (epoch 76), train_loss = -4.955, time/batch = 0.013\n",
      "2533/3300 (epoch 76), train_loss = -4.492, time/batch = 0.005\n",
      "2534/3300 (epoch 76), train_loss = -3.652, time/batch = 0.005\n",
      "2535/3300 (epoch 76), train_loss = -4.124, time/batch = 0.014\n",
      "2536/3300 (epoch 76), train_loss = -4.547, time/batch = 0.016\n",
      "2537/3300 (epoch 76), train_loss = -3.672, time/batch = 0.005\n",
      "2538/3300 (epoch 76), train_loss = -4.207, time/batch = 0.013\n",
      "2539/3300 (epoch 76), train_loss = -4.445, time/batch = 0.013\n",
      "2540/3300 (epoch 76), train_loss = -3.936, time/batch = 0.008\n",
      "2541/3300 (epoch 77), train_loss = -1.170, time/batch = 0.007\n",
      "2542/3300 (epoch 77), train_loss = -2.382, time/batch = 0.005\n",
      "2543/3300 (epoch 77), train_loss = -2.414, time/batch = 0.010\n",
      "2544/3300 (epoch 77), train_loss = -2.626, time/batch = 0.005\n",
      "2545/3300 (epoch 77), train_loss = -2.219, time/batch = 0.005\n",
      "2546/3300 (epoch 77), train_loss = -0.659, time/batch = 0.009\n",
      "2547/3300 (epoch 77), train_loss = -1.876, time/batch = 0.012\n",
      "2548/3300 (epoch 77), train_loss = -1.568, time/batch = 0.008\n",
      "2549/3300 (epoch 77), train_loss = -3.567, time/batch = 0.011\n",
      "2550/3300 (epoch 77), train_loss = -3.777, time/batch = 0.011\n",
      "2551/3300 (epoch 77), train_loss = -3.774, time/batch = 0.009\n",
      "2552/3300 (epoch 77), train_loss = -4.014, time/batch = 0.009\n",
      "2553/3300 (epoch 77), train_loss = -4.070, time/batch = 0.004\n",
      "2554/3300 (epoch 77), train_loss = -4.038, time/batch = 0.006\n",
      "2555/3300 (epoch 77), train_loss = -4.368, time/batch = 0.009\n",
      "2556/3300 (epoch 77), train_loss = -4.112, time/batch = 0.008\n",
      "2557/3300 (epoch 77), train_loss = -3.818, time/batch = 0.004\n",
      "2558/3300 (epoch 77), train_loss = -4.262, time/batch = 0.004\n",
      "2559/3300 (epoch 77), train_loss = -3.822, time/batch = 0.006\n",
      "2560/3300 (epoch 77), train_loss = -3.998, time/batch = 0.004\n",
      "2561/3300 (epoch 77), train_loss = -3.986, time/batch = 0.007\n",
      "2562/3300 (epoch 77), train_loss = -4.228, time/batch = 0.006\n",
      "2563/3300 (epoch 77), train_loss = -4.149, time/batch = 0.006\n",
      "2564/3300 (epoch 77), train_loss = -4.713, time/batch = 0.004\n",
      "2565/3300 (epoch 77), train_loss = -5.050, time/batch = 0.004\n",
      "2566/3300 (epoch 77), train_loss = -5.010, time/batch = 0.005\n",
      "2567/3300 (epoch 77), train_loss = -4.522, time/batch = 0.005\n",
      "2568/3300 (epoch 77), train_loss = -4.114, time/batch = 0.006\n",
      "2569/3300 (epoch 77), train_loss = -4.170, time/batch = 0.006\n",
      "2570/3300 (epoch 77), train_loss = -4.947, time/batch = 0.006\n",
      "2571/3300 (epoch 77), train_loss = -5.263, time/batch = 0.005\n",
      "2572/3300 (epoch 77), train_loss = -5.255, time/batch = 0.005\n",
      "2573/3300 (epoch 77), train_loss = -5.374, time/batch = 0.005\n",
      "2574/3300 (epoch 78), train_loss = -1.247, time/batch = 0.005\n",
      "2575/3300 (epoch 78), train_loss = -2.069, time/batch = 0.006\n",
      "2576/3300 (epoch 78), train_loss = -2.417, time/batch = 0.005\n",
      "2577/3300 (epoch 78), train_loss = -2.414, time/batch = 0.005\n",
      "2578/3300 (epoch 78), train_loss = -1.671, time/batch = 0.005\n",
      "2579/3300 (epoch 78), train_loss = -1.433, time/batch = 0.005\n",
      "2580/3300 (epoch 78), train_loss = -1.203, time/batch = 0.005\n",
      "2581/3300 (epoch 78), train_loss = -1.318, time/batch = 0.006\n",
      "2582/3300 (epoch 78), train_loss = -4.210, time/batch = 0.009\n",
      "2583/3300 (epoch 78), train_loss = -4.099, time/batch = 0.008\n",
      "2584/3300 (epoch 78), train_loss = -4.059, time/batch = 0.007\n",
      "2585/3300 (epoch 78), train_loss = -4.008, time/batch = 0.005\n",
      "2586/3300 (epoch 78), train_loss = -4.034, time/batch = 0.007\n",
      "2587/3300 (epoch 78), train_loss = -4.430, time/batch = 0.005\n",
      "2588/3300 (epoch 78), train_loss = -4.175, time/batch = 0.007\n",
      "2589/3300 (epoch 78), train_loss = -3.596, time/batch = 0.005\n",
      "2590/3300 (epoch 78), train_loss = -4.249, time/batch = 0.005\n",
      "2591/3300 (epoch 78), train_loss = -3.801, time/batch = 0.005\n",
      "2592/3300 (epoch 78), train_loss = -3.727, time/batch = 0.005\n",
      "2593/3300 (epoch 78), train_loss = -3.851, time/batch = 0.006\n",
      "2594/3300 (epoch 78), train_loss = -4.302, time/batch = 0.005\n",
      "2595/3300 (epoch 78), train_loss = -3.728, time/batch = 0.005\n",
      "2596/3300 (epoch 78), train_loss = -5.075, time/batch = 0.005\n",
      "2597/3300 (epoch 78), train_loss = -5.037, time/batch = 0.005\n",
      "2598/3300 (epoch 78), train_loss = -5.210, time/batch = 0.006\n",
      "2599/3300 (epoch 78), train_loss = -5.104, time/batch = 0.005\n",
      "2600/3300 (epoch 78), train_loss = -4.830, time/batch = 0.008\n",
      "2601/3300 (epoch 78), train_loss = -4.254, time/batch = 0.007\n",
      "2602/3300 (epoch 78), train_loss = -4.104, time/batch = 0.006\n",
      "2603/3300 (epoch 78), train_loss = -4.824, time/batch = 0.007\n",
      "2604/3300 (epoch 78), train_loss = -5.130, time/batch = 0.005\n",
      "2605/3300 (epoch 78), train_loss = -3.763, time/batch = 0.005\n",
      "2606/3300 (epoch 78), train_loss = -4.522, time/batch = 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2607/3300 (epoch 79), train_loss = -1.232, time/batch = 0.006\n",
      "2608/3300 (epoch 79), train_loss = -2.370, time/batch = 0.007\n",
      "2609/3300 (epoch 79), train_loss = -2.247, time/batch = 0.008\n",
      "2610/3300 (epoch 79), train_loss = -2.037, time/batch = 0.008\n",
      "2611/3300 (epoch 79), train_loss = -1.355, time/batch = 0.005\n",
      "2612/3300 (epoch 79), train_loss = -1.929, time/batch = 0.007\n",
      "2613/3300 (epoch 79), train_loss = -1.209, time/batch = 0.006\n",
      "2614/3300 (epoch 79), train_loss = -2.883, time/batch = 0.005\n",
      "2615/3300 (epoch 79), train_loss = -4.085, time/batch = 0.006\n",
      "2616/3300 (epoch 79), train_loss = -3.888, time/batch = 0.005\n",
      "2617/3300 (epoch 79), train_loss = -4.553, time/batch = 0.006\n",
      "2618/3300 (epoch 79), train_loss = -3.845, time/batch = 0.008\n",
      "2619/3300 (epoch 79), train_loss = -3.774, time/batch = 0.008\n",
      "2620/3300 (epoch 79), train_loss = -4.285, time/batch = 0.006\n",
      "2621/3300 (epoch 79), train_loss = -4.252, time/batch = 0.008\n",
      "2622/3300 (epoch 79), train_loss = -3.843, time/batch = 0.009\n",
      "2623/3300 (epoch 79), train_loss = -3.928, time/batch = 0.010\n",
      "2624/3300 (epoch 79), train_loss = -4.367, time/batch = 0.010\n",
      "2625/3300 (epoch 79), train_loss = -3.463, time/batch = 0.014\n",
      "2626/3300 (epoch 79), train_loss = -3.873, time/batch = 0.010\n",
      "2627/3300 (epoch 79), train_loss = -4.068, time/batch = 0.015\n",
      "2628/3300 (epoch 79), train_loss = -3.632, time/batch = 0.015\n",
      "2629/3300 (epoch 79), train_loss = -3.560, time/batch = 0.011\n",
      "2630/3300 (epoch 79), train_loss = -4.199, time/batch = 0.007\n",
      "2631/3300 (epoch 79), train_loss = -5.111, time/batch = 0.014\n",
      "2632/3300 (epoch 79), train_loss = -5.122, time/batch = 0.011\n",
      "2633/3300 (epoch 79), train_loss = -5.159, time/batch = 0.008\n",
      "2634/3300 (epoch 79), train_loss = -5.213, time/batch = 0.013\n",
      "2635/3300 (epoch 79), train_loss = -4.269, time/batch = 0.013\n",
      "2636/3300 (epoch 79), train_loss = -4.174, time/batch = 0.008\n",
      "2637/3300 (epoch 79), train_loss = -4.087, time/batch = 0.010\n",
      "2638/3300 (epoch 79), train_loss = -5.356, time/batch = 0.005\n",
      "2639/3300 (epoch 79), train_loss = -5.369, time/batch = 0.014\n",
      "2640/3300 (epoch 80), train_loss = -1.158, time/batch = 0.015\n",
      "2641/3300 (epoch 80), train_loss = -1.974, time/batch = 0.015\n",
      "2642/3300 (epoch 80), train_loss = -2.133, time/batch = 0.005\n",
      "2643/3300 (epoch 80), train_loss = -2.511, time/batch = 0.013\n",
      "2644/3300 (epoch 80), train_loss = -1.407, time/batch = 0.009\n",
      "2645/3300 (epoch 80), train_loss = -1.255, time/batch = 0.007\n",
      "2646/3300 (epoch 80), train_loss = -1.779, time/batch = 0.009\n",
      "2647/3300 (epoch 80), train_loss = -1.691, time/batch = 0.009\n",
      "2648/3300 (epoch 80), train_loss = -4.130, time/batch = 0.009\n",
      "2649/3300 (epoch 80), train_loss = -4.035, time/batch = 0.007\n",
      "2650/3300 (epoch 80), train_loss = -4.025, time/batch = 0.009\n",
      "2651/3300 (epoch 80), train_loss = -3.928, time/batch = 0.008\n",
      "2652/3300 (epoch 80), train_loss = -3.852, time/batch = 0.007\n",
      "2653/3300 (epoch 80), train_loss = -4.455, time/batch = 0.012\n",
      "2654/3300 (epoch 80), train_loss = -4.231, time/batch = 0.008\n",
      "2655/3300 (epoch 80), train_loss = -3.773, time/batch = 0.014\n",
      "2656/3300 (epoch 80), train_loss = -3.948, time/batch = 0.011\n",
      "2657/3300 (epoch 80), train_loss = -3.838, time/batch = 0.011\n",
      "2658/3300 (epoch 80), train_loss = -3.924, time/batch = 0.008\n",
      "2659/3300 (epoch 80), train_loss = -3.525, time/batch = 0.009\n",
      "2660/3300 (epoch 80), train_loss = -4.110, time/batch = 0.010\n",
      "2661/3300 (epoch 80), train_loss = -4.330, time/batch = 0.013\n",
      "2662/3300 (epoch 80), train_loss = -4.580, time/batch = 0.010\n",
      "2663/3300 (epoch 80), train_loss = -4.612, time/batch = 0.011\n",
      "2664/3300 (epoch 80), train_loss = -4.666, time/batch = 0.010\n",
      "2665/3300 (epoch 80), train_loss = -4.379, time/batch = 0.011\n",
      "2666/3300 (epoch 80), train_loss = -3.543, time/batch = 0.015\n",
      "2667/3300 (epoch 80), train_loss = -4.447, time/batch = 0.009\n",
      "2668/3300 (epoch 80), train_loss = -3.975, time/batch = 0.011\n",
      "2669/3300 (epoch 80), train_loss = -4.605, time/batch = 0.012\n",
      "2670/3300 (epoch 80), train_loss = -4.405, time/batch = 0.005\n",
      "2671/3300 (epoch 80), train_loss = -4.253, time/batch = 0.006\n",
      "2672/3300 (epoch 80), train_loss = -4.004, time/batch = 0.012\n",
      "2673/3300 (epoch 81), train_loss = -1.235, time/batch = 0.004\n",
      "2674/3300 (epoch 81), train_loss = -1.899, time/batch = 0.005\n",
      "2675/3300 (epoch 81), train_loss = -2.377, time/batch = 0.012\n",
      "2676/3300 (epoch 81), train_loss = -2.083, time/batch = 0.004\n",
      "2677/3300 (epoch 81), train_loss = -2.082, time/batch = 0.008\n",
      "2678/3300 (epoch 81), train_loss = -0.588, time/batch = 0.004\n",
      "2679/3300 (epoch 81), train_loss = -1.358, time/batch = 0.004\n",
      "2680/3300 (epoch 81), train_loss = -1.953, time/batch = 0.004\n",
      "2681/3300 (epoch 81), train_loss = -4.262, time/batch = 0.004\n",
      "2682/3300 (epoch 81), train_loss = -3.872, time/batch = 0.006\n",
      "2683/3300 (epoch 81), train_loss = -3.619, time/batch = 0.004\n",
      "2684/3300 (epoch 81), train_loss = -3.947, time/batch = 0.006\n",
      "2685/3300 (epoch 81), train_loss = -4.091, time/batch = 0.004\n",
      "2686/3300 (epoch 81), train_loss = -4.488, time/batch = 0.005\n",
      "2687/3300 (epoch 81), train_loss = -3.694, time/batch = 0.004\n",
      "2688/3300 (epoch 81), train_loss = -3.981, time/batch = 0.006\n",
      "2689/3300 (epoch 81), train_loss = -4.172, time/batch = 0.007\n",
      "2690/3300 (epoch 81), train_loss = -3.386, time/batch = 0.011\n",
      "2691/3300 (epoch 81), train_loss = -4.000, time/batch = 0.004\n",
      "2692/3300 (epoch 81), train_loss = -4.157, time/batch = 0.004\n",
      "2693/3300 (epoch 81), train_loss = -4.285, time/batch = 0.005\n",
      "2694/3300 (epoch 81), train_loss = -4.392, time/batch = 0.005\n",
      "2695/3300 (epoch 81), train_loss = -4.083, time/batch = 0.005\n",
      "2696/3300 (epoch 81), train_loss = -5.149, time/batch = 0.004\n",
      "2697/3300 (epoch 81), train_loss = -5.081, time/batch = 0.007\n",
      "2698/3300 (epoch 81), train_loss = -5.200, time/batch = 0.009\n",
      "2699/3300 (epoch 81), train_loss = -5.046, time/batch = 0.006\n",
      "2700/3300 (epoch 81), train_loss = -5.125, time/batch = 0.005\n",
      "2701/3300 (epoch 81), train_loss = -4.763, time/batch = 0.006\n",
      "2702/3300 (epoch 81), train_loss = -3.986, time/batch = 0.005\n",
      "2703/3300 (epoch 81), train_loss = -3.605, time/batch = 0.006\n",
      "2704/3300 (epoch 81), train_loss = -4.447, time/batch = 0.005\n",
      "2705/3300 (epoch 81), train_loss = -4.223, time/batch = 0.005\n",
      "2706/3300 (epoch 82), train_loss = -1.186, time/batch = 0.005\n",
      "2707/3300 (epoch 82), train_loss = -2.130, time/batch = 0.005\n",
      "2708/3300 (epoch 82), train_loss = -2.218, time/batch = 0.005\n",
      "2709/3300 (epoch 82), train_loss = -3.308, time/batch = 0.006\n",
      "2710/3300 (epoch 82), train_loss = -1.344, time/batch = 0.005\n",
      "2711/3300 (epoch 82), train_loss = -1.595, time/batch = 0.007\n",
      "2712/3300 (epoch 82), train_loss = -0.334, time/batch = 0.005\n",
      "2713/3300 (epoch 82), train_loss = -1.022, time/batch = 0.005\n",
      "2714/3300 (epoch 82), train_loss = -1.248, time/batch = 0.008\n",
      "2715/3300 (epoch 82), train_loss = -3.674, time/batch = 0.005\n",
      "2716/3300 (epoch 82), train_loss = -3.838, time/batch = 0.009\n",
      "2717/3300 (epoch 82), train_loss = -3.919, time/batch = 0.010\n",
      "2718/3300 (epoch 82), train_loss = -4.012, time/batch = 0.009\n",
      "2719/3300 (epoch 82), train_loss = -4.079, time/batch = 0.009\n",
      "2720/3300 (epoch 82), train_loss = -4.418, time/batch = 0.006\n",
      "2721/3300 (epoch 82), train_loss = -3.690, time/batch = 0.009\n",
      "2722/3300 (epoch 82), train_loss = -3.988, time/batch = 0.009\n",
      "2723/3300 (epoch 82), train_loss = -3.808, time/batch = 0.008\n",
      "2724/3300 (epoch 82), train_loss = -3.781, time/batch = 0.007\n",
      "2725/3300 (epoch 82), train_loss = -3.981, time/batch = 0.008\n",
      "2726/3300 (epoch 82), train_loss = -4.456, time/batch = 0.008\n",
      "2727/3300 (epoch 82), train_loss = -4.551, time/batch = 0.007\n",
      "2728/3300 (epoch 82), train_loss = -4.527, time/batch = 0.010\n",
      "2729/3300 (epoch 82), train_loss = -4.078, time/batch = 0.007\n",
      "2730/3300 (epoch 82), train_loss = -4.787, time/batch = 0.008\n",
      "2731/3300 (epoch 82), train_loss = -4.183, time/batch = 0.009\n",
      "2732/3300 (epoch 82), train_loss = -4.174, time/batch = 0.009\n",
      "2733/3300 (epoch 82), train_loss = -4.509, time/batch = 0.008\n",
      "2734/3300 (epoch 82), train_loss = -4.447, time/batch = 0.010\n",
      "2735/3300 (epoch 82), train_loss = -3.831, time/batch = 0.010\n",
      "2736/3300 (epoch 82), train_loss = -4.359, time/batch = 0.010\n",
      "2737/3300 (epoch 82), train_loss = -4.464, time/batch = 0.008\n",
      "2738/3300 (epoch 82), train_loss = -3.987, time/batch = 0.008\n",
      "2739/3300 (epoch 83), train_loss = -1.204, time/batch = 0.009\n",
      "2740/3300 (epoch 83), train_loss = -2.086, time/batch = 0.009\n",
      "2741/3300 (epoch 83), train_loss = -2.286, time/batch = 0.009\n",
      "2742/3300 (epoch 83), train_loss = -2.286, time/batch = 0.009\n",
      "2743/3300 (epoch 83), train_loss = -1.694, time/batch = 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744/3300 (epoch 83), train_loss = -0.642, time/batch = 0.009\n",
      "2745/3300 (epoch 83), train_loss = -0.895, time/batch = 0.011\n",
      "2746/3300 (epoch 83), train_loss = -2.032, time/batch = 0.009\n",
      "2747/3300 (epoch 83), train_loss = -4.473, time/batch = 0.010\n",
      "2748/3300 (epoch 83), train_loss = -4.095, time/batch = 0.010\n",
      "2749/3300 (epoch 83), train_loss = -4.254, time/batch = 0.010\n",
      "2750/3300 (epoch 83), train_loss = -4.020, time/batch = 0.012\n",
      "2751/3300 (epoch 83), train_loss = -3.784, time/batch = 0.009\n",
      "2752/3300 (epoch 83), train_loss = -3.874, time/batch = 0.010\n",
      "2753/3300 (epoch 83), train_loss = -4.565, time/batch = 0.008\n",
      "2754/3300 (epoch 83), train_loss = -3.837, time/batch = 0.008\n",
      "2755/3300 (epoch 83), train_loss = -4.236, time/batch = 0.012\n",
      "2756/3300 (epoch 83), train_loss = -3.939, time/batch = 0.008\n",
      "2757/3300 (epoch 83), train_loss = -3.818, time/batch = 0.012\n",
      "2758/3300 (epoch 83), train_loss = -4.069, time/batch = 0.009\n",
      "2759/3300 (epoch 83), train_loss = -3.995, time/batch = 0.007\n",
      "2760/3300 (epoch 83), train_loss = -4.423, time/batch = 0.010\n",
      "2761/3300 (epoch 83), train_loss = -4.283, time/batch = 0.013\n",
      "2762/3300 (epoch 83), train_loss = -4.324, time/batch = 0.009\n",
      "2763/3300 (epoch 83), train_loss = -5.070, time/batch = 0.009\n",
      "2764/3300 (epoch 83), train_loss = -4.570, time/batch = 0.010\n",
      "2765/3300 (epoch 83), train_loss = -4.219, time/batch = 0.011\n",
      "2766/3300 (epoch 83), train_loss = -4.350, time/batch = 0.010\n",
      "2767/3300 (epoch 83), train_loss = -5.193, time/batch = 0.008\n",
      "2768/3300 (epoch 83), train_loss = -5.105, time/batch = 0.009\n",
      "2769/3300 (epoch 83), train_loss = -3.723, time/batch = 0.009\n",
      "2770/3300 (epoch 83), train_loss = -4.389, time/batch = 0.009\n",
      "2771/3300 (epoch 83), train_loss = -4.281, time/batch = 0.008\n",
      "2772/3300 (epoch 84), train_loss = -1.197, time/batch = 0.010\n",
      "2773/3300 (epoch 84), train_loss = -2.203, time/batch = 0.013\n",
      "2774/3300 (epoch 84), train_loss = -2.487, time/batch = 0.014\n",
      "2775/3300 (epoch 84), train_loss = -2.418, time/batch = 0.010\n",
      "2776/3300 (epoch 84), train_loss = -1.430, time/batch = 0.011\n",
      "2777/3300 (epoch 84), train_loss = -1.152, time/batch = 0.007\n",
      "2778/3300 (epoch 84), train_loss = -1.133, time/batch = 0.006\n",
      "2779/3300 (epoch 84), train_loss = -1.669, time/batch = 0.012\n",
      "2780/3300 (epoch 84), train_loss = -3.502, time/batch = 0.010\n",
      "2781/3300 (epoch 84), train_loss = -4.148, time/batch = 0.014\n",
      "2782/3300 (epoch 84), train_loss = -4.047, time/batch = 0.010\n",
      "2783/3300 (epoch 84), train_loss = -4.328, time/batch = 0.006\n",
      "2784/3300 (epoch 84), train_loss = -3.718, time/batch = 0.005\n",
      "2785/3300 (epoch 84), train_loss = -4.338, time/batch = 0.008\n",
      "2786/3300 (epoch 84), train_loss = -4.376, time/batch = 0.005\n",
      "2787/3300 (epoch 84), train_loss = -3.883, time/batch = 0.006\n",
      "2788/3300 (epoch 84), train_loss = -4.259, time/batch = 0.012\n",
      "2789/3300 (epoch 84), train_loss = -3.888, time/batch = 0.007\n",
      "2790/3300 (epoch 84), train_loss = -3.369, time/batch = 0.009\n",
      "2791/3300 (epoch 84), train_loss = -4.017, time/batch = 0.009\n",
      "2792/3300 (epoch 84), train_loss = -3.967, time/batch = 0.009\n",
      "2793/3300 (epoch 84), train_loss = -3.957, time/batch = 0.008\n",
      "2794/3300 (epoch 84), train_loss = -4.687, time/batch = 0.011\n",
      "2795/3300 (epoch 84), train_loss = -4.541, time/batch = 0.010\n",
      "2796/3300 (epoch 84), train_loss = -4.239, time/batch = 0.006\n",
      "2797/3300 (epoch 84), train_loss = -4.835, time/batch = 0.008\n",
      "2798/3300 (epoch 84), train_loss = -5.123, time/batch = 0.012\n",
      "2799/3300 (epoch 84), train_loss = -4.744, time/batch = 0.011\n",
      "2800/3300 (epoch 84), train_loss = -3.612, time/batch = 0.009\n",
      "2801/3300 (epoch 84), train_loss = -4.878, time/batch = 0.005\n",
      "2802/3300 (epoch 84), train_loss = -4.868, time/batch = 0.008\n",
      "2803/3300 (epoch 84), train_loss = -3.942, time/batch = 0.005\n",
      "2804/3300 (epoch 84), train_loss = -4.392, time/batch = 0.005\n",
      "2805/3300 (epoch 85), train_loss = -1.216, time/batch = 0.009\n",
      "2806/3300 (epoch 85), train_loss = -2.162, time/batch = 0.010\n",
      "2807/3300 (epoch 85), train_loss = -2.252, time/batch = 0.008\n",
      "2808/3300 (epoch 85), train_loss = -1.648, time/batch = 0.007\n",
      "2809/3300 (epoch 85), train_loss = -2.111, time/batch = 0.015\n",
      "2810/3300 (epoch 85), train_loss = -1.124, time/batch = 0.009\n",
      "2811/3300 (epoch 85), train_loss = -1.113, time/batch = 0.010\n",
      "2812/3300 (epoch 85), train_loss = -1.128, time/batch = 0.006\n",
      "2813/3300 (epoch 85), train_loss = -3.397, time/batch = 0.008\n",
      "2814/3300 (epoch 85), train_loss = -4.173, time/batch = 0.006\n",
      "2815/3300 (epoch 85), train_loss = -3.525, time/batch = 0.012\n",
      "2816/3300 (epoch 85), train_loss = -4.056, time/batch = 0.009\n",
      "2817/3300 (epoch 85), train_loss = -4.238, time/batch = 0.007\n",
      "2818/3300 (epoch 85), train_loss = -4.114, time/batch = 0.006\n",
      "2819/3300 (epoch 85), train_loss = -4.198, time/batch = 0.006\n",
      "2820/3300 (epoch 85), train_loss = -3.998, time/batch = 0.006\n",
      "2821/3300 (epoch 85), train_loss = -3.771, time/batch = 0.006\n",
      "2822/3300 (epoch 85), train_loss = -4.054, time/batch = 0.005\n",
      "2823/3300 (epoch 85), train_loss = -4.162, time/batch = 0.005\n",
      "2824/3300 (epoch 85), train_loss = -4.318, time/batch = 0.006\n",
      "2825/3300 (epoch 85), train_loss = -3.882, time/batch = 0.005\n",
      "2826/3300 (epoch 85), train_loss = -4.701, time/batch = 0.005\n",
      "2827/3300 (epoch 85), train_loss = -5.109, time/batch = 0.005\n",
      "2828/3300 (epoch 85), train_loss = -5.182, time/batch = 0.013\n",
      "2829/3300 (epoch 85), train_loss = -5.228, time/batch = 0.005\n",
      "2830/3300 (epoch 85), train_loss = -5.077, time/batch = 0.004\n",
      "2831/3300 (epoch 85), train_loss = -4.450, time/batch = 0.004\n",
      "2832/3300 (epoch 85), train_loss = -3.994, time/batch = 0.006\n",
      "2833/3300 (epoch 85), train_loss = -4.670, time/batch = 0.005\n",
      "2834/3300 (epoch 85), train_loss = -5.318, time/batch = 0.006\n",
      "2835/3300 (epoch 85), train_loss = -5.326, time/batch = 0.011\n",
      "2836/3300 (epoch 85), train_loss = -4.109, time/batch = 0.010\n",
      "2837/3300 (epoch 85), train_loss = -4.177, time/batch = 0.006\n",
      "2838/3300 (epoch 86), train_loss = -1.251, time/batch = 0.005\n",
      "2839/3300 (epoch 86), train_loss = -2.176, time/batch = 0.007\n",
      "2840/3300 (epoch 86), train_loss = -2.177, time/batch = 0.007\n",
      "2841/3300 (epoch 86), train_loss = -1.927, time/batch = 0.005\n",
      "2842/3300 (epoch 86), train_loss = -1.442, time/batch = 0.006\n",
      "2843/3300 (epoch 86), train_loss = -1.464, time/batch = 0.005\n",
      "2844/3300 (epoch 86), train_loss = -1.124, time/batch = 0.009\n",
      "2845/3300 (epoch 86), train_loss = -1.595, time/batch = 0.021\n",
      "2846/3300 (epoch 86), train_loss = -4.598, time/batch = 0.006\n",
      "2847/3300 (epoch 86), train_loss = -3.809, time/batch = 0.004\n",
      "2848/3300 (epoch 86), train_loss = -4.049, time/batch = 0.006\n",
      "2849/3300 (epoch 86), train_loss = -3.998, time/batch = 0.005\n",
      "2850/3300 (epoch 86), train_loss = -4.099, time/batch = 0.006\n",
      "2851/3300 (epoch 86), train_loss = -4.236, time/batch = 0.004\n",
      "2852/3300 (epoch 86), train_loss = -4.368, time/batch = 0.004\n",
      "2853/3300 (epoch 86), train_loss = -3.994, time/batch = 0.005\n",
      "2854/3300 (epoch 86), train_loss = -4.423, time/batch = 0.006\n",
      "2855/3300 (epoch 86), train_loss = -3.996, time/batch = 0.006\n",
      "2856/3300 (epoch 86), train_loss = -3.567, time/batch = 0.005\n",
      "2857/3300 (epoch 86), train_loss = -4.009, time/batch = 0.005\n",
      "2858/3300 (epoch 86), train_loss = -4.104, time/batch = 0.005\n",
      "2859/3300 (epoch 86), train_loss = -4.553, time/batch = 0.005\n",
      "2860/3300 (epoch 86), train_loss = -3.967, time/batch = 0.006\n",
      "2861/3300 (epoch 86), train_loss = -5.102, time/batch = 0.005\n",
      "2862/3300 (epoch 86), train_loss = -4.984, time/batch = 0.005\n",
      "2863/3300 (epoch 86), train_loss = -4.232, time/batch = 0.005\n",
      "2864/3300 (epoch 86), train_loss = -4.226, time/batch = 0.006\n",
      "2865/3300 (epoch 86), train_loss = -4.505, time/batch = 0.005\n",
      "2866/3300 (epoch 86), train_loss = -5.379, time/batch = 0.007\n",
      "2867/3300 (epoch 86), train_loss = -5.419, time/batch = 0.006\n",
      "2868/3300 (epoch 86), train_loss = -5.463, time/batch = 0.008\n",
      "2869/3300 (epoch 86), train_loss = -4.509, time/batch = 0.008\n",
      "2870/3300 (epoch 86), train_loss = -3.878, time/batch = 0.006\n",
      "2871/3300 (epoch 87), train_loss = -1.211, time/batch = 0.005\n",
      "2872/3300 (epoch 87), train_loss = -2.235, time/batch = 0.004\n",
      "2873/3300 (epoch 87), train_loss = -2.236, time/batch = 0.004\n",
      "2874/3300 (epoch 87), train_loss = -2.205, time/batch = 0.004\n",
      "2875/3300 (epoch 87), train_loss = -0.746, time/batch = 0.006\n",
      "2876/3300 (epoch 87), train_loss = -1.148, time/batch = 0.004\n",
      "2877/3300 (epoch 87), train_loss = -0.847, time/batch = 0.006\n",
      "2878/3300 (epoch 87), train_loss = -1.951, time/batch = 0.005\n",
      "2879/3300 (epoch 87), train_loss = -4.471, time/batch = 0.005\n",
      "2880/3300 (epoch 87), train_loss = -4.279, time/batch = 0.005\n",
      "2881/3300 (epoch 87), train_loss = -4.160, time/batch = 0.006\n",
      "2882/3300 (epoch 87), train_loss = -3.941, time/batch = 0.005\n",
      "2883/3300 (epoch 87), train_loss = -4.135, time/batch = 0.004\n",
      "2884/3300 (epoch 87), train_loss = -4.340, time/batch = 0.004\n",
      "2885/3300 (epoch 87), train_loss = -4.059, time/batch = 0.005\n",
      "2886/3300 (epoch 87), train_loss = -4.475, time/batch = 0.005\n",
      "2887/3300 (epoch 87), train_loss = -3.776, time/batch = 0.006\n",
      "2888/3300 (epoch 87), train_loss = -3.320, time/batch = 0.005\n",
      "2889/3300 (epoch 87), train_loss = -4.406, time/batch = 0.004\n",
      "2890/3300 (epoch 87), train_loss = -4.435, time/batch = 0.004\n",
      "2891/3300 (epoch 87), train_loss = -4.123, time/batch = 0.005\n",
      "2892/3300 (epoch 87), train_loss = -3.672, time/batch = 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893/3300 (epoch 87), train_loss = -4.432, time/batch = 0.005\n",
      "2894/3300 (epoch 87), train_loss = -5.139, time/batch = 0.004\n",
      "2895/3300 (epoch 87), train_loss = -5.195, time/batch = 0.005\n",
      "2896/3300 (epoch 87), train_loss = -4.425, time/batch = 0.004\n",
      "2897/3300 (epoch 87), train_loss = -3.724, time/batch = 0.005\n",
      "2898/3300 (epoch 87), train_loss = -4.979, time/batch = 0.007\n",
      "2899/3300 (epoch 87), train_loss = -4.207, time/batch = 0.012\n",
      "2900/3300 (epoch 87), train_loss = -4.346, time/batch = 0.007\n",
      "2901/3300 (epoch 87), train_loss = -4.300, time/batch = 0.005\n",
      "2902/3300 (epoch 87), train_loss = -4.084, time/batch = 0.005\n",
      "2903/3300 (epoch 87), train_loss = -4.537, time/batch = 0.005\n",
      "2904/3300 (epoch 88), train_loss = -1.198, time/batch = 0.005\n",
      "2905/3300 (epoch 88), train_loss = -2.168, time/batch = 0.005\n",
      "2906/3300 (epoch 88), train_loss = -2.328, time/batch = 0.004\n",
      "2907/3300 (epoch 88), train_loss = -2.061, time/batch = 0.004\n",
      "2908/3300 (epoch 88), train_loss = -1.122, time/batch = 0.004\n",
      "2909/3300 (epoch 88), train_loss = -0.790, time/batch = 0.004\n",
      "2910/3300 (epoch 88), train_loss = -1.341, time/batch = 0.004\n",
      "2911/3300 (epoch 88), train_loss = -1.178, time/batch = 0.004\n",
      "2912/3300 (epoch 88), train_loss = -3.409, time/batch = 0.004\n",
      "2913/3300 (epoch 88), train_loss = -3.911, time/batch = 0.005\n",
      "2914/3300 (epoch 88), train_loss = -3.810, time/batch = 0.004\n",
      "2915/3300 (epoch 88), train_loss = -3.527, time/batch = 0.004\n",
      "2916/3300 (epoch 88), train_loss = -4.078, time/batch = 0.004\n",
      "2917/3300 (epoch 88), train_loss = -4.476, time/batch = 0.005\n",
      "2918/3300 (epoch 88), train_loss = -4.533, time/batch = 0.004\n",
      "2919/3300 (epoch 88), train_loss = -4.460, time/batch = 0.004\n",
      "2920/3300 (epoch 88), train_loss = -4.312, time/batch = 0.004\n",
      "2921/3300 (epoch 88), train_loss = -3.917, time/batch = 0.004\n",
      "2922/3300 (epoch 88), train_loss = -3.839, time/batch = 0.004\n",
      "2923/3300 (epoch 88), train_loss = -3.833, time/batch = 0.004\n",
      "2924/3300 (epoch 88), train_loss = -4.046, time/batch = 0.004\n",
      "2925/3300 (epoch 88), train_loss = -4.090, time/batch = 0.004\n",
      "2926/3300 (epoch 88), train_loss = -4.428, time/batch = 0.005\n",
      "2927/3300 (epoch 88), train_loss = -4.659, time/batch = 0.006\n",
      "2928/3300 (epoch 88), train_loss = -4.238, time/batch = 0.004\n",
      "2929/3300 (epoch 88), train_loss = -5.127, time/batch = 0.008\n",
      "2930/3300 (epoch 88), train_loss = -5.015, time/batch = 0.005\n",
      "2931/3300 (epoch 88), train_loss = -4.174, time/batch = 0.004\n",
      "2932/3300 (epoch 88), train_loss = -4.216, time/batch = 0.004\n",
      "2933/3300 (epoch 88), train_loss = -4.702, time/batch = 0.004\n",
      "2934/3300 (epoch 88), train_loss = -5.438, time/batch = 0.006\n",
      "2935/3300 (epoch 88), train_loss = -5.478, time/batch = 0.004\n",
      "2936/3300 (epoch 88), train_loss = -3.830, time/batch = 0.005\n",
      "2937/3300 (epoch 89), train_loss = -1.212, time/batch = 0.005\n",
      "2938/3300 (epoch 89), train_loss = -2.139, time/batch = 0.007\n",
      "2939/3300 (epoch 89), train_loss = -2.526, time/batch = 0.006\n",
      "2940/3300 (epoch 89), train_loss = -2.226, time/batch = 0.007\n",
      "2941/3300 (epoch 89), train_loss = -1.465, time/batch = 0.006\n",
      "2942/3300 (epoch 89), train_loss = -0.627, time/batch = 0.005\n",
      "2943/3300 (epoch 89), train_loss = -0.492, time/batch = 0.005\n",
      "2944/3300 (epoch 89), train_loss = -2.122, time/batch = 0.006\n",
      "2945/3300 (epoch 89), train_loss = -4.725, time/batch = 0.006\n",
      "2946/3300 (epoch 89), train_loss = -4.433, time/batch = 0.005\n",
      "2947/3300 (epoch 89), train_loss = -4.264, time/batch = 0.007\n",
      "2948/3300 (epoch 89), train_loss = -4.046, time/batch = 0.007\n",
      "2949/3300 (epoch 89), train_loss = -3.822, time/batch = 0.011\n",
      "2950/3300 (epoch 89), train_loss = -4.093, time/batch = 0.007\n",
      "2951/3300 (epoch 89), train_loss = -4.350, time/batch = 0.004\n",
      "2952/3300 (epoch 89), train_loss = -3.941, time/batch = 0.004\n",
      "2953/3300 (epoch 89), train_loss = -4.318, time/batch = 0.004\n",
      "2954/3300 (epoch 89), train_loss = -4.101, time/batch = 0.004\n",
      "2955/3300 (epoch 89), train_loss = -3.359, time/batch = 0.005\n",
      "2956/3300 (epoch 89), train_loss = -3.787, time/batch = 0.004\n",
      "2957/3300 (epoch 89), train_loss = -4.145, time/batch = 0.011\n",
      "2958/3300 (epoch 89), train_loss = -4.287, time/batch = 0.007\n",
      "2959/3300 (epoch 89), train_loss = -4.413, time/batch = 0.005\n",
      "2960/3300 (epoch 89), train_loss = -3.600, time/batch = 0.004\n",
      "2961/3300 (epoch 89), train_loss = -5.108, time/batch = 0.004\n",
      "2962/3300 (epoch 89), train_loss = -5.167, time/batch = 0.006\n",
      "2963/3300 (epoch 89), train_loss = -4.871, time/batch = 0.004\n",
      "2964/3300 (epoch 89), train_loss = -4.146, time/batch = 0.005\n",
      "2965/3300 (epoch 89), train_loss = -4.264, time/batch = 0.005\n",
      "2966/3300 (epoch 89), train_loss = -5.411, time/batch = 0.004\n",
      "2967/3300 (epoch 89), train_loss = -5.378, time/batch = 0.005\n",
      "2968/3300 (epoch 89), train_loss = -4.664, time/batch = 0.004\n",
      "2969/3300 (epoch 89), train_loss = -4.072, time/batch = 0.005\n",
      "2970/3300 (epoch 90), train_loss = -1.192, time/batch = 0.004\n",
      "2971/3300 (epoch 90), train_loss = -2.248, time/batch = 0.006\n",
      "2972/3300 (epoch 90), train_loss = -2.515, time/batch = 0.006\n",
      "2973/3300 (epoch 90), train_loss = -2.271, time/batch = 0.006\n",
      "2974/3300 (epoch 90), train_loss = -1.598, time/batch = 0.004\n",
      "2975/3300 (epoch 90), train_loss = -0.141, time/batch = 0.005\n",
      "2976/3300 (epoch 90), train_loss = -0.786, time/batch = 0.005\n",
      "2977/3300 (epoch 90), train_loss = -0.789, time/batch = 0.006\n",
      "2978/3300 (epoch 90), train_loss = -3.514, time/batch = 0.006\n",
      "2979/3300 (epoch 90), train_loss = -3.988, time/batch = 0.005\n",
      "2980/3300 (epoch 90), train_loss = -4.285, time/batch = 0.005\n",
      "2981/3300 (epoch 90), train_loss = -4.017, time/batch = 0.004\n",
      "2982/3300 (epoch 90), train_loss = -3.946, time/batch = 0.006\n",
      "2983/3300 (epoch 90), train_loss = -4.232, time/batch = 0.011\n",
      "2984/3300 (epoch 90), train_loss = -4.582, time/batch = 0.007\n",
      "2985/3300 (epoch 90), train_loss = -4.011, time/batch = 0.005\n",
      "2986/3300 (epoch 90), train_loss = -4.272, time/batch = 0.005\n",
      "2987/3300 (epoch 90), train_loss = -4.143, time/batch = 0.009\n",
      "2988/3300 (epoch 90), train_loss = -3.277, time/batch = 0.007\n",
      "2989/3300 (epoch 90), train_loss = -4.058, time/batch = 0.004\n",
      "2990/3300 (epoch 90), train_loss = -4.052, time/batch = 0.004\n",
      "2991/3300 (epoch 90), train_loss = -4.322, time/batch = 0.004\n",
      "2992/3300 (epoch 90), train_loss = -4.554, time/batch = 0.004\n",
      "2993/3300 (epoch 90), train_loss = -4.345, time/batch = 0.004\n",
      "2994/3300 (epoch 90), train_loss = -4.565, time/batch = 0.006\n",
      "2995/3300 (epoch 90), train_loss = -5.195, time/batch = 0.004\n",
      "2996/3300 (epoch 90), train_loss = -4.447, time/batch = 0.004\n",
      "2997/3300 (epoch 90), train_loss = -4.505, time/batch = 0.004\n",
      "2998/3300 (epoch 90), train_loss = -4.121, time/batch = 0.004\n",
      "2999/3300 (epoch 90), train_loss = -5.111, time/batch = 0.004\n",
      "3000/3300 (epoch 90), train_loss = -5.383, time/batch = 0.004\n",
      "3001/3300 (epoch 90), train_loss = -5.455, time/batch = 0.004\n",
      "3002/3300 (epoch 90), train_loss = -5.551, time/batch = 0.004\n",
      "3003/3300 (epoch 91), train_loss = -1.216, time/batch = 0.004\n",
      "3004/3300 (epoch 91), train_loss = -2.131, time/batch = 0.004\n",
      "3005/3300 (epoch 91), train_loss = -2.411, time/batch = 0.004\n",
      "3006/3300 (epoch 91), train_loss = -2.981, time/batch = 0.004\n",
      "3007/3300 (epoch 91), train_loss = -2.038, time/batch = 0.004\n",
      "3008/3300 (epoch 91), train_loss = 0.204, time/batch = 0.005\n",
      "3009/3300 (epoch 91), train_loss = -0.672, time/batch = 0.005\n",
      "3010/3300 (epoch 91), train_loss = -1.131, time/batch = 0.006\n",
      "3011/3300 (epoch 91), train_loss = -2.600, time/batch = 0.005\n",
      "3012/3300 (epoch 91), train_loss = -4.467, time/batch = 0.006\n",
      "3013/3300 (epoch 91), train_loss = -4.181, time/batch = 0.004\n",
      "3014/3300 (epoch 91), train_loss = -4.230, time/batch = 0.006\n",
      "3015/3300 (epoch 91), train_loss = -4.218, time/batch = 0.006\n",
      "3016/3300 (epoch 91), train_loss = -3.757, time/batch = 0.005\n",
      "3017/3300 (epoch 91), train_loss = -4.099, time/batch = 0.004\n",
      "3018/3300 (epoch 91), train_loss = -4.459, time/batch = 0.006\n",
      "3019/3300 (epoch 91), train_loss = -3.860, time/batch = 0.006\n",
      "3020/3300 (epoch 91), train_loss = -3.832, time/batch = 0.005\n",
      "3021/3300 (epoch 91), train_loss = -2.949, time/batch = 0.008\n",
      "3022/3300 (epoch 91), train_loss = -3.937, time/batch = 0.008\n",
      "3023/3300 (epoch 91), train_loss = -3.824, time/batch = 0.007\n",
      "3024/3300 (epoch 91), train_loss = -4.267, time/batch = 0.006\n",
      "3025/3300 (epoch 91), train_loss = -4.634, time/batch = 0.004\n",
      "3026/3300 (epoch 91), train_loss = -5.289, time/batch = 0.006\n",
      "3027/3300 (epoch 91), train_loss = -5.266, time/batch = 0.006\n",
      "3028/3300 (epoch 91), train_loss = -5.345, time/batch = 0.005\n",
      "3029/3300 (epoch 91), train_loss = -5.305, time/batch = 0.005\n",
      "3030/3300 (epoch 91), train_loss = -5.393, time/batch = 0.006\n",
      "3031/3300 (epoch 91), train_loss = -5.289, time/batch = 0.005\n",
      "3032/3300 (epoch 91), train_loss = -4.979, time/batch = 0.005\n",
      "3033/3300 (epoch 91), train_loss = -4.795, time/batch = 0.004\n",
      "3034/3300 (epoch 91), train_loss = -4.063, time/batch = 0.005\n",
      "3035/3300 (epoch 91), train_loss = -4.012, time/batch = 0.004\n",
      "3036/3300 (epoch 92), train_loss = -1.277, time/batch = 0.006\n",
      "3037/3300 (epoch 92), train_loss = -2.326, time/batch = 0.014\n",
      "3038/3300 (epoch 92), train_loss = -2.417, time/batch = 0.011\n",
      "3039/3300 (epoch 92), train_loss = -2.124, time/batch = 0.010\n",
      "3040/3300 (epoch 92), train_loss = -1.247, time/batch = 0.005\n",
      "3041/3300 (epoch 92), train_loss = -0.332, time/batch = 0.007\n",
      "3042/3300 (epoch 92), train_loss = -0.320, time/batch = 0.012\n",
      "3043/3300 (epoch 92), train_loss = -0.957, time/batch = 0.011\n",
      "3044/3300 (epoch 92), train_loss = -4.548, time/batch = 0.007\n",
      "3045/3300 (epoch 92), train_loss = -3.739, time/batch = 0.007\n",
      "3046/3300 (epoch 92), train_loss = -3.827, time/batch = 0.005\n",
      "3047/3300 (epoch 92), train_loss = -4.362, time/batch = 0.005\n",
      "3048/3300 (epoch 92), train_loss = -3.988, time/batch = 0.007\n",
      "3049/3300 (epoch 92), train_loss = -4.002, time/batch = 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3050/3300 (epoch 92), train_loss = -4.314, time/batch = 0.005\n",
      "3051/3300 (epoch 92), train_loss = -4.478, time/batch = 0.005\n",
      "3052/3300 (epoch 92), train_loss = -3.785, time/batch = 0.005\n",
      "3053/3300 (epoch 92), train_loss = -4.228, time/batch = 0.006\n",
      "3054/3300 (epoch 92), train_loss = -3.643, time/batch = 0.004\n",
      "3055/3300 (epoch 92), train_loss = -3.683, time/batch = 0.004\n",
      "3056/3300 (epoch 92), train_loss = -4.132, time/batch = 0.006\n",
      "3057/3300 (epoch 92), train_loss = -4.370, time/batch = 0.005\n",
      "3058/3300 (epoch 92), train_loss = -4.601, time/batch = 0.006\n",
      "3059/3300 (epoch 92), train_loss = -4.057, time/batch = 0.004\n",
      "3060/3300 (epoch 92), train_loss = -4.909, time/batch = 0.004\n",
      "3061/3300 (epoch 92), train_loss = -5.145, time/batch = 0.005\n",
      "3062/3300 (epoch 92), train_loss = -4.159, time/batch = 0.004\n",
      "3063/3300 (epoch 92), train_loss = -4.191, time/batch = 0.005\n",
      "3064/3300 (epoch 92), train_loss = -5.266, time/batch = 0.004\n",
      "3065/3300 (epoch 92), train_loss = -5.426, time/batch = 0.007\n",
      "3066/3300 (epoch 92), train_loss = -4.918, time/batch = 0.010\n",
      "3067/3300 (epoch 92), train_loss = -4.477, time/batch = 0.008\n",
      "3068/3300 (epoch 92), train_loss = -4.477, time/batch = 0.006\n",
      "3069/3300 (epoch 93), train_loss = -1.242, time/batch = 0.007\n",
      "3070/3300 (epoch 93), train_loss = -2.223, time/batch = 0.005\n",
      "3071/3300 (epoch 93), train_loss = -2.057, time/batch = 0.005\n",
      "3072/3300 (epoch 93), train_loss = -2.339, time/batch = 0.004\n",
      "3073/3300 (epoch 93), train_loss = -1.317, time/batch = 0.006\n",
      "3074/3300 (epoch 93), train_loss = -0.334, time/batch = 0.011\n",
      "3075/3300 (epoch 93), train_loss = -0.423, time/batch = 0.013\n",
      "3076/3300 (epoch 93), train_loss = -0.099, time/batch = 0.005\n",
      "3077/3300 (epoch 93), train_loss = -3.443, time/batch = 0.015\n",
      "3078/3300 (epoch 93), train_loss = -3.862, time/batch = 0.004\n",
      "3079/3300 (epoch 93), train_loss = -3.722, time/batch = 0.009\n",
      "3080/3300 (epoch 93), train_loss = -4.055, time/batch = 0.008\n",
      "3081/3300 (epoch 93), train_loss = -4.355, time/batch = 0.005\n",
      "3082/3300 (epoch 93), train_loss = -4.582, time/batch = 0.005\n",
      "3083/3300 (epoch 93), train_loss = -3.915, time/batch = 0.004\n",
      "3084/3300 (epoch 93), train_loss = -4.364, time/batch = 0.004\n",
      "3085/3300 (epoch 93), train_loss = -4.119, time/batch = 0.004\n",
      "3086/3300 (epoch 93), train_loss = -4.147, time/batch = 0.004\n",
      "3087/3300 (epoch 93), train_loss = -4.072, time/batch = 0.006\n",
      "3088/3300 (epoch 93), train_loss = -4.387, time/batch = 0.004\n",
      "3089/3300 (epoch 93), train_loss = -4.694, time/batch = 0.004\n",
      "3090/3300 (epoch 93), train_loss = -4.455, time/batch = 0.009\n",
      "3091/3300 (epoch 93), train_loss = -4.196, time/batch = 0.004\n",
      "3092/3300 (epoch 93), train_loss = -5.231, time/batch = 0.004\n",
      "3093/3300 (epoch 93), train_loss = -4.982, time/batch = 0.012\n",
      "3094/3300 (epoch 93), train_loss = -4.500, time/batch = 0.013\n",
      "3095/3300 (epoch 93), train_loss = -4.237, time/batch = 0.005\n",
      "3096/3300 (epoch 93), train_loss = -4.398, time/batch = 0.011\n",
      "3097/3300 (epoch 93), train_loss = -5.009, time/batch = 0.009\n",
      "3098/3300 (epoch 93), train_loss = -5.449, time/batch = 0.005\n",
      "3099/3300 (epoch 93), train_loss = -3.860, time/batch = 0.021\n",
      "3100/3300 (epoch 93), train_loss = -4.050, time/batch = 0.009\n",
      "3101/3300 (epoch 93), train_loss = -4.299, time/batch = 0.006\n",
      "3102/3300 (epoch 94), train_loss = -1.313, time/batch = 0.016\n",
      "3103/3300 (epoch 94), train_loss = -2.400, time/batch = 0.007\n",
      "3104/3300 (epoch 94), train_loss = -2.077, time/batch = 0.018\n",
      "3105/3300 (epoch 94), train_loss = -2.905, time/batch = 0.012\n",
      "3106/3300 (epoch 94), train_loss = -1.095, time/batch = 0.007\n",
      "3107/3300 (epoch 94), train_loss = 0.674, time/batch = 0.006\n",
      "3108/3300 (epoch 94), train_loss = 0.403, time/batch = 0.019\n",
      "3109/3300 (epoch 94), train_loss = 0.371, time/batch = 0.015\n",
      "3110/3300 (epoch 94), train_loss = -1.793, time/batch = 0.007\n",
      "3111/3300 (epoch 94), train_loss = -4.409, time/batch = 0.007\n",
      "3112/3300 (epoch 94), train_loss = -3.544, time/batch = 0.006\n",
      "3113/3300 (epoch 94), train_loss = -4.331, time/batch = 0.006\n",
      "3114/3300 (epoch 94), train_loss = -3.970, time/batch = 0.006\n",
      "3115/3300 (epoch 94), train_loss = -4.133, time/batch = 0.005\n",
      "3116/3300 (epoch 94), train_loss = -4.165, time/batch = 0.005\n",
      "3117/3300 (epoch 94), train_loss = -4.402, time/batch = 0.007\n",
      "3118/3300 (epoch 94), train_loss = -3.954, time/batch = 0.005\n",
      "3119/3300 (epoch 94), train_loss = -4.195, time/batch = 0.005\n",
      "3120/3300 (epoch 94), train_loss = -4.092, time/batch = 0.006\n",
      "3121/3300 (epoch 94), train_loss = -3.958, time/batch = 0.005\n",
      "3122/3300 (epoch 94), train_loss = -3.165, time/batch = 0.005\n",
      "3123/3300 (epoch 94), train_loss = -3.672, time/batch = 0.007\n",
      "3124/3300 (epoch 94), train_loss = -4.460, time/batch = 0.006\n",
      "3125/3300 (epoch 94), train_loss = -4.563, time/batch = 0.005\n",
      "3126/3300 (epoch 94), train_loss = -4.817, time/batch = 0.005\n",
      "3127/3300 (epoch 94), train_loss = -4.635, time/batch = 0.005\n",
      "3128/3300 (epoch 94), train_loss = -4.393, time/batch = 0.005\n",
      "3129/3300 (epoch 94), train_loss = -5.189, time/batch = 0.006\n",
      "3130/3300 (epoch 94), train_loss = -5.252, time/batch = 0.005\n",
      "3131/3300 (epoch 94), train_loss = -5.290, time/batch = 0.007\n",
      "3132/3300 (epoch 94), train_loss = -4.883, time/batch = 0.005\n",
      "3133/3300 (epoch 94), train_loss = -4.255, time/batch = 0.007\n",
      "3134/3300 (epoch 94), train_loss = -4.074, time/batch = 0.006\n",
      "3135/3300 (epoch 95), train_loss = -1.246, time/batch = 0.010\n",
      "3136/3300 (epoch 95), train_loss = -2.198, time/batch = 0.007\n",
      "3137/3300 (epoch 95), train_loss = -2.326, time/batch = 0.005\n",
      "3138/3300 (epoch 95), train_loss = -2.504, time/batch = 0.009\n",
      "3139/3300 (epoch 95), train_loss = -1.301, time/batch = 0.008\n",
      "3140/3300 (epoch 95), train_loss = -0.349, time/batch = 0.005\n",
      "3141/3300 (epoch 95), train_loss = -0.816, time/batch = 0.012\n",
      "3142/3300 (epoch 95), train_loss = -0.086, time/batch = 0.010\n",
      "3143/3300 (epoch 95), train_loss = -4.325, time/batch = 0.006\n",
      "3144/3300 (epoch 95), train_loss = -4.010, time/batch = 0.005\n",
      "3145/3300 (epoch 95), train_loss = -3.930, time/batch = 0.004\n",
      "3146/3300 (epoch 95), train_loss = -4.163, time/batch = 0.005\n",
      "3147/3300 (epoch 95), train_loss = -3.898, time/batch = 0.004\n",
      "3148/3300 (epoch 95), train_loss = -4.258, time/batch = 0.005\n",
      "3149/3300 (epoch 95), train_loss = -4.179, time/batch = 0.005\n",
      "3150/3300 (epoch 95), train_loss = -4.121, time/batch = 0.009\n",
      "3151/3300 (epoch 95), train_loss = -4.211, time/batch = 0.005\n",
      "3152/3300 (epoch 95), train_loss = -3.446, time/batch = 0.005\n",
      "3153/3300 (epoch 95), train_loss = -3.329, time/batch = 0.008\n",
      "3154/3300 (epoch 95), train_loss = -4.222, time/batch = 0.009\n",
      "3155/3300 (epoch 95), train_loss = -4.435, time/batch = 0.009\n",
      "3156/3300 (epoch 95), train_loss = -4.761, time/batch = 0.017\n",
      "3157/3300 (epoch 95), train_loss = -4.718, time/batch = 0.016\n",
      "3158/3300 (epoch 95), train_loss = -3.718, time/batch = 0.009\n",
      "3159/3300 (epoch 95), train_loss = -4.656, time/batch = 0.009\n",
      "3160/3300 (epoch 95), train_loss = -5.207, time/batch = 0.007\n",
      "3161/3300 (epoch 95), train_loss = -5.323, time/batch = 0.008\n",
      "3162/3300 (epoch 95), train_loss = -4.874, time/batch = 0.006\n",
      "3163/3300 (epoch 95), train_loss = -3.840, time/batch = 0.007\n",
      "3164/3300 (epoch 95), train_loss = -4.152, time/batch = 0.008\n",
      "3165/3300 (epoch 95), train_loss = -5.369, time/batch = 0.005\n",
      "3166/3300 (epoch 95), train_loss = -4.927, time/batch = 0.009\n",
      "3167/3300 (epoch 95), train_loss = -3.926, time/batch = 0.007\n",
      "3168/3300 (epoch 96), train_loss = -1.267, time/batch = 0.008\n",
      "3169/3300 (epoch 96), train_loss = -2.113, time/batch = 0.009\n",
      "3170/3300 (epoch 96), train_loss = -2.252, time/batch = 0.009\n",
      "3171/3300 (epoch 96), train_loss = -2.193, time/batch = 0.012\n",
      "3172/3300 (epoch 96), train_loss = -1.388, time/batch = 0.008\n",
      "3173/3300 (epoch 96), train_loss = -1.513, time/batch = 0.009\n",
      "3174/3300 (epoch 96), train_loss = -1.321, time/batch = 0.010\n",
      "3175/3300 (epoch 96), train_loss = -1.584, time/batch = 0.016\n",
      "3176/3300 (epoch 96), train_loss = -4.365, time/batch = 0.009\n",
      "3177/3300 (epoch 96), train_loss = -4.045, time/batch = 0.006\n",
      "3178/3300 (epoch 96), train_loss = -4.000, time/batch = 0.008\n",
      "3179/3300 (epoch 96), train_loss = -3.962, time/batch = 0.006\n",
      "3180/3300 (epoch 96), train_loss = -4.312, time/batch = 0.007\n",
      "3181/3300 (epoch 96), train_loss = -4.488, time/batch = 0.009\n",
      "3182/3300 (epoch 96), train_loss = -4.163, time/batch = 0.009\n",
      "3183/3300 (epoch 96), train_loss = -4.235, time/batch = 0.007\n",
      "3184/3300 (epoch 96), train_loss = -4.306, time/batch = 0.006\n",
      "3185/3300 (epoch 96), train_loss = -2.854, time/batch = 0.010\n",
      "3186/3300 (epoch 96), train_loss = -4.242, time/batch = 0.009\n",
      "3187/3300 (epoch 96), train_loss = -3.923, time/batch = 0.005\n",
      "3188/3300 (epoch 96), train_loss = -3.801, time/batch = 0.005\n",
      "3189/3300 (epoch 96), train_loss = -4.726, time/batch = 0.005\n",
      "3190/3300 (epoch 96), train_loss = -4.810, time/batch = 0.006\n",
      "3191/3300 (epoch 96), train_loss = -4.097, time/batch = 0.009\n",
      "3192/3300 (epoch 96), train_loss = -5.237, time/batch = 0.007\n",
      "3193/3300 (epoch 96), train_loss = -5.245, time/batch = 0.008\n",
      "3194/3300 (epoch 96), train_loss = -5.327, time/batch = 0.007\n",
      "3195/3300 (epoch 96), train_loss = -5.266, time/batch = 0.008\n",
      "3196/3300 (epoch 96), train_loss = -5.317, time/batch = 0.008\n",
      "3197/3300 (epoch 96), train_loss = -4.699, time/batch = 0.006\n",
      "3198/3300 (epoch 96), train_loss = -3.678, time/batch = 0.006\n",
      "3199/3300 (epoch 96), train_loss = -4.322, time/batch = 0.005\n",
      "3200/3300 (epoch 96), train_loss = -4.892, time/batch = 0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3201/3300 (epoch 97), train_loss = -1.279, time/batch = 0.009\n",
      "3202/3300 (epoch 97), train_loss = -2.163, time/batch = 0.010\n",
      "3203/3300 (epoch 97), train_loss = -2.333, time/batch = 0.007\n",
      "3204/3300 (epoch 97), train_loss = -1.972, time/batch = 0.006\n",
      "3205/3300 (epoch 97), train_loss = -1.629, time/batch = 0.007\n",
      "3206/3300 (epoch 97), train_loss = 0.073, time/batch = 0.007\n",
      "3207/3300 (epoch 97), train_loss = 0.632, time/batch = 0.007\n",
      "3208/3300 (epoch 97), train_loss = -0.161, time/batch = 0.007\n",
      "3209/3300 (epoch 97), train_loss = -3.039, time/batch = 0.009\n",
      "3210/3300 (epoch 97), train_loss = -4.395, time/batch = 0.005\n",
      "3211/3300 (epoch 97), train_loss = -3.827, time/batch = 0.006\n",
      "3212/3300 (epoch 97), train_loss = -4.208, time/batch = 0.006\n",
      "3213/3300 (epoch 97), train_loss = -3.915, time/batch = 0.005\n",
      "3214/3300 (epoch 97), train_loss = -3.939, time/batch = 0.007\n",
      "3215/3300 (epoch 97), train_loss = -4.432, time/batch = 0.008\n",
      "3216/3300 (epoch 97), train_loss = -4.517, time/batch = 0.010\n",
      "3217/3300 (epoch 97), train_loss = -4.167, time/batch = 0.006\n",
      "3218/3300 (epoch 97), train_loss = -4.289, time/batch = 0.007\n",
      "3219/3300 (epoch 97), train_loss = -3.763, time/batch = 0.007\n",
      "3220/3300 (epoch 97), train_loss = -3.873, time/batch = 0.009\n",
      "3221/3300 (epoch 97), train_loss = -4.352, time/batch = 0.008\n",
      "3222/3300 (epoch 97), train_loss = -4.364, time/batch = 0.008\n",
      "3223/3300 (epoch 97), train_loss = -4.118, time/batch = 0.008\n",
      "3224/3300 (epoch 97), train_loss = -5.240, time/batch = 0.008\n",
      "3225/3300 (epoch 97), train_loss = -5.350, time/batch = 0.007\n",
      "3226/3300 (epoch 97), train_loss = -5.336, time/batch = 0.009\n",
      "3227/3300 (epoch 97), train_loss = -5.356, time/batch = 0.007\n",
      "3228/3300 (epoch 97), train_loss = -5.404, time/batch = 0.007\n",
      "3229/3300 (epoch 97), train_loss = -5.363, time/batch = 0.007\n",
      "3230/3300 (epoch 97), train_loss = -4.497, time/batch = 0.007\n",
      "3231/3300 (epoch 97), train_loss = -4.416, time/batch = 0.010\n",
      "3232/3300 (epoch 97), train_loss = -4.205, time/batch = 0.009\n",
      "3233/3300 (epoch 97), train_loss = -5.195, time/batch = 0.010\n",
      "3234/3300 (epoch 98), train_loss = -1.317, time/batch = 0.009\n",
      "3235/3300 (epoch 98), train_loss = -2.111, time/batch = 0.008\n",
      "3236/3300 (epoch 98), train_loss = -2.494, time/batch = 0.010\n",
      "3237/3300 (epoch 98), train_loss = -3.443, time/batch = 0.008\n",
      "3238/3300 (epoch 98), train_loss = -1.843, time/batch = 0.007\n",
      "3239/3300 (epoch 98), train_loss = -0.543, time/batch = 0.009\n",
      "3240/3300 (epoch 98), train_loss = 0.854, time/batch = 0.006\n",
      "3241/3300 (epoch 98), train_loss = 1.128, time/batch = 0.008\n",
      "3242/3300 (epoch 98), train_loss = 0.232, time/batch = 0.008\n",
      "3243/3300 (epoch 98), train_loss = -4.514, time/batch = 0.006\n",
      "3244/3300 (epoch 98), train_loss = -3.724, time/batch = 0.009\n",
      "3245/3300 (epoch 98), train_loss = -4.292, time/batch = 0.009\n",
      "3246/3300 (epoch 98), train_loss = -3.910, time/batch = 0.007\n",
      "3247/3300 (epoch 98), train_loss = -4.505, time/batch = 0.007\n",
      "3248/3300 (epoch 98), train_loss = -4.340, time/batch = 0.009\n",
      "3249/3300 (epoch 98), train_loss = -4.032, time/batch = 0.006\n",
      "3250/3300 (epoch 98), train_loss = -4.153, time/batch = 0.010\n",
      "3251/3300 (epoch 98), train_loss = -4.152, time/batch = 0.008\n",
      "3252/3300 (epoch 98), train_loss = -3.693, time/batch = 0.006\n",
      "3253/3300 (epoch 98), train_loss = -2.700, time/batch = 0.007\n",
      "3254/3300 (epoch 98), train_loss = -4.012, time/batch = 0.010\n",
      "3255/3300 (epoch 98), train_loss = -4.215, time/batch = 0.010\n",
      "3256/3300 (epoch 98), train_loss = -4.491, time/batch = 0.014\n",
      "3257/3300 (epoch 98), train_loss = -4.655, time/batch = 0.010\n",
      "3258/3300 (epoch 98), train_loss = -5.282, time/batch = 0.016\n",
      "3259/3300 (epoch 98), train_loss = -5.216, time/batch = 0.008\n",
      "3260/3300 (epoch 98), train_loss = -3.509, time/batch = 0.005\n",
      "3261/3300 (epoch 98), train_loss = -4.305, time/batch = 0.007\n",
      "3262/3300 (epoch 98), train_loss = -4.933, time/batch = 0.007\n",
      "3263/3300 (epoch 98), train_loss = -3.588, time/batch = 0.007\n",
      "3264/3300 (epoch 98), train_loss = -3.961, time/batch = 0.007\n",
      "3265/3300 (epoch 98), train_loss = -4.530, time/batch = 0.007\n",
      "3266/3300 (epoch 98), train_loss = -4.476, time/batch = 0.005\n",
      "3267/3300 (epoch 99), train_loss = -1.291, time/batch = 0.007\n",
      "3268/3300 (epoch 99), train_loss = -2.084, time/batch = 0.006\n",
      "3269/3300 (epoch 99), train_loss = -2.189, time/batch = 0.009\n",
      "3270/3300 (epoch 99), train_loss = -2.081, time/batch = 0.009\n",
      "3271/3300 (epoch 99), train_loss = -1.132, time/batch = 0.008\n",
      "3272/3300 (epoch 99), train_loss = -0.761, time/batch = 0.006\n",
      "3273/3300 (epoch 99), train_loss = -0.212, time/batch = 0.010\n",
      "3274/3300 (epoch 99), train_loss = -1.019, time/batch = 0.008\n",
      "3275/3300 (epoch 99), train_loss = -4.191, time/batch = 0.009\n",
      "3276/3300 (epoch 99), train_loss = -3.963, time/batch = 0.012\n",
      "3277/3300 (epoch 99), train_loss = -4.177, time/batch = 0.013\n",
      "3278/3300 (epoch 99), train_loss = -4.205, time/batch = 0.016\n",
      "3279/3300 (epoch 99), train_loss = -4.057, time/batch = 0.006\n",
      "3280/3300 (epoch 99), train_loss = -4.544, time/batch = 0.007\n",
      "3281/3300 (epoch 99), train_loss = -3.869, time/batch = 0.004\n",
      "3282/3300 (epoch 99), train_loss = -4.434, time/batch = 0.011\n",
      "3283/3300 (epoch 99), train_loss = -3.582, time/batch = 0.007\n",
      "3284/3300 (epoch 99), train_loss = -3.450, time/batch = 0.005\n",
      "3285/3300 (epoch 99), train_loss = -3.967, time/batch = 0.004\n",
      "3286/3300 (epoch 99), train_loss = -4.465, time/batch = 0.005\n",
      "3287/3300 (epoch 99), train_loss = -4.713, time/batch = 0.004\n",
      "3288/3300 (epoch 99), train_loss = -4.054, time/batch = 0.004\n",
      "3289/3300 (epoch 99), train_loss = -5.157, time/batch = 0.006\n",
      "3290/3300 (epoch 99), train_loss = -5.277, time/batch = 0.004\n",
      "3291/3300 (epoch 99), train_loss = -5.322, time/batch = 0.004\n",
      "3292/3300 (epoch 99), train_loss = -5.055, time/batch = 0.004\n",
      "3293/3300 (epoch 99), train_loss = -4.134, time/batch = 0.004\n",
      "3294/3300 (epoch 99), train_loss = -3.707, time/batch = 0.004\n",
      "3295/3300 (epoch 99), train_loss = -4.647, time/batch = 0.006\n",
      "3296/3300 (epoch 99), train_loss = -5.470, time/batch = 0.006\n",
      "3297/3300 (epoch 99), train_loss = -5.060, time/batch = 0.006\n",
      "3298/3300 (epoch 99), train_loss = -3.613, time/batch = 0.006\n",
      "3299/3300 (epoch 99), train_loss = -4.169, time/batch = 0.004\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "lstm = BasicLSTM()\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    # Assign the learning rate (decayed acc. to the epoch number)\n",
    "    lstm.sess.run(tf.assign(lstm.lr, lstm.learning_rate * (decay_rate ** e)))\n",
    "    # Reset the pointers in the data loader object\n",
    "    pointer = reset_batch_pointer()\n",
    "    # Get the initial cell state of the LSTM\n",
    "    state = lstm.sess.run(lstm.initial_state)\n",
    "    \n",
    "    # For each batch in this epoch\n",
    "    for b in range(num_batches):\n",
    "        start = time.time()\n",
    "        # Get the source and target data of the current batch\n",
    "        # x has the source data, y has the target data\n",
    "        x, y, pointer = next_batch(loaded_data, pointer)\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Feed the source, target data and the initial LSTM state to the model\n",
    "        feed = {lstm.input_data: x[:, :, 1:], lstm.target_data: y[:, :, 1:], lstm.initial_state: state}\n",
    "        # Fetch the loss of the model on this batch, the final LSTM state from the session\n",
    "        train_loss, state, _ = lstm.sess.run([lstm.cost, lstm.final_state, lstm.train_op], feed)\n",
    "        # Toc\n",
    "        end = time.time()\n",
    "        # Print epoch, batch, loss and time taken\n",
    "        print(\n",
    "            \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "            .format(\n",
    "                e * num_batches + b,\n",
    "                num_epochs * num_batches,\n",
    "                e,\n",
    "                train_loss, end - start))\n",
    "\n",
    "lstm.save_json(\"params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/todor/Documents/data/ewap_dataset/seq_hotel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_length = 5\n",
    "predicted_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "sequence_length = observed_length + predicted_length\n",
    "total_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/todor/Documents/data/ewap_dataset/seq_hotel\n"
     ]
    }
   ],
   "source": [
    "agentsData, statistics, peds_in_frame = frame_preprocess(directory)\n",
    "data, num_batches = load_preprocessed(agentsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_error(predicted_traj, true_traj, observed_length):\n",
    "    '''\n",
    "    Function that computes the mean euclidean distance error between the\n",
    "    predicted and the true trajectory\n",
    "    params:\n",
    "    predicted_traj : numpy matrix with the points of the predicted trajectory\n",
    "    true_traj : numpy matrix with the points of the true trajectory\n",
    "    observed_length : The length of trajectory observed\n",
    "    taken from: https://github.com/vvanirudh/social-lstm-tf\n",
    "    '''\n",
    "    # The data structure to store all errors\n",
    "    error = np.zeros(len(true_traj) - observed_length)\n",
    "    # For each point in the predicted part of the trajectory\n",
    "    for i in range(observed_length, len(true_traj)):\n",
    "        # The predicted position\n",
    "        pred_pos = predicted_traj[i, :]\n",
    "        # The true position\n",
    "        true_pos = true_traj[i, :]\n",
    "\n",
    "        # The euclidean distance is the error\n",
    "        error[i-observed_length] = np.linalg.norm(true_pos - pred_pos)\n",
    "\n",
    "    # Return the mean error\n",
    "    return np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph successfully reset\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "lstm = BasicLSTM(\n",
    "            batch_size=1,\n",
    "            sequence_length=1)\n",
    "lstm.load_json(\"params.json\")\n",
    "\n",
    "state = lstm.sess.run(lstm.initial_state)\n",
    "pointer = reset_batch_pointer()\n",
    "\n",
    "xs = []\n",
    "ys = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trajectory number :  0 out of  352  trajectories\n",
      "Processed trajectory number :  1 out of  352  trajectories\n",
      "Processed trajectory number :  2 out of  352  trajectories\n",
      "Processed trajectory number :  3 out of  352  trajectories\n",
      "Processed trajectory number :  4 out of  352  trajectories\n",
      "Processed trajectory number :  5 out of  352  trajectories\n",
      "Processed trajectory number :  6 out of  352  trajectories\n",
      "Processed trajectory number :  7 out of  352  trajectories\n",
      "Processed trajectory number :  8 out of  352  trajectories\n",
      "Processed trajectory number :  9 out of  352  trajectories\n",
      "Processed trajectory number :  10 out of  352  trajectories\n",
      "Processed trajectory number :  11 out of  352  trajectories\n",
      "Processed trajectory number :  12 out of  352  trajectories\n",
      "Processed trajectory number :  13 out of  352  trajectories\n",
      "Processed trajectory number :  14 out of  352  trajectories\n",
      "Processed trajectory number :  15 out of  352  trajectories\n",
      "Processed trajectory number :  16 out of  352  trajectories\n",
      "Processed trajectory number :  17 out of  352  trajectories\n",
      "Processed trajectory number :  18 out of  352  trajectories\n",
      "Processed trajectory number :  19 out of  352  trajectories\n",
      "Processed trajectory number :  20 out of  352  trajectories\n",
      "Processed trajectory number :  21 out of  352  trajectories\n",
      "Processed trajectory number :  22 out of  352  trajectories\n",
      "Processed trajectory number :  23 out of  352  trajectories\n",
      "Processed trajectory number :  24 out of  352  trajectories\n",
      "Processed trajectory number :  25 out of  352  trajectories\n",
      "Processed trajectory number :  26 out of  352  trajectories\n",
      "Processed trajectory number :  27 out of  352  trajectories\n",
      "Processed trajectory number :  28 out of  352  trajectories\n",
      "Processed trajectory number :  29 out of  352  trajectories\n",
      "Processed trajectory number :  30 out of  352  trajectories\n",
      "Processed trajectory number :  31 out of  352  trajectories\n",
      "Processed trajectory number :  32 out of  352  trajectories\n",
      "Processed trajectory number :  33 out of  352  trajectories\n",
      "Processed trajectory number :  34 out of  352  trajectories\n",
      "Processed trajectory number :  35 out of  352  trajectories\n",
      "Processed trajectory number :  36 out of  352  trajectories\n",
      "Processed trajectory number :  37 out of  352  trajectories\n",
      "Processed trajectory number :  38 out of  352  trajectories\n",
      "Processed trajectory number :  39 out of  352  trajectories\n",
      "Processed trajectory number :  40 out of  352  trajectories\n",
      "Processed trajectory number :  41 out of  352  trajectories\n",
      "Processed trajectory number :  42 out of  352  trajectories\n",
      "Processed trajectory number :  43 out of  352  trajectories\n",
      "Processed trajectory number :  44 out of  352  trajectories\n",
      "Processed trajectory number :  45 out of  352  trajectories\n",
      "Processed trajectory number :  46 out of  352  trajectories\n",
      "Processed trajectory number :  47 out of  352  trajectories\n",
      "Processed trajectory number :  48 out of  352  trajectories\n",
      "Processed trajectory number :  49 out of  352  trajectories\n",
      "Processed trajectory number :  50 out of  352  trajectories\n",
      "Processed trajectory number :  51 out of  352  trajectories\n",
      "Processed trajectory number :  52 out of  352  trajectories\n",
      "Processed trajectory number :  53 out of  352  trajectories\n",
      "Processed trajectory number :  54 out of  352  trajectories\n",
      "Processed trajectory number :  55 out of  352  trajectories\n",
      "Processed trajectory number :  56 out of  352  trajectories\n",
      "Processed trajectory number :  57 out of  352  trajectories\n",
      "Processed trajectory number :  58 out of  352  trajectories\n",
      "Processed trajectory number :  59 out of  352  trajectories\n",
      "Processed trajectory number :  60 out of  352  trajectories\n",
      "Processed trajectory number :  61 out of  352  trajectories\n",
      "Processed trajectory number :  62 out of  352  trajectories\n",
      "Processed trajectory number :  63 out of  352  trajectories\n",
      "Processed trajectory number :  64 out of  352  trajectories\n",
      "Processed trajectory number :  65 out of  352  trajectories\n",
      "Processed trajectory number :  66 out of  352  trajectories\n",
      "Processed trajectory number :  67 out of  352  trajectories\n",
      "Processed trajectory number :  68 out of  352  trajectories\n",
      "Processed trajectory number :  69 out of  352  trajectories\n",
      "Processed trajectory number :  70 out of  352  trajectories\n",
      "Processed trajectory number :  71 out of  352  trajectories\n",
      "Processed trajectory number :  72 out of  352  trajectories\n",
      "Processed trajectory number :  73 out of  352  trajectories\n",
      "Processed trajectory number :  74 out of  352  trajectories\n",
      "Processed trajectory number :  75 out of  352  trajectories\n",
      "Processed trajectory number :  76 out of  352  trajectories\n",
      "Processed trajectory number :  77 out of  352  trajectories\n",
      "Processed trajectory number :  78 out of  352  trajectories\n",
      "Processed trajectory number :  79 out of  352  trajectories\n",
      "Processed trajectory number :  80 out of  352  trajectories\n",
      "Processed trajectory number :  81 out of  352  trajectories\n",
      "Processed trajectory number :  82 out of  352  trajectories\n",
      "Processed trajectory number :  83 out of  352  trajectories\n",
      "Processed trajectory number :  84 out of  352  trajectories\n",
      "Processed trajectory number :  85 out of  352  trajectories\n",
      "Processed trajectory number :  86 out of  352  trajectories\n",
      "Processed trajectory number :  87 out of  352  trajectories\n",
      "Processed trajectory number :  88 out of  352  trajectories\n",
      "Processed trajectory number :  89 out of  352  trajectories\n",
      "Processed trajectory number :  90 out of  352  trajectories\n",
      "Processed trajectory number :  91 out of  352  trajectories\n",
      "Processed trajectory number :  92 out of  352  trajectories\n",
      "Processed trajectory number :  93 out of  352  trajectories\n",
      "Processed trajectory number :  94 out of  352  trajectories\n",
      "Processed trajectory number :  95 out of  352  trajectories\n",
      "Processed trajectory number :  96 out of  352  trajectories\n",
      "Processed trajectory number :  97 out of  352  trajectories\n",
      "Processed trajectory number :  98 out of  352  trajectories\n",
      "Processed trajectory number :  99 out of  352  trajectories\n",
      "Processed trajectory number :  100 out of  352  trajectories\n",
      "Processed trajectory number :  101 out of  352  trajectories\n",
      "Processed trajectory number :  102 out of  352  trajectories\n",
      "Processed trajectory number :  103 out of  352  trajectories\n",
      "Processed trajectory number :  104 out of  352  trajectories\n",
      "Processed trajectory number :  105 out of  352  trajectories\n",
      "Processed trajectory number :  106 out of  352  trajectories\n",
      "Processed trajectory number :  107 out of  352  trajectories\n",
      "Processed trajectory number :  108 out of  352  trajectories\n",
      "Processed trajectory number :  109 out of  352  trajectories\n",
      "Processed trajectory number :  110 out of  352  trajectories\n",
      "Processed trajectory number :  111 out of  352  trajectories\n",
      "Processed trajectory number :  112 out of  352  trajectories\n",
      "Processed trajectory number :  113 out of  352  trajectories\n",
      "Processed trajectory number :  114 out of  352  trajectories\n",
      "Processed trajectory number :  115 out of  352  trajectories\n",
      "Processed trajectory number :  116 out of  352  trajectories\n",
      "Processed trajectory number :  117 out of  352  trajectories\n",
      "Processed trajectory number :  118 out of  352  trajectories\n",
      "Processed trajectory number :  119 out of  352  trajectories\n",
      "Processed trajectory number :  120 out of  352  trajectories\n",
      "Processed trajectory number :  121 out of  352  trajectories\n",
      "Processed trajectory number :  122 out of  352  trajectories\n",
      "Processed trajectory number :  123 out of  352  trajectories\n",
      "Processed trajectory number :  124 out of  352  trajectories\n",
      "Processed trajectory number :  125 out of  352  trajectories\n",
      "Processed trajectory number :  126 out of  352  trajectories\n",
      "Processed trajectory number :  127 out of  352  trajectories\n",
      "Processed trajectory number :  128 out of  352  trajectories\n",
      "Processed trajectory number :  129 out of  352  trajectories\n",
      "Processed trajectory number :  130 out of  352  trajectories\n",
      "Processed trajectory number :  131 out of  352  trajectories\n",
      "Processed trajectory number :  132 out of  352  trajectories\n",
      "Processed trajectory number :  133 out of  352  trajectories\n",
      "Processed trajectory number :  134 out of  352  trajectories\n",
      "Processed trajectory number :  135 out of  352  trajectories\n",
      "Processed trajectory number :  136 out of  352  trajectories\n",
      "Processed trajectory number :  137 out of  352  trajectories\n",
      "Processed trajectory number :  138 out of  352  trajectories\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trajectory number :  139 out of  352  trajectories\n",
      "Processed trajectory number :  140 out of  352  trajectories\n",
      "Processed trajectory number :  141 out of  352  trajectories\n",
      "Processed trajectory number :  142 out of  352  trajectories\n",
      "Processed trajectory number :  143 out of  352  trajectories\n",
      "Processed trajectory number :  144 out of  352  trajectories\n",
      "Processed trajectory number :  145 out of  352  trajectories\n",
      "Processed trajectory number :  146 out of  352  trajectories\n",
      "Processed trajectory number :  147 out of  352  trajectories\n",
      "Processed trajectory number :  148 out of  352  trajectories\n",
      "Processed trajectory number :  149 out of  352  trajectories\n",
      "Processed trajectory number :  150 out of  352  trajectories\n",
      "Processed trajectory number :  151 out of  352  trajectories\n",
      "Processed trajectory number :  152 out of  352  trajectories\n",
      "Processed trajectory number :  153 out of  352  trajectories\n",
      "Processed trajectory number :  154 out of  352  trajectories\n",
      "Processed trajectory number :  155 out of  352  trajectories\n",
      "Processed trajectory number :  156 out of  352  trajectories\n",
      "Processed trajectory number :  157 out of  352  trajectories\n",
      "Processed trajectory number :  158 out of  352  trajectories\n",
      "Processed trajectory number :  159 out of  352  trajectories\n",
      "Processed trajectory number :  160 out of  352  trajectories\n",
      "Processed trajectory number :  161 out of  352  trajectories\n",
      "Processed trajectory number :  162 out of  352  trajectories\n",
      "Processed trajectory number :  163 out of  352  trajectories\n",
      "Processed trajectory number :  164 out of  352  trajectories\n",
      "Processed trajectory number :  165 out of  352  trajectories\n",
      "Processed trajectory number :  166 out of  352  trajectories\n",
      "Processed trajectory number :  167 out of  352  trajectories\n",
      "Processed trajectory number :  168 out of  352  trajectories\n",
      "Processed trajectory number :  169 out of  352  trajectories\n",
      "Processed trajectory number :  170 out of  352  trajectories\n",
      "Processed trajectory number :  171 out of  352  trajectories\n",
      "Processed trajectory number :  172 out of  352  trajectories\n",
      "Processed trajectory number :  173 out of  352  trajectories\n",
      "Processed trajectory number :  174 out of  352  trajectories\n",
      "Processed trajectory number :  175 out of  352  trajectories\n",
      "Processed trajectory number :  176 out of  352  trajectories\n",
      "Processed trajectory number :  177 out of  352  trajectories\n",
      "Processed trajectory number :  178 out of  352  trajectories\n",
      "Processed trajectory number :  179 out of  352  trajectories\n",
      "Processed trajectory number :  180 out of  352  trajectories\n",
      "Processed trajectory number :  181 out of  352  trajectories\n",
      "Processed trajectory number :  182 out of  352  trajectories\n",
      "Processed trajectory number :  183 out of  352  trajectories\n",
      "Processed trajectory number :  184 out of  352  trajectories\n",
      "Processed trajectory number :  185 out of  352  trajectories\n",
      "Processed trajectory number :  186 out of  352  trajectories\n",
      "Processed trajectory number :  187 out of  352  trajectories\n",
      "Processed trajectory number :  188 out of  352  trajectories\n",
      "Processed trajectory number :  189 out of  352  trajectories\n",
      "Processed trajectory number :  190 out of  352  trajectories\n",
      "Processed trajectory number :  191 out of  352  trajectories\n",
      "Processed trajectory number :  192 out of  352  trajectories\n",
      "Processed trajectory number :  193 out of  352  trajectories\n",
      "Processed trajectory number :  194 out of  352  trajectories\n",
      "Processed trajectory number :  195 out of  352  trajectories\n",
      "Processed trajectory number :  196 out of  352  trajectories\n",
      "Processed trajectory number :  197 out of  352  trajectories\n",
      "Processed trajectory number :  198 out of  352  trajectories\n",
      "Processed trajectory number :  199 out of  352  trajectories\n",
      "Processed trajectory number :  200 out of  352  trajectories\n",
      "Processed trajectory number :  201 out of  352  trajectories\n",
      "Processed trajectory number :  202 out of  352  trajectories\n",
      "Processed trajectory number :  203 out of  352  trajectories\n",
      "Processed trajectory number :  204 out of  352  trajectories\n",
      "Processed trajectory number :  205 out of  352  trajectories\n",
      "Processed trajectory number :  206 out of  352  trajectories\n",
      "Processed trajectory number :  207 out of  352  trajectories\n",
      "Processed trajectory number :  208 out of  352  trajectories\n",
      "Processed trajectory number :  209 out of  352  trajectories\n",
      "Processed trajectory number :  210 out of  352  trajectories\n",
      "Processed trajectory number :  211 out of  352  trajectories\n",
      "Processed trajectory number :  212 out of  352  trajectories\n",
      "Processed trajectory number :  213 out of  352  trajectories\n",
      "Processed trajectory number :  214 out of  352  trajectories\n",
      "Processed trajectory number :  215 out of  352  trajectories\n",
      "Processed trajectory number :  216 out of  352  trajectories\n",
      "Processed trajectory number :  217 out of  352  trajectories\n",
      "Processed trajectory number :  218 out of  352  trajectories\n",
      "Processed trajectory number :  219 out of  352  trajectories\n",
      "Processed trajectory number :  220 out of  352  trajectories\n",
      "Processed trajectory number :  221 out of  352  trajectories\n",
      "Processed trajectory number :  222 out of  352  trajectories\n",
      "Processed trajectory number :  223 out of  352  trajectories\n",
      "Processed trajectory number :  224 out of  352  trajectories\n",
      "Processed trajectory number :  225 out of  352  trajectories\n",
      "Processed trajectory number :  226 out of  352  trajectories\n",
      "Processed trajectory number :  227 out of  352  trajectories\n",
      "Processed trajectory number :  228 out of  352  trajectories\n",
      "Processed trajectory number :  229 out of  352  trajectories\n",
      "Processed trajectory number :  230 out of  352  trajectories\n",
      "Processed trajectory number :  231 out of  352  trajectories\n",
      "Processed trajectory number :  232 out of  352  trajectories\n",
      "Processed trajectory number :  233 out of  352  trajectories\n",
      "Processed trajectory number :  234 out of  352  trajectories\n",
      "Processed trajectory number :  235 out of  352  trajectories\n",
      "Processed trajectory number :  236 out of  352  trajectories\n",
      "Processed trajectory number :  237 out of  352  trajectories\n",
      "Processed trajectory number :  238 out of  352  trajectories\n",
      "Processed trajectory number :  239 out of  352  trajectories\n",
      "Processed trajectory number :  240 out of  352  trajectories\n",
      "Processed trajectory number :  241 out of  352  trajectories\n",
      "Processed trajectory number :  242 out of  352  trajectories\n",
      "Processed trajectory number :  243 out of  352  trajectories\n",
      "Processed trajectory number :  244 out of  352  trajectories\n",
      "Processed trajectory number :  245 out of  352  trajectories\n",
      "Processed trajectory number :  246 out of  352  trajectories\n",
      "Processed trajectory number :  247 out of  352  trajectories\n",
      "Processed trajectory number :  248 out of  352  trajectories\n",
      "Processed trajectory number :  249 out of  352  trajectories\n",
      "Processed trajectory number :  250 out of  352  trajectories\n",
      "Processed trajectory number :  251 out of  352  trajectories\n",
      "Processed trajectory number :  252 out of  352  trajectories\n",
      "Processed trajectory number :  253 out of  352  trajectories\n",
      "Processed trajectory number :  254 out of  352  trajectories\n",
      "Processed trajectory number :  255 out of  352  trajectories\n",
      "Processed trajectory number :  256 out of  352  trajectories\n",
      "Processed trajectory number :  257 out of  352  trajectories\n",
      "Processed trajectory number :  258 out of  352  trajectories\n",
      "Processed trajectory number :  259 out of  352  trajectories\n",
      "Processed trajectory number :  260 out of  352  trajectories\n",
      "Processed trajectory number :  261 out of  352  trajectories\n",
      "Processed trajectory number :  262 out of  352  trajectories\n",
      "Processed trajectory number :  263 out of  352  trajectories\n",
      "Processed trajectory number :  264 out of  352  trajectories\n",
      "Processed trajectory number :  265 out of  352  trajectories\n",
      "Processed trajectory number :  266 out of  352  trajectories\n",
      "Processed trajectory number :  267 out of  352  trajectories\n",
      "Processed trajectory number :  268 out of  352  trajectories\n",
      "Processed trajectory number :  269 out of  352  trajectories\n",
      "Processed trajectory number :  270 out of  352  trajectories\n",
      "Processed trajectory number :  271 out of  352  trajectories\n",
      "Processed trajectory number :  272 out of  352  trajectories\n",
      "Processed trajectory number :  273 out of  352  trajectories\n",
      "Processed trajectory number :  274 out of  352  trajectories\n",
      "Processed trajectory number :  275 out of  352  trajectories\n",
      "Processed trajectory number :  276 out of  352  trajectories\n",
      "Processed trajectory number :  277 out of  352  trajectories\n",
      "Processed trajectory number :  278 out of  352  trajectories\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trajectory number :  279 out of  352  trajectories\n",
      "Processed trajectory number :  280 out of  352  trajectories\n",
      "Processed trajectory number :  281 out of  352  trajectories\n",
      "Processed trajectory number :  282 out of  352  trajectories\n",
      "Processed trajectory number :  283 out of  352  trajectories\n",
      "Processed trajectory number :  284 out of  352  trajectories\n",
      "Processed trajectory number :  285 out of  352  trajectories\n",
      "Processed trajectory number :  286 out of  352  trajectories\n",
      "Processed trajectory number :  287 out of  352  trajectories\n",
      "Processed trajectory number :  288 out of  352  trajectories\n",
      "Processed trajectory number :  289 out of  352  trajectories\n",
      "Processed trajectory number :  290 out of  352  trajectories\n",
      "Processed trajectory number :  291 out of  352  trajectories\n",
      "Processed trajectory number :  292 out of  352  trajectories\n",
      "Processed trajectory number :  293 out of  352  trajectories\n",
      "Processed trajectory number :  294 out of  352  trajectories\n",
      "Processed trajectory number :  295 out of  352  trajectories\n",
      "Processed trajectory number :  296 out of  352  trajectories\n",
      "Processed trajectory number :  297 out of  352  trajectories\n",
      "Processed trajectory number :  298 out of  352  trajectories\n",
      "Processed trajectory number :  299 out of  352  trajectories\n",
      "Processed trajectory number :  300 out of  352  trajectories\n",
      "Processed trajectory number :  301 out of  352  trajectories\n",
      "Processed trajectory number :  302 out of  352  trajectories\n",
      "Processed trajectory number :  303 out of  352  trajectories\n",
      "Processed trajectory number :  304 out of  352  trajectories\n",
      "Processed trajectory number :  305 out of  352  trajectories\n",
      "Processed trajectory number :  306 out of  352  trajectories\n",
      "Processed trajectory number :  307 out of  352  trajectories\n",
      "Processed trajectory number :  308 out of  352  trajectories\n",
      "Processed trajectory number :  309 out of  352  trajectories\n",
      "Processed trajectory number :  310 out of  352  trajectories\n",
      "Processed trajectory number :  311 out of  352  trajectories\n",
      "Processed trajectory number :  312 out of  352  trajectories\n",
      "Processed trajectory number :  313 out of  352  trajectories\n",
      "Processed trajectory number :  314 out of  352  trajectories\n",
      "Processed trajectory number :  315 out of  352  trajectories\n",
      "Processed trajectory number :  316 out of  352  trajectories\n",
      "Processed trajectory number :  317 out of  352  trajectories\n",
      "Processed trajectory number :  318 out of  352  trajectories\n",
      "Processed trajectory number :  319 out of  352  trajectories\n",
      "Processed trajectory number :  320 out of  352  trajectories\n",
      "Processed trajectory number :  321 out of  352  trajectories\n",
      "Processed trajectory number :  322 out of  352  trajectories\n",
      "Processed trajectory number :  323 out of  352  trajectories\n",
      "Processed trajectory number :  324 out of  352  trajectories\n",
      "Processed trajectory number :  325 out of  352  trajectories\n",
      "Processed trajectory number :  326 out of  352  trajectories\n",
      "Processed trajectory number :  327 out of  352  trajectories\n",
      "Processed trajectory number :  328 out of  352  trajectories\n",
      "Processed trajectory number :  329 out of  352  trajectories\n",
      "Processed trajectory number :  330 out of  352  trajectories\n",
      "Processed trajectory number :  331 out of  352  trajectories\n",
      "Processed trajectory number :  332 out of  352  trajectories\n",
      "Processed trajectory number :  333 out of  352  trajectories\n",
      "Processed trajectory number :  334 out of  352  trajectories\n",
      "Processed trajectory number :  335 out of  352  trajectories\n",
      "Processed trajectory number :  336 out of  352  trajectories\n",
      "Processed trajectory number :  337 out of  352  trajectories\n",
      "Processed trajectory number :  338 out of  352  trajectories\n",
      "Processed trajectory number :  339 out of  352  trajectories\n",
      "Processed trajectory number :  340 out of  352  trajectories\n",
      "Processed trajectory number :  341 out of  352  trajectories\n",
      "Processed trajectory number :  342 out of  352  trajectories\n",
      "Processed trajectory number :  343 out of  352  trajectories\n",
      "Processed trajectory number :  344 out of  352  trajectories\n",
      "Processed trajectory number :  345 out of  352  trajectories\n",
      "Processed trajectory number :  346 out of  352  trajectories\n",
      "Processed trajectory number :  347 out of  352  trajectories\n",
      "Processed trajectory number :  348 out of  352  trajectories\n",
      "Processed trajectory number :  349 out of  352  trajectories\n",
      "Processed trajectory number :  350 out of  352  trajectories\n",
      "Processed trajectory number :  351 out of  352  trajectories\n",
      "Total mean error of the model is  0.10317052771137915\n"
     ]
    }
   ],
   "source": [
    "for b in range(num_batches):\n",
    "# for b in range(1):\n",
    "    start = time.time()\n",
    "    # Get the source and target data of the current batch\n",
    "    # x has the source data, y has the target data\n",
    "    x, y, pointer = next_batch(data, pointer)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    for _ in range(1):\n",
    "        obs_traj = x[0,:observed_length, 1:]\n",
    "        for position in obs_traj[:-1]:\n",
    "            # Create the input data tensor\n",
    "            input_data_tensor = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "            input_data_tensor[0, 0, 0] = position[0]  # x\n",
    "            input_data_tensor[0, 0, 1] = position[1]  # y\n",
    "\n",
    "            # Create the feed dict\n",
    "            feed = {lstm.input_data: input_data_tensor, lstm.initial_state: state}\n",
    "            # Get the final state after processing the current position\n",
    "            [state] = lstm.sess.run([lstm.final_state], feed)\n",
    "\n",
    "        returned_traj = obs_traj\n",
    "        last_position = obs_traj[-1]\n",
    "\n",
    "        prev_data = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "        prev_data[0, 0, 0] = last_position[0]  # x\n",
    "        prev_data[0, 0, 1] = last_position[1]  # y\n",
    "\n",
    "        prev_target_data = np.reshape(x[0][obs_traj.shape[0], 1:], (1, 1, 2))\n",
    "        for t in range(predicted_length):\n",
    "            feed = {lstm.input_data: prev_data, lstm.initial_state: state, lstm.target_data: prev_target_data}\n",
    "            [o_mux, o_muy, o_sx, o_sy, o_corr, state, cost] = lstm.sess.run(\n",
    "                [lstm.mux, lstm.muy, lstm.sx, lstm.sy, lstm.corr, lstm.final_state, lstm.cost], feed)\n",
    "\n",
    "            mean = [o_mux[0][0], o_muy[0][0]]\n",
    "            # Extract covariance matrix\n",
    "            cov = [[o_sx[0][0]*o_sx[0][0], o_corr[0][0]*o_sx[0][0]*o_sy[0][0]], [o_corr[0][0]*o_sx[0][0]*o_sy[0][0], o_sy[0][0]*o_sy[0][0]]]\n",
    "            # Sample a point from the multivariate normal distribution\n",
    "            sampled_x = np.random.multivariate_normal(mean, cov, 1)\n",
    "            next_x, next_y = sampled_x[0][0], sampled_x[0][1]\n",
    "            returned_traj = np.vstack((returned_traj, [next_x, next_y]))\n",
    "\n",
    "            prev_data[0, 0, 0] = next_x\n",
    "            prev_data[0, 0, 1] = next_y\n",
    "\n",
    "        complete_traj = returned_traj\n",
    "#         for point in complete_traj:\n",
    "#             xs.append(point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0])\n",
    "#             ys.append(point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0])\n",
    "\n",
    "    total_error += get_mean_error(complete_traj, x[0, :, 1:], observed_length)\n",
    "    print(\"Processed trajectory number : \", b, \"out of \", num_batches, \" trajectories\")\n",
    "\n",
    "print(\"Total mean error of the model is \", total_error/num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(frame, pt, frame_txt):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 0.6\n",
    "    thickness = 1\n",
    "    sz, baseline = cv2.getTextSize(frame_txt, font, scale, thickness)\n",
    "    baseline += thickness\n",
    "    lower_left = (pt[0], pt[1])\n",
    "    pt = (pt[0], pt[1]-baseline)\n",
    "    upper_right = (pt[0]+sz[0], pt[1]-sz[1]-2)\n",
    "    cv2.rectangle(frame, lower_left, upper_right, (0,0,0), -1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, frame_txt, pt, font, scale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "    return lower_left, upper_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossline(curr, prev, length):\n",
    "    diff = curr - prev\n",
    "    if diff[1] == 0:\n",
    "        p1 = (int(curr[1]), int(curr[0]-length/2))\n",
    "        p2 = (int(curr[1]), int(curr[0]+length/2))\n",
    "    else:\n",
    "        slope = -diff[0]/diff[1]\n",
    "        x = np.cos(np.arctan(slope)) * length / 2\n",
    "        y = slope * x\n",
    "        p1 = (int(curr[1]-y), int(curr[0]-x))\n",
    "        p2 = (int(curr[1]+y), int(curr[0]+x))\n",
    "    return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqname = \"seq_hotel\"\n",
    "cap = cv2.VideoCapture(os.path.join(\"/home/todor/Documents/data/ewap_dataset/seq_hotel\", seqname+\".avi\"))\n",
    "cap.set(POS_FRAMES, int(x[0][0][0]))\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # float\n",
    "frame_num = int(cap.get(POS_FRAMES))\n",
    "now = int(cap.get(POS_MSEC) / 1000)\n",
    "_, frame = cap.read()\n",
    "frame_txt = \"{:0>2}:{:0>2}\".format(now//60, now%60)\n",
    "frame_txt += ' (' + str(frame_num) + ')'\n",
    "# agent_txt = 'Agent: {}'.format(int())\n",
    "pt = (3, frame.shape[0]-3)\n",
    "ll, ur = draw_text(frame, pt, frame_txt)\n",
    "\n",
    "truth = []\n",
    "for idx, point in enumerate(x[0, :, 1:]):\n",
    "#     (step[1] - statistics[0][0]) / (statistics[0][1] - statistics[0][0])\n",
    "    point[0] = point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0]\n",
    "    point[1] = point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0]\n",
    "    truth.append(point)\n",
    "truth = np.array(truth)\n",
    "color = (0, 255, 0)\n",
    "prev = truth[0]\n",
    "for curr in truth[1:]:\n",
    "    loc1 = (int(prev[1]), int(prev[0])) # (y, x)\n",
    "    loc2 = (int(curr[1]), int(curr[0])) # (y, x)\n",
    "    p1, p2 = crossline(curr, prev, 3)\n",
    "    cv2.line(frame, p1, p2, color, 1, cv2.LINE_AA)\n",
    "    cv2.line(frame, loc1, loc2, color, 1, cv2.LINE_AA)\n",
    "    prev = curr\n",
    "\n",
    "color = (192,19,192)\n",
    "pred = []\n",
    "for idx, point in enumerate(complete_traj):\n",
    "    point[0] = point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0]\n",
    "    point[1] = point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0]\n",
    "    pred.append(point)\n",
    "pred = np.array(pred)\n",
    "prev = pred[0]\n",
    "for curr in pred[1:]:\n",
    "    loc1 = (int(prev[1]), int(prev[0])) # (y, x)\n",
    "    loc2 = (int(curr[1]), int(curr[0])) # (y, x)\n",
    "    p1, p2 = crossline(curr, prev, 3)\n",
    "    cv2.line(frame, p1, p2, color, 1, cv2.LINE_AA)\n",
    "    cv2.line(frame, loc1, loc2, color, 1, cv2.LINE_AA)\n",
    "    prev = curr\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/todor/Documents/data/ewap_dataset/seq_hotel\n"
     ]
    }
   ],
   "source": [
    "agentsData, statistics, peds_in_frame = frame_preprocess(directory)\n",
    "data, num_batches = load_preprocessed(agentsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph successfully reset\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "lstm = BasicLSTM(\n",
    "            batch_size=1,\n",
    "            sequence_length=1)\n",
    "lstm.load_json(\"params.json\")\n",
    "\n",
    "state = lstm.sess.run(lstm.initial_state)\n",
    "pointer = reset_batch_pointer()\n",
    "\n",
    "xs = []\n",
    "ys = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Get the source and target data of the current batch\n",
    "# x has the source data, y has the target data\n",
    "x, y, pointer = next_batch(data, pointer)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_traj = x[0,:observed_length, 1:]\n",
    "    for position in obs_traj[:-1]:\n",
    "        # Create the input data tensor\n",
    "        input_data_tensor = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "        input_data_tensor[0, 0, 0] = position[0]  # x\n",
    "        input_data_tensor[0, 0, 1] = position[1]  # y\n",
    "\n",
    "        # Create the feed dict\n",
    "        feed = {lstm.input_data: input_data_tensor, lstm.initial_state: state}\n",
    "        # Get the final state after processing the current position\n",
    "        [state] = lstm.sess.run([lstm.final_state], feed)\n",
    "\n",
    "    returned_traj = obs_traj\n",
    "    last_position = obs_traj[-1]\n",
    "\n",
    "    prev_data = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "    prev_data[0, 0, 0] = last_position[0]  # x\n",
    "    prev_data[0, 0, 1] = last_position[1]  # y\n",
    "\n",
    "    prev_target_data = np.reshape(x[0][obs_traj.shape[0], 1:], (1, 1, 2))\n",
    "    for t in range(predicted_length):\n",
    "        feed = {lstm.input_data: prev_data, lstm.initial_state: state, lstm.target_data: prev_target_data}\n",
    "        [o_mux, o_muy, o_sx, o_sy, o_corr, state, cost] = lstm.sess.run(\n",
    "            [lstm.mux, lstm.muy, lstm.sx, lstm.sy, lstm.corr, lstm.final_state, lstm.cost], feed)\n",
    "\n",
    "        mean = [o_mux[0][0], o_muy[0][0]]\n",
    "        # Extract covariance matrix\n",
    "        cov = [[o_sx[0][0]*o_sx[0][0], o_corr[0][0]*o_sx[0][0]*o_sy[0][0]], [o_corr[0][0]*o_sx[0][0]*o_sy[0][0], o_sy[0][0]*o_sy[0][0]]]\n",
    "        # Sample a point from the multivariate normal distribution\n",
    "        sampled_x = np.random.multivariate_normal(mean, cov, 1)\n",
    "        next_x, next_y = sampled_x[0][0], sampled_x[0][1]\n",
    "        returned_traj = np.vstack((returned_traj, [next_x, next_y]))\n",
    "\n",
    "        prev_data[0, 0, 0] = next_x\n",
    "        prev_data[0, 0, 1] = next_y\n",
    "\n",
    "    complete_traj = returned_traj\n",
    "    for point in complete_traj:\n",
    "        xs.append(point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0])\n",
    "        ys.append(point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f055c114978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFE1JREFUeJzt3XGsnfV93/H3B9vYBAJeEoe4xpOTQFYyGgxxidV2TWOoAkyKOylZ6daEZaGWsnRt0q6V2k7tyBppaaqgJWpTOQIJMrZCQ2gsBu2IEtYiFXuGGgN1Qm63RHiw0ARM4lAcbL774/xMTuxr7rm+9/oc39/7JR3d5/ye33Pu93cfH3/O85znnF+qCklSf04ZdwGSpPEwACSpUwaAJHXKAJCkThkAktQpA0CSOjVjACRZkWRHkgeTPJLk2tZ+aZIHkuxKcm+Sc1v78iS3JJlKsj3JuoUdgiTpeIxyBHAA2FRVFwLrgcuTbAQ+BfzLqloP/Ffg37f+7wOerqpzgeuAj85/2ZKkuZoxAGpgf7u7rN2q3c5s7WcBj7flzcCNbfmzwKVJMm8VS5LmxdJROiVZAtwPnAv8QVVtT3INcGeSvwe+DWxs3dcAjwFU1cEkzwCvBL55xGNuAbYALGHJm1/2YpZI0mQ7eO6KcZcAwN9PPfHNqlp1vNuPFABVdQhYn2QlcHuSC4APAVe2MPg14OPANcB0r/aP+r6JqtoKbAU4M6+ot+TS4xyCJJ1Y3/j4+eMuAYCH3vEfvz6X7Wd1FVBV7QPuAa4ALqyq7W3VLcCPteW9wFqAJEsZnB56ai5FSpLm3yhXAa1qr/xJchpwGbAHOCvJG1q3n25tANuAq9vyO4Evlt84J0kTZ5RTQKuBG9v7AKcAt1bVHUl+AbgtyQvA08C/bv2vBz6TZIrBK/+rFqBuSdIczRgAVbUbuGia9tuB26dpfw5417xUJ0laMH4SWJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjo1ypSQkjQxvvH588ddAmdv3jNzpxPgoTlu7xGAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMzBkCSFUl2JHkwySNJrm3tSfKRJI8m2ZPkl4baP5FkKsnuJBcv9CAkSbM3yieBDwCbqmp/kmXAvUnuAs4H1gI/XFUvJHl1638FcF67vQX4VPspSZogMwZAVRWwv91d1m4FvB/4F1X1Quv3ZOuzGbipbXdfkpVJVlfVE/NevSTpuI30HkCSJUl2AU8Cd1fVduD1wM8m2ZnkriTnte5rgMeGNt/b2o58zC1t253Pc2Buo5AkzdpIAVBVh6pqPXAOcEmSC4DlwHNVtQH4NHBD657pHmKax9xaVRuqasMylh9f9ZKk4zarq4Cqah9wD3A5g1f2t7VVtwNvast7Gbw3cNg5wONzqlKSNO9GuQpoVZKVbfk04DLgy8CfAptat7cCj7blbcB72tVAG4FnPP8vSZNnlKuAVgM3JlnCIDBurao7ktwL3JzkQwzeJL6m9b8TuBKYAp4F3jv/ZUuS5mqUq4B2AxdN074P+KfTtBfwgXmpTpK0YPwksCR1ygCQpE4ZAJLUKSeF19EyIa8LBh8yl37ApEzIvhhMyDNdknSiGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerU0nEXMCmydNm4SwCgDh0adwksOeP0cZcAwKH93x13CVAvjLuCifGNz58/7hIAOHvznnGXsGjMeASQZEWSHUkeTPJIkmuPWP/JJPuH7i9PckuSqSTbk6yb/7IlSXM1yimgA8CmqroQWA9cnmQjQJINwMoj+r8PeLqqzgWuAz46j/VKkubJjAFQA4df4S9rt0qyBPgY8OtHbLIZuLEtfxa4NEnmqV5J0jwZ6U3gJEuS7AKeBO6uqu3ALwLbquqJI7qvAR4DqKqDwDPAK6d5zC1JdibZ+TwH5jIGSdJxGOlN4Ko6BKxPshK4PclPAu8Cfmqa7tO92q9pHnMrsBXgzLziqPWSpIU1q8tAq2ofcA/wNuBcYCrJ14CXJZlq3fYCawGSLAXOAp6ap3olSfNklKuAVrVX/iQ5DbgMuL+qXlNV66pqHfBse9MXYBtwdVt+J/DFqvIVviRNmFFOAa0Gbmxv+p4C3FpVd7xE/+uBz7QjgqeAq+ZepiRpvs0YAFW1G7hohj5nDC0/x+D9AUnSBPOrICSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqZEmhDkhMt4sqkOHxvr7J0kdPDjuEgBYuvrscZfAwcePnPBuPE459dRxl8DZm/eMuwTNM48AJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOjVjACRZkWRHkgeTPJLk2tZ+c5KvJHk4yQ1JlrX2JPlEkqkku5NcvNCDkCTN3ihHAAeATVV1IbAeuDzJRuBm4IeBHwFOA65p/a8Azmu3LcCn5rtoSdLczRgANbC/3V3WblVVd7Z1BewAzml9NgM3tVX3ASuTrF6I4iVJx2+k9wCSLEmyC3gSuLuqtg+tWwa8G/iz1rQGeGxo872t7cjH3JJkZ5Kdz3PgeOuXJB2nkQKgqg5V1XoGr/IvSXLB0Oo/BP6iqv6y3c90DzHNY26tqg1VtWEZy2dbtyRpjmZ1FVBV7QPuAS4HSPI7wCrgV4a67QXWDt0/B3h8TlVKkubdKFcBrUqysi2fBlwGfDnJNcDbgZ+rqheGNtkGvKddDbQReKaqJmNiVUnSi0aZFH41cGOSJQwC49aquiPJQeDrwF8lAfhcVX0YuBO4EpgCngXeuyCVS5LmZMYAqKrdwEXTtE+7bbsq6AOzqiIhS5bMapP5lmWjZOHCO+WM08ddAode90PjLmHgW/tn7rPADn7hH467BACW/7Onx10CPH9w3BUM/MAJB82FnwSWpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnJmMm9AlwyqpXjruEgeXLx10B3/qRM8ZdAgDfO2P8dZz9H54ddwkA5NTvjrsElox/dwBw6DvfGXcJi4ZHAJLUKQNAkjplAEhSpwwASeqUASBJnZoxAJKsSLIjyYNJHklybWt/bZLtSb6a5JYkp7b25e3+VFu/bmGHIEk6HqMcARwANlXVhcB64PIkG4GPAtdV1XnA08D7Wv/3AU9X1bnAda2fJGnCzBgANbC/3V3WbgVsAj7b2m8EfqYtb273aesvTZJ5q1iSNC9Geg8gyZIku4AngbuBvwX2VdXB1mUvsKYtrwEeA2jrnwGO+pRVki1JdibZ+Xw9N7dRSJJmbaQAqKpDVbUeOAe4BDh/um7t53Sv9uuohqqtVbWhqjYsy4pR65UkzZNZXQVUVfuAe4CNwMokh79K4hzg8ba8F1gL0NafBTw1H8VKkubPKFcBrUqysi2fBlwG7AG+BLyzdbsa+Hxb3tbu09Z/saqOOgKQJI3XKF8Gtxq4MckSBoFxa1XdkeRvgD9O8rvAXwPXt/7XA59JMsXglf9VC1C3JGmOZgyAqtoNXDRN+/9m8H7Ake3PAe+al+okSQvGTwJLUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpUT4ItvCqqIPPj7eEp/aN9fe/aOn4d8np3zjqu/vG4n99+NPjLoGfvupfjbsEAOrAgXGXwAvPjb8GgCxdNu4Sxv7/1XzxCECSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NWMAJFmb5EtJ9iR5JMkvt/b1Se5LsivJziSXtPYk+USSqSS7k1y80IOQJM3eKPMPHgR+taoeSPJy4P4kdwO/B1xbVXclubLd/yngCuC8dnsL8Kn2U5I0QWY8AqiqJ6rqgbb8HWAPsAYo4MzW7Szg8ba8GbipBu4DViZZPe+VS5LmZFYzkCdZB1wEbAc+CPx5kt9nECQ/1rqtAR4b2mxva3tijrVKkubRyAGQ5AzgNuCDVfXtJL8LfKiqbkvyz4HrgcuATLN5TfN4W4AtACt42fHUPq8OfffZcZcAwJLTx/+3OH3H18ZdAgCXv+Pnx10Cpz75d+MuAYDKdE+rE1zDwefHXcJAvHZlvoz0l0yyjMF//jdX1eda89XA4eU/AS5py3uBtUObn8P3Tw+9qKq2VtWGqtqwjOXHU7skaQ5GuQooDF7d76mqjw+tehx4a1veBHy1LW8D3tOuBtoIPFNVnv6RpAkzyimgHwfeDTyUZFdr+03gF4D/nGQp8BztdA5wJ3AlMAU8C7x3XiuWJM2LGQOgqu5l+vP6AG+epn8BH5hjXZKkBea7KZLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpUaaE7EO9MO4KADj03WfHXQLs3z/uCgDIt54edwnUiuXjLgGA+t73xl0CZEJeL07Ic3UxmJA9Kkk60QwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NWMAJFmb5EtJ9iR5JMkvD637t0m+0tp/b6j9N5JMtXVvX6jiJUnHb5RPAh8EfrWqHkjycuD+JHcDZwObgTdV1YEkrwZI8kbgKuAfAz8EfCHJG6rq0MIMQZJ0PGY8AqiqJ6rqgbb8HWAPsAZ4P/CfqupAW/dk22Qz8MdVdaCq/g8wBVyyEMVLko7frN4DSLIOuAjYDrwB+CdJtif5n0l+tHVbAzw2tNne1nbkY21JsjPJzuc5cDy1S5LmYOQvg0tyBnAb8MGq+naSpcA/ADYCPwrcmuR1QKbZvI5qqNoKbAU4M684ar0kaWGNdASQZBmD//xvrqrPtea9wOdqYAfwAvCq1r52aPNzgMfnr2RJ0nwY5SqgANcDe6rq40Or/hTY1Pq8ATgV+CawDbgqyfIkrwXOA3bMd+GSpLkZ5RTQjwPvBh5Ksqu1/SZwA3BDkoeB7wFXV1UBjyS5FfgbBlcQfcArgCRp8swYAFV1L9Of1wf4+WNs8xHgI3OoS5K0wPwksCR1ygCQpE4ZAJLUqQzetx1zEcnfAV8/gb/yVQyuWFoMHMtkWkxjgcU1nsU0ln9UVS8/3o1H/iDYQqqqVSfy9yXZWVUbTuTvXCiOZTItprHA4hrPYhvLXLb3FJAkdcoAkKRO9RoAW8ddwDxyLJNpMY0FFtd4HEszEW8CS5JOvF6PACSpewaAJHVq0QXAS81h3Nb/uySV5FXtfpJ8os1hvDvJxeOp/GiLaT7mY40lyfok9yXZ1SYIuqS1T+x+AUiyIsmOJA+28Vzb2l/bJkn6apJbkpza2pe3+1Nt/bpx1j/sJcZyc/t39HCSG9rXwk/0vjnWWIbWfzLJ/qH7J+N+SZKPJHm0PZ9+aah9dvulqhbVDVgNXNyWXw48Cryx3V8L/DmDD529qrVdCdzF4AvvNgLbxz2GmcYCvA34ArC8rXt1+/lG4EFgOfBa4G+BJeMexwxj+R/AFUP74p5J3y+tvgBntOVlDGbJ2wjcClzV2v8IeH9b/jfAH7Xlq4Bbxj2GEcZyZVsX4L8NjWVi982xxtLubwA+A+wf6n8y7pf3AjcBp7R1h5//s94vi+4IoI49hzHAdcCv84MzlG0GbqqB+4CVSVafyJqP5SXGctLNx/wSYyngzNbtLL4/edDE7heAVtfhV5LL2q0YzJHx2dZ+I/AzbXlzu09bf2mSY33L7gl1rLFU1Z1tXTGY0+Oc1mdi982xxpJkCfAxBs//YSfdfmHw/P9wVb3Q+g0//2e1XxZdAAzL0BzGSd4B/N+qevCIbiPNYTxumcf5mMftiLF8EPhYkseA3wd+o3Wb+LEkWZLBHBlPAnczOOLaV1UHW5fhml8cT1v/DPDKE1vxsR05lqraPrRuGYM5Qf6sNU30vjnGWH4R2FZVTxzR/WTcL68HfradMr0ryXmt+6z3y6INgAzNYcxgYprfAn57uq7TtE3UtbE5Yj5mBl/hcXg+5l9jMB/z4UP1I036WN4PfKiq1gIfYjD7HJwEY6mqQ1W1nsEr40uA86fr1n5O9HiOHEuSC4ZW/yHwF1X1l+3+yTaWnwTeBXxymu4n21guYHCK97kafJ3FpxlMzgXHMZZFGQA5eg7j1zM4J/5gkq8x+GM+kOQ1TPgcxtOMBU7S+ZiPMZargcPLf8L3T1lN9FiGVdU+4B4GgbwyyeHv2Bqu+cXxtPVnAU+d2EpnNjSWywGS/A6wCviVoW4nxb4ZGsvbgHOBqfb8f1mSqdbtZNwvexk8jwBuB97Ulme9XxZdALRXwj8wh3FVPVRVr66qdVW1jsEf6uKq+n8M5jB+T3sHfSPwzDSHiWMx3Viak24+5pcYy+PAW9vyJuCrbXli9wtAklVJVrbl04DLGLyv8SXgna3b1cDn2/K2dp+2/ovt3PrYHWMsX05yDfB24OcOn29uJnbfHGMs91fVa4ae/89W1bltk5NuvzD0/Gfw3Hm0Lc9+v8z0LvHJdgN+gsFhz25gV7tdeUSfr/H9q4AC/AGD87cPARvGPYaZxsLgP/z/AjwMPABsGtrmt9pYvkK7umYSbi8xlp8A7mdw9dJ24M2Tvl9afW8C/rqN52Hgt1v76xiE7hSDI5rDV2qtaPen2vrXjXsMI4zlYPv7H95fh9sndt8cayxH9Bm+Cuhk3C8rgf/e/vZ/BVx4vPvFr4KQpE4tulNAkqTRGACS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU/8fwCwScLGv9P4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# xs = np.abs(xs-np.max(xs))\n",
    "ys = np.abs(np.array(ys)-width)\n",
    "Z, xedges, yedges = np.histogram2d(xs,ys)\n",
    "plt.pcolormesh(xedges, yedges, Z.T)\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "k = gaussian_kde(np.vstack([xs, ys]))\n",
    "xi, yi = np.mgrid[xs.min():xs.max():xs.size**0.5*1j,ys.min():ys.max():ys.size**0.5*1j]\n",
    "zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "fig = plt.figure(figsize=(7,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "# alpha=0.5 will make the plots semitransparent\n",
    "ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)\n",
    "ax2.contourf(xi, yi, zi.reshape(xi.shape), alpha=0.5)\n",
    "\n",
    "ax1.set_xlim(xs.min(), xs.max())\n",
    "ax1.set_ylim(ys.min(), ys.max())\n",
    "ax2.set_xlim(xs.min(), xs.max())\n",
    "ax2.set_ylim(ys.min(), ys.max())\n",
    "\n",
    "im = plt.imread('/home/todor/Documents/data/ewap_dataset/seq_hotel/map.png')\n",
    "ax1.imshow(im, extent=[xs.min(), xs.max(), ys.min(), ys.max()], aspect='auto')\n",
    "ax2.imshow(im, extent=[xs.min(), xs.max(), ys.min(), ys.max()], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "_xs = [[] for _ in range(len(peds_in_frame[0]))]\n",
    "_ys = [[] for _ in range(len(peds_in_frame[0]))]\n",
    "# Get the source and target data of the current batch\n",
    "# x has the source data, y has the target data\n",
    "# x, y, pointer = next_batch(data, pointer)\n",
    "for id_ped, ped in enumerate(peds_in_frame[0]):\n",
    "    curr_traj = agentsData[ped]\n",
    "    if curr_traj.shape[0] > (sequence_length+2):\n",
    "        x = curr_traj[:sequence_length]\n",
    "        y = curr_traj[1:sequence_length+1]\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        # peds_in_frame\n",
    "        for _ in range(100):\n",
    "            obs_traj = x[:observed_length, 1:]\n",
    "            for position in obs_traj[:-1]:\n",
    "                # Create the input data tensor\n",
    "                input_data_tensor = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "                input_data_tensor[0, 0, 0] = position[0]  # x\n",
    "                input_data_tensor[0, 0, 1] = position[1]  # y\n",
    "\n",
    "                # Create the feed dict\n",
    "                feed = {lstm.input_data: input_data_tensor, lstm.initial_state: state}\n",
    "                # Get the final state after processing the current position\n",
    "                [state] = lstm.sess.run([lstm.final_state], feed)\n",
    "\n",
    "            returned_traj = obs_traj\n",
    "            last_position = obs_traj[-1]\n",
    "\n",
    "            prev_data = np.zeros((1, 1, 2), dtype=np.float32)\n",
    "            prev_data[0, 0, 0] = last_position[0]  # x\n",
    "            prev_data[0, 0, 1] = last_position[1]  # y\n",
    "\n",
    "            prev_target_data = np.reshape(x[obs_traj.shape[0], 1:], (1, 1, 2))\n",
    "            for t in range(predicted_length):\n",
    "                feed = {lstm.input_data: prev_data, lstm.initial_state: state, lstm.target_data: prev_target_data}\n",
    "                [o_mux, o_muy, o_sx, o_sy, o_corr, state, cost] = lstm.sess.run(\n",
    "                    [lstm.mux, lstm.muy, lstm.sx, lstm.sy, lstm.corr, lstm.final_state, lstm.cost], feed)\n",
    "\n",
    "                mean = [o_mux[0][0], o_muy[0][0]]\n",
    "                # Extract covariance matrix\n",
    "                cov = [[o_sx[0][0]*o_sx[0][0], o_corr[0][0]*o_sx[0][0]*o_sy[0][0]], [o_corr[0][0]*o_sx[0][0]*o_sy[0][0], o_sy[0][0]*o_sy[0][0]]]\n",
    "                # Sample a point from the multivariate normal distribution\n",
    "                sampled_x = np.random.multivariate_normal(mean, cov, 1)\n",
    "                next_x, next_y = sampled_x[0][0], sampled_x[0][1]\n",
    "                returned_traj = np.vstack((returned_traj, [next_x, next_y]))\n",
    "\n",
    "                prev_data[0, 0, 0] = next_x\n",
    "                prev_data[0, 0, 1] = next_y\n",
    "\n",
    "            complete_traj = returned_traj\n",
    "            for point in complete_traj:\n",
    "                _xs[id_ped].append(point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0])\n",
    "                _ys[id_ped].append(point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0])\n",
    "    else:\n",
    "        for _ in range(100):\n",
    "            for point in curr_traj:\n",
    "                _xs[id_ped].append(point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0])\n",
    "                _ys[id_ped].append(point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqname = \"seq_hotel\"\n",
    "cap = cv2.VideoCapture(os.path.join(\"/home/todor/Documents/data/ewap_dataset/seq_hotel\", seqname+\".avi\"))\n",
    "cap.set(POS_FRAMES, int(1))\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # float\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # float\n",
    "frame_num = int(cap.get(POS_FRAMES))\n",
    "now = int(cap.get(POS_MSEC) / 1000)\n",
    "_, frame = cap.read()\n",
    "frame_txt = \"{:0>2}:{:0>2}\".format(now//60, now%60)\n",
    "frame_txt += ' (' + str(frame_num) + ')'\n",
    "# agent_txt = 'Agent: {}'.format(int())\n",
    "pt = (3, frame.shape[0]-3)\n",
    "ll, ur = draw_text(frame, pt, frame_txt)\n",
    "\n",
    "truth = []\n",
    "for idx, point in enumerate(agentsData[peds_in_frame[0][3]][:, 1:]):\n",
    "    truth.append([point[0]*(statistics[0][1] - statistics[0][0]) + statistics[0][0], point[1]*(statistics[1][1] - statistics[1][0]) + statistics[1][0]])\n",
    "truth = np.array(truth)\n",
    "\n",
    "color = (0, 255, 0)\n",
    "prev = truth[0]\n",
    "for curr in truth[1:]:\n",
    "    loc1 = (int(prev[1]), int(prev[0])) # (y, x)\n",
    "    loc2 = (int(curr[1]), int(curr[0])) # (y, x)\n",
    "    p1, p2 = crossline(curr, prev, 3)\n",
    "    cv2.line(frame, p1, p2, color, 1, cv2.LINE_AA)\n",
    "    cv2.line(frame, loc1, loc2, color, 1, cv2.LINE_AA)\n",
    "    prev = curr\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53eb4f73c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# xs = np.abs(xs-np.max(xs))\n",
    "ys = np.abs(np.array(_ys[3])-width)\n",
    "xs = _xs[3]\n",
    "Z, xedges, yedges = np.histogram2d(xs,ys)\n",
    "plt.pcolormesh(xedges, yedges, Z.T)\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "k = gaussian_kde(np.vstack([xs, ys]))\n",
    "xi, yi = np.mgrid[xs.min():xs.max():xs.size**0.5*1j,ys.min():ys.max():ys.size**0.5*1j]\n",
    "zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
    "fig = plt.figure(figsize=(7,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "# alpha=0.5 will make the plots semitransparent\n",
    "ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)\n",
    "ax2.contourf(xi, yi, zi.reshape(xi.shape), alpha=0.5)\n",
    "\n",
    "ax1.set_xlim(xs.min(), xs.max())\n",
    "ax1.set_ylim(ys.min(), ys.max())\n",
    "ax2.set_xlim(xs.min(), xs.max())\n",
    "ax2.set_ylim(ys.min(), ys.max())\n",
    "\n",
    "im = plt.imread('/home/todor/Documents/data/ewap_dataset/seq_hotel/map.png')\n",
    "ax1.imshow(im, extent=[xs.min(), xs.max(), ys.min(), ys.max()], aspect='auto')\n",
    "ax2.imshow(im, extent=[xs.min(), xs.max(), ys.min(), ys.max()], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_frame_preprocess():\n",
    "    '''\n",
    "    Preprocess the frames from the datasets.\n",
    "    Convert values to pixel locations from millimeters\n",
    "    obtain and store all frames data the actually used frames (as some are skipped), \n",
    "    the ids of all pedestrians that are present at each of those frames and the sufficient statistics.\n",
    "    '''\n",
    "    def collect_stats():\n",
    "        ''' Collect the means and standard deviation of all (x, y) positions of the agents '''\n",
    "        x_pos = []\n",
    "        y_pos = []\n",
    "        for agent_id in range(1, len(agents)):\n",
    "            trajectory = [[] for _ in range(3)]\n",
    "            traj = agents[agent_id]\n",
    "            for step in traj:\n",
    "                x_pos.append(step[1])\n",
    "                y_pos.append(step[2])\n",
    "        x_pos = np.asarray(x_pos)\n",
    "        y_pos = np.asarray(y_pos)\n",
    "        # takes the average over all points through all agents\n",
    "        return [[np.mean(x_pos), np.std(x_pos)], [np.mean(y_pos), np.std(y_pos)]]\n",
    "    \n",
    "    allPedsInFrame = []\n",
    "    Hfile = os.path.join(directory, \"H.txt\")\n",
    "    obsfile = os.path.join(directory, \"obsmat.txt\")\n",
    "    # Parse homography matrix.\n",
    "    H = np.loadtxt(Hfile)\n",
    "    Hinv = np.linalg.inv(H)\n",
    "    # Parse pedestrian annotations.\n",
    "    frames, pedsInFrame, agents = parse_annotations(Hinv, obsfile)\n",
    "    allPedsInFrame.append(agents)\n",
    "    framesContent = np.zeros((len(frames), 40, 3))\n",
    "    # collect mean and std\n",
    "    statistics = collect_stats()\n",
    "    # collect the id, normalised x and normalised y of each agent's position\n",
    "    for ind, agent_list in enumerate(pedsInFrame):\n",
    "        pedsWithPos = []\n",
    "        for agent_id in agent_list:\n",
    "            traj = agents[agent_id]\n",
    "            for step in traj:\n",
    "                if step[0] == ind:\n",
    "                    pedsWithPos.append([\n",
    "                        agent_id,\n",
    "                        (step[1] - statistics[0][0]) / statistics[0][1],\n",
    "                        (step[2] - statistics[1][0]) / statistics[1][1]\n",
    "                    ])\n",
    "        framesContent[ind, 0:len(agent_list), :] = np.array(pedsWithPos)\n",
    "\n",
    "    return framesContent, frames, allPedsInFrame, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(Hinv, obsmat_txt):\n",
    "    def to_image_frame(loc):\n",
    "        \"\"\"\n",
    "        Given H^-1 and (x, y, z) in world coordinates, returns (u, v, 1) in image\n",
    "        frame coordinates.\n",
    "        \"\"\"\n",
    "        loc = np.dot(Hinv, loc) # to camera frame\n",
    "        return loc/loc[2] # to pixels (from millimeters)\n",
    "\n",
    "    mat = np.loadtxt(obsmat_txt)\n",
    "    num_frames = (mat[-1,0] + 1).astype(\"int\")\n",
    "    num_unique_frames = np.unique(mat[:,0]).size\n",
    "    num_peds = int(np.max(mat[:,1])) + 1\n",
    "    frames = [-1] * num_unique_frames # maps timestep -> (unique) frame\n",
    "    peds_in_frame = [[] for _ in range(num_unique_frames)] # maps timestep -> ped IDs\n",
    "    peds = [np.array([]).reshape(0,4) for _ in range(num_peds)] # maps ped ID -> (t,x,y,z) path\n",
    "    frame = 0\n",
    "    time = -1\n",
    "    for row in mat:\n",
    "        if row[0] != frame:\n",
    "            frame = int(row[0])\n",
    "            time += 1\n",
    "            frames[time] = frame\n",
    "\n",
    "        ped = int(row[1])\n",
    "        # if ped not in ignored_peds: # TEMP HACK - can cause empty timesteps\n",
    "        peds_in_frame[time].append(ped)\n",
    "        loc = np.array([row[2], row[4], 1])\n",
    "        loc = to_image_frame(loc)\n",
    "        loc = [time, loc[0], loc[1], loc[2]]\n",
    "        peds[ped] = np.vstack((peds[ped], loc))\n",
    "\n",
    "    return (frames, peds_in_frame, peds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
